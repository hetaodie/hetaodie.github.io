1
00:00:00,000 --> 00:00:02,549
大家好 我是 Jay 这节课

2
00:00:02,549 --> 00:00:05,234
我们要讨论的是降维

3
00:00:05,235 --> 00:00:09,570
我们学习的第一个方法是随机投影

4
00:00:09,570 --> 00:00:12,375
这是一个很有效的降维方法

5
00:00:12,375 --> 00:00:15,509
在计算上比主成分分析更有效率

6
00:00:15,509 --> 00:00:18,300
它通常应用于 当一个数据集里有太多维度

7
00:00:18,300 --> 00:00:21,705
主成分分析无法直接计算的情境下

8
00:00:21,704 --> 00:00:26,909
假定运行你的应用程序的系统计算资源有限

9
00:00:26,910 --> 00:00:31,324
或者你发现 在你所处具体情境下 主成分分析太繁重

10
00:00:31,324 --> 00:00:33,685
就像主成分分析 这需要一个数据集

11
00:00:33,685 --> 00:00:38,800
假定这是我们的 d 维度数据集 设 d 为 1000

12
00:00:38,799 --> 00:00:42,184
及某些样本数组成行

13
00:00:42,185 --> 00:00:45,100
设其为 N 所以这些是我们的列

14
00:00:45,100 --> 00:00:49,164
它需要我们的数据集 并将其转化成

15
00:00:49,164 --> 00:00:55,659
数量少得多的 k 个列

16
00:00:55,659 --> 00:00:58,329
此例中设为 50

17
00:00:58,329 --> 00:01:00,909
样本数不变

18
00:01:00,909 --> 00:01:02,139
此处的每个列

19
00:01:02,140 --> 00:01:06,765
从另一处多个列中采集信息

20
00:01:06,765 --> 00:01:10,805
我们来看一个简单化的例子

21
00:01:10,805 --> 00:01:15,815
将一个数据集的维度从二维降至一维

22
00:01:15,814 --> 00:01:20,829
主成分分析会试着将方差最大化

23
00:01:20,829 --> 00:01:26,450
它寻找将方差最大化的矢量或方向

24
00:01:26,450 --> 00:01:28,640
所以 当它将数据从二维投影为一维时

25
00:01:28,640 --> 00:01:33,094
它损失最少的信息

26
00:01:33,094 --> 00:01:35,659
那这条线就会像这样

27
00:01:35,659 --> 00:01:38,569
这些就是投影

28
00:01:38,569 --> 00:01:40,864
在一维里

29
00:01:40,864 --> 00:01:43,224
该数据集看上去就是这样

30
00:01:43,224 --> 00:01:46,634
随机投影 进行主成分分析做过的计算

31
00:01:46,635 --> 00:01:49,980
尤其是存在很多维度时

32
00:01:49,980 --> 00:01:53,019
它会消耗一定量的资源

33
00:01:53,019 --> 00:01:54,734
随机投影只是表示

34
00:01:54,734 --> 00:01:57,239
我们会选择一条线 任何一条

35
00:01:57,239 --> 00:02:00,539
在那条线上进行投射 这就是我们的数据集

36
00:02:00,540 --> 00:02:04,804
它在某些情境下没有太大意义

37
00:02:04,804 --> 00:02:08,314
在我们简化了的从二维到一维的情境下

38
00:02:08,314 --> 00:02:13,164
它其实是有作用的 在更高维度的效果更好

39
00:02:13,164 --> 00:02:18,144
且工作效能很高

40
00:02:18,145 --> 00:02:21,340
随机投影的基本前提是

41
00:02:21,340 --> 00:02:24,099
我们可以用数据集乘以一个这样的随机矩阵

42
00:02:24,099 --> 00:02:31,000
来减少其中的维度数量

43
00:02:31,000 --> 00:02:35,432
我们的数据集里有 d

44
00:02:35,432 --> 00:02:37,764
但是 k 要么是我们计算的

45
00:02:37,764 --> 00:02:39,459
要么是我们要得出的

46
00:02:39,460 --> 00:02:42,939
我们也有方法可以计算 k 的一个保守估计

47
00:02:42,939 --> 00:02:47,770
或合适的估值 这就是产生的数据集

48
00:02:47,770 --> 00:02:49,795
乘以一个随机矩阵

49
00:02:49,794 --> 00:02:53,829
在某种程度上 这就是全部的随机投影

50
00:02:53,830 --> 00:02:57,460
我们举一个具体的例子
51
00:02:57,460 --> 00:02:59,185,
假定这是我们的数据集

51
00:02:59,185 --> 00:03:02,455
它有 12000 个维度

52
00:03:02,455 --> 00:03:04,698
这是 d

53
00:03:04,698 --> 00:03:08,665
它有 1500 行或样本

54
00:03:08,664 --> 00:03:12,370
如果我们把它交给 Scikit-learn 问它 “Scikit-learn 

55
00:03:12,370 --> 00:03:16,569
你能只用默认值对这个数据及进行随机投影吗？”

56
00:03:16,569 --> 00:03:18,500
它给出的是这个数据集

57
00:03:18,500 --> 00:03:27,194
它有 6200 个维度 显然样本数不变

58
00:03:27,194 --> 00:03:30,900
我们怎么知道它起了作用呢？

59
00:03:30,900 --> 00:03:33,555
我们怎么知道 k 是怎么来的呢？

60
00:03:33,555 --> 00:03:36,750
支撑随机投影的

61
00:03:36,750 --> 00:03:41,009
是 Johnson-Lindenstrauss 引理

62
00:03:41,009 --> 00:03:48,519
该引理认为 处于高维度空间的 有 N 个点的数据集 比如这个数据集

63
00:03:48,519 --> 00:03:53,805
12000 的高维度空间端点很高 

64
00:03:53,805 --> 00:03:57,974
只要乘以随机矩阵

65
00:03:57,973 --> 00:04:00,244
就可以被映射成很低维度的空间

66
00:04:00,245 --> 00:04:03,905
即这个缩小的数据集

67
00:04:03,905 --> 00:04:07,287
某种程度上这就是它的重要之处

68
00:04:07,287 --> 00:04:09,740
因为它可以在很大程度上

69
00:04:09,740 --> 00:04:13,715
保留点之间的距离

70
00:04:13,715 --> 00:04:16,879
所以该数据集投影后

71
00:04:16,879 --> 00:04:21,254
每两点之间的距离 每对之间的距离

72
00:04:21,254 --> 00:04:23,555
在某种程度上可以得到保留

73
00:04:23,555 --> 00:04:26,990
这很重要 因为在

74
00:04:26,990 --> 00:04:30,999
大部分或许多监督学习与非监督学习中

75
00:04:30,999 --> 00:04:34,290
很多算法和点之间的距离大有关联

76
00:04:34,290 --> 00:04:36,950
所以我们要保证

77
00:04:36,949 --> 00:04:40,534
这些距离会有些失真

78
00:04:40,535 --> 00:04:43,250
但可以得到保留

79
00:04:43,250 --> 00:04:46,579
那如何保留呢？

80
00:04:46,579 --> 00:04:48,995
我们又如何保证呢？

81
00:04:48,995 --> 00:04:53,090
我们简单举个例子来实际计算一下

82
00:04:53,089 --> 00:04:57,259
假定我们取这里的头两行

83
00:04:57,259 --> 00:04:58,992
这一行和这一行

84
00:04:58,992 --> 00:05:02,269
我们数据集里的头两个点

85
00:05:02,269 --> 00:05:05,164
这就是它们投影后的值

86
00:05:05,165 --> 00:05:10,230
即 同样的样本 不同的维度水平

87
00:05:10,230 --> 00:05:16,210
Johnson-Lindenstrauss 引理指出 投影后两点距离的

88
00:05:16,209 --> 00:05:23,289
平方值稍有压缩

89
00:05:23,290 --> 00:05:26,980
它大于原数据集里两点距离的平方值

90
00:05:26,980 --> 00:05:32,754
乘以 (1-eps) 的值

91
00:05:32,754 --> 00:05:34,360
Epsilon 就类似于

92
00:05:34,360 --> 00:05:42,790
我们在此次投影中可接受的随机投影的误差

93
00:05:42,790 --> 00:05:49,110
所以 投影后的数据集里两点距离

94
00:05:49,110 --> 00:05:52,395
大于 (1-eps) 

95
00:05:52,394 --> 00:05:56,669
乘原数据集里两点距离的平方值

96
00:05:56,670 --> 00:06:02,670
且小于 (1+eps) 乘原数据集里两点距离的平方值

97
00:06:02,670 --> 00:06:05,100
我实际上算出了这些数字

98
00:06:05,100 --> 00:06:09,490
这两点之间的距离为 125.6

99
00:06:09,490 --> 00:06:11,970
我们放在这里

100
00:06:11,970 --> 00:06:14,250
Epsilon 的默认值

101
00:06:14,250 --> 00:06:15,533
我们没做任何改动

102
00:06:15,533 --> 00:06:19,395
在 Scikit-learn 里的默认值就为 0.1

103
00:06:19,394 --> 00:06:22,549
它可以取 0 到 1 的任何数值

104
00:06:22,550 --> 00:06:26,350
如果我们像这样做

105
00:06:26,350 --> 00:06:29,725
这两点之间的距离为 125.8

106
00:06:29,725 --> 00:06:35,110
那么 这个距离会比该距离平方值的

107
00:06:35,110 --> 00:06:41,905
0.9 倍大 且比该距离平方值的 1.1 倍小

108
00:06:41,904 --> 00:06:46,569
你会发现 Epsilon 有点像一个操纵杆

109
00:06:46,569 --> 00:06:53,740
它用于计算产生了多少列

110
00:06:53,740 --> 00:06:56,995
并且是我们在此降维中可接受的

111
00:06:56,995 --> 00:07:02,340
失真的误差水平

112
00:07:02,339 --> 00:07:06,074
这就产生了保障

113
00:07:06,074 --> 00:07:12,675
保持了数据集里每对点之间的距离

114
00:07:12,675 --> 00:07:14,835
这不仅是关乎 1 和 2

115
00:07:14,834 --> 00:07:16,919
它关乎 1 和其他任何点

116
00:07:16,920 --> 00:07:18,060
关乎 2 和其他任何点

117
00:07:18,060 --> 00:07:20,490
在该保障下

118
00:07:20,490 --> 00:07:26,250
Epsilon 只是经我们输入到函数中

119
00:07:26,250 --> 00:07:32,759
用来在此程度上保持距离

