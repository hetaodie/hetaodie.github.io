1
00:00:00,000 --> 00:00:02,040
Welcome back. In this video,

2
00:00:02,040 --> 00:00:06,389
we'll talk a little bit about the independent component analysis algorithm.

3
00:00:06,389 --> 00:00:08,105
So, this is going to be a very high level view.

4
00:00:08,105 --> 00:00:10,365
We will not delve deep into the math.

5
00:00:10,365 --> 00:00:15,780
We'll refer you to the right place but it's good and invaluable that you know,

6
00:00:15,779 --> 00:00:18,539
just have a general idea about how it works and

7
00:00:18,539 --> 00:00:22,434
what assumptions are there when we want to use it.

8
00:00:22,434 --> 00:00:25,327
And so, what we have,

9
00:00:25,327 --> 00:00:28,829
the data set that we have we call X.

10
00:00:28,829 --> 00:00:37,379
And so that X was generated by multiplying what we would call it a mixing matrix,

11
00:00:37,380 --> 00:00:40,170
which is A by the source signals,

12
00:00:40,170 --> 00:00:41,460
which we also don't have.

13
00:00:41,460 --> 00:00:43,950
So, we don't have A, we don't have S. But,

14
00:00:43,950 --> 00:00:46,760
S is what we want to calculate in the end.

15
00:00:46,759 --> 00:00:55,089
And so, if X is A times S. We can say that S, which is the source.

16
00:00:55,090 --> 00:00:58,710
The goal of what you want here is W,

17
00:00:58,710 --> 00:01:01,725
which is the inverse of A.

18
00:01:01,725 --> 00:01:05,480
So, if A is the mixing matrix we can call W the unmixing

19
00:01:05,480 --> 00:01:09,530
matrix times X, times a data set that we have.

20
00:01:09,530 --> 00:01:12,605
The original recordings that we have.

21
00:01:12,605 --> 00:01:15,105
And so, this formula here.

22
00:01:15,105 --> 00:01:16,780
So, X is an input that we have.

23
00:01:16,780 --> 00:01:19,849
W is what we are trying to calculate.

24
00:01:19,849 --> 00:01:21,704
S is the result.

25
00:01:21,704 --> 00:01:27,784
So, we ICA algorithm and process is all about approximating W or finding

26
00:01:27,784 --> 00:01:37,569
the best W that we can multiply by X the data set here to produce the original signals.

27
00:01:39,159 --> 00:01:45,670
The ICA algorithm is explained clearly in

28
00:01:45,670 --> 00:01:50,945
this paper called the Independent Component Analysis: Algorithms and Applications.

29
00:01:50,944 --> 00:01:53,664
It goes into the derivation of everything here.

30
00:01:53,665 --> 00:02:01,445
It shows a couple of ways to calculate a number of different parts of the algorithm.

31
00:02:01,444 --> 00:02:05,769
But, if we're to have a just a very high level view of the algorithm,

32
00:02:05,769 --> 00:02:09,185
called FastICA. So, this is one way.

33
00:02:09,185 --> 00:02:14,332
This is the one that's implemented actually in scikit-learn.

34
00:02:14,332 --> 00:02:16,090
First, we have X,

35
00:02:16,090 --> 00:02:17,366
which is our data set.

36
00:02:17,366 --> 00:02:20,135
We will center it and whiten it.

37
00:02:20,134 --> 00:02:23,751
And then, we will choose an initial random weight matrix,

38
00:02:23,752 --> 00:02:27,760
we'll call that W. And third step we

39
00:02:27,759 --> 00:02:32,394
estimate W and W as a matrix contains vectors, a number of vectors.

40
00:02:32,395 --> 00:02:36,605
Each one is as a weight vector.

41
00:02:36,604 --> 00:02:41,424
After we estimate it we decorrelate it and decorrelation is

42
00:02:41,425 --> 00:02:46,885
to prevent W_1 and W_2 to sort of convert the same values.

43
00:02:46,884 --> 00:02:51,795
So, we want them to convert to different values.

44
00:02:51,795 --> 00:02:56,655
And then, we repeat from three until we convert,

45
00:02:56,655 --> 00:03:02,134
until we find a value of W that we're satisfied with.

46
00:03:02,134 --> 00:03:06,044
So, most of the math comes at step number three here.

47
00:03:06,044 --> 00:03:10,364
And so, how does the estimation happen?

48
00:03:10,365 --> 00:03:15,670
This is the sort of formula for estimating each of the vectors here.

49
00:03:15,669 --> 00:03:19,584
Where E is the expected value,

50
00:03:19,585 --> 00:03:21,865
x is the data set,

51
00:03:21,865 --> 00:03:25,360
g is just some non-quadratic function.

52
00:03:25,360 --> 00:03:29,470
We have the ability to choose a number of them.

53
00:03:29,469 --> 00:03:31,650
What is commonly used,

54
00:03:31,650 --> 00:03:37,715
what scikit-learn uses and what the paper proposes as one of the options is tanh.

55
00:03:37,715 --> 00:03:42,414
Hyper-tangent function.

56
00:03:42,414 --> 00:03:47,504
And the decorrelation is calculated like this.

57
00:03:47,504 --> 00:03:51,069
So, let's talk a little bit about it.

58
00:03:51,069 --> 00:03:53,034
So, since it might seem a little bit cryptic.

59
00:03:53,034 --> 00:03:56,305
So, ICA assumes a couple of things.

60
00:03:56,305 --> 00:04:00,849
It assumes that the components are statistically independent and

61
00:04:00,849 --> 00:04:06,574
the paper explains a little bit what that means in statistical language.

62
00:04:06,574 --> 00:04:11,669
And, it also assumes that components must have non-Gaussian distributions.

63
00:04:11,669 --> 00:04:14,952
And, non-Gaussianity is very important here.

64
00:04:14,953 --> 00:04:20,605
It's actually the key to estimating ICA and without it will not be able to calculate.

65
00:04:20,605 --> 00:04:26,415
We'll not be able to restore the original signals if they were Gaussian.

66
00:04:26,415 --> 00:04:28,600
And so, building from here,

67
00:04:28,600 --> 00:04:33,070
the central limit theorem tells us that the distribution of a sum of

68
00:04:33,069 --> 00:04:38,814
independent variables tends towards a Gaussian distribution.

69
00:04:38,814 --> 00:04:43,074
And so, knowing that we take W,

70
00:04:43,074 --> 00:04:45,639
this weight matrix here.

71
00:04:45,639 --> 00:04:52,814
We take it to be a matrix that maximizes the non-Gaussianity of W transpose X.

72
00:04:52,814 --> 00:04:58,279
And so, non-Gaussianity strikes here again.

73
00:04:58,279 --> 00:05:03,189
But here, it's actually we have to calculate non-Gaussianity because

74
00:05:03,189 --> 00:05:09,214
that's the term that this entire algorithm tries to maximize.

75
00:05:09,214 --> 00:05:14,149
And so, what's one way to calculate non-Gaussianity?

76
00:05:14,149 --> 00:05:19,389
This term here is an approximation of something called negentropy and negentropy

77
00:05:19,389 --> 00:05:25,314
comes from information theory where the idea of entropy comes from.

78
00:05:25,314 --> 00:05:30,964
And this is a way to sort of approximate it.

79
00:05:30,964 --> 00:05:33,669
You don't need to know all these details as long as you know

80
00:05:33,670 --> 00:05:37,249
the assumptions of non-Gaussianity.

81
00:05:37,249 --> 00:05:42,520
That the independent components have to be independent.

82
00:05:42,519 --> 00:05:47,620
Or else, I say we will not be able to find them.

83
00:05:47,620 --> 00:05:51,759
If you know these conditions and you know

84
00:05:51,759 --> 00:05:56,719
the situation where ICA needs to work, that's important.

85
00:05:56,720 --> 00:06:01,800
If you want to know the details of and look at the derivation,

86
00:06:01,800 --> 00:06:03,160
the paper is link in the-

