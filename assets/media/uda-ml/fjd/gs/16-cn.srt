1
00:00:00,000 --> 00:00:03,165
外部指标都很不错

2
00:00:03,165 --> 00:00:07,275
但它要求我们的数据集有标签

3
00:00:07,275 --> 00:00:09,025
但很多时候

4
00:00:09,025 --> 00:00:12,915
我们的数据集并没有标签

5
00:00:12,914 --> 00:00:16,289
更具体点 就是在非监督学习的情况下

6
00:00:16,289 --> 00:00:21,629
我们要借助于内部指标

7
00:00:21,629 --> 00:00:25,500
这些是内部指标的一些示例

8
00:00:25,500 --> 00:00:28,019
其中一些在 scikit-learn 中也有用到

9
00:00:28,019 --> 00:00:31,454
我们进一步来看轮廓系数

10
00:00:31,454 --> 00:00:36,299
它会给任何一个聚类在 -1 到 1 之间的值评分

11
00:00:36,299 --> 00:00:38,354
我们看个例子

12
00:00:38,354 --> 00:00:40,979
假如这是我们的数据集 我们对其进行聚类算法

13
00:00:40,979 --> 00:00:43,694
然后得到这个结果

14
00:00:43,695 --> 00:00:46,170
这个数据集是没有标签的

15
00:00:46,170 --> 00:00:51,885
所以我们不能与原始真相或原始标签进行比较

16
00:00:51,884 --> 00:00:55,777
我们必须要做并且能做的是

17
00:00:55,777 --> 00:01:01,109
借助内部指标并利用轮廓系数对其进行评分

18
00:01:01,109 --> 00:01:04,500
所以 计算轮廓系数的过程是这样

19
00:01:04,500 --> 00:01:07,140
每个点有一个轮廓系数

20
00:01:07,140 --> 00:01:08,670
对于数据集中的每个样本

21
00:01:08,670 --> 00:01:11,564
我们都可以计算轮廓系数

22
00:01:11,564 --> 00:01:14,594
计算公式为 b-a

23
00:01:14,594 --> 00:01:17,579
除以 a 和 b 中较大的一个

24
00:01:17,579 --> 00:01:20,715
所以 a 表示什么呢

25
00:01:20,715 --> 00:01:24,659
a 是同一个聚类中到其它样本的平均距离

26
00:01:24,659 --> 00:01:27,015
我们以这个点为例

27
00:01:27,015 --> 00:01:30,765
来计算这个点的轮廓系数

28
00:01:30,765 --> 00:01:35,099
由于 a 是同一个聚类中到其它样本的平均距离

29
00:01:35,099 --> 00:01:37,979
所以它等于这两个距离的平均值

30
00:01:37,980 --> 00:01:40,935
这就是 a 那么 b 又是什么呢

31
00:01:40,935 --> 00:01:47,445
b 是与它距离最近不同聚类中到样本的平均距离

32
00:01:47,444 --> 00:01:51,059
所以要得到 b 我们必须计算这个点

33
00:01:51,060 --> 00:01:55,290
到其它各个聚类中点的距离

34
00:01:55,290 --> 00:01:58,020
这个点到这个橘黄色的聚类之间的距离

35
00:01:58,019 --> 00:02:02,280
就是这个点到这两个橘黄色点距离的平均值

36
00:02:02,280 --> 00:02:05,969
这个点到绿色聚类的距离就是这个点到这两个绿色点之间距离的平均值

37
00:02:05,969 --> 00:02:10,169
我们进行比较可知

38
00:02:10,169 --> 00:02:12,554
这个绿色聚类位于这个点距离最近的聚类

39
00:02:12,555 --> 00:02:16,140
所以 b 等于这个点到这两个绿色点距离的平均值

40
00:02:16,139 --> 00:02:21,239
我们将 a、b 代入得到该点的轮廓系数

41
00:02:21,240 --> 00:02:24,600
我们计算聚类中每个点的轮廓系数

42
00:02:24,599 --> 00:02:29,370
然后取其平均值 就得到了整个聚类的轮廓系数

43
00:02:29,370 --> 00:02:33,719
我们来看一个轮廓系数的例子

44
00:02:33,719 --> 00:02:38,039
让它帮助我们找到聚类数据集的中聚类的最佳数量

45
00:02:38,039 --> 00:02:42,504
要算得 K 假如我们有这个

46
00:02:42,504 --> 00:02:45,504
原始的 没有标签的数据集

47
00:02:45,504 --> 00:02:51,549
如果我们进行 K 均值聚类且 K=2

48
00:02:51,550 --> 00:02:52,840
结果如下

49
00:02:52,840 --> 00:02:54,295
得到评分为

50
00:02:54,294 --> 00:02:57,280
0.798

51
00:02:57,280 --> 00:03:00,564
如果我们进行 K 均值聚类且 K=3

52
00:03:00,564 --> 00:03:01,870
结果如下

53
00:03:01,870 --> 00:03:05,064
得到的轮廓得分更高

54
00:03:05,064 --> 00:03:08,425
这更加符合我们的直觉

55
00:03:08,425 --> 00:03:10,225
如果 K=4 呢？

56
00:03:10,224 --> 00:03:15,340
我们希望 K=4 的轮廓得分比 K=3 的低

57
00:03:15,340 --> 00:03:18,439
结果确实如此

58
00:03:18,439 --> 00:03:22,204
其得分为 0.641

59
00:03:22,205 --> 00:03:23,915
所以在这三个 K 值中

60
00:03:23,914 --> 00:03:29,674
轮廓系数告诉我们 K=3 是最正确的选择

61
00:03:29,675 --> 00:03:35,660
这对我们来说是个好消息

62
00:03:35,659 --> 00:03:41,990
我们现在可以通过数学公式来验证对数据集中聚类数量的猜测了

63
00:03:41,990 --> 00:03:45,290
如果有多个我们无法想象并直观分裂的维度

64
00:03:45,289 --> 00:03:49,444
我们就可以这样做

65
00:03:49,444 --> 00:03:54,364
K=5 时的评分结果更糟糕 为 0.491

66
00:03:54,365 --> 00:03:57,509
当 K=4 或 K=5 时

67
00:03:57,508 --> 00:04:01,354
轮廓系数在进行罚分 因为在这样的聚类中

68
00:04:01,354 --> 00:04:04,429
这两个聚类之间的距离不够

69
00:04:04,430 --> 00:04:08,224
所以其结果为罚分 得分更低

70
00:04:08,224 --> 00:04:12,710
所以 在这个例子中 我们试着为 K 赋了四个值

71
00:04:12,710 --> 00:04:16,160
我们还可以为其赋更多的值 

72
00:04:16,160 --> 00:04:22,005
所以 这张图表显示的是 K 在 2 到 100 之间的得分

73
00:04:22,004 --> 00:04:29,889
我们可以看到 无论我们将数据集分割成多少个类

74
00:04:29,889 --> 00:04:33,264
K=3 时的轮廓得分最高

75
00:04:33,264 --> 00:04:36,759
我们再来看一个例子

76
00:04:36,759 --> 00:04:40,060
我们能很明显的发现 K=2 或 3

77
00:04:40,060 --> 00:04:43,030
是这个数据集最好的 K 值

78
00:04:43,029 --> 00:04:47,739
不管我们再如何增大聚类的数量

79
00:04:47,740 --> 00:04:53,530
轮廓得分都没有 K=2 或 3 时的好

80
00:04:53,529 --> 00:04:56,844
我们也可以利用轮廓系数来比较聚类算法

81
00:04:56,845 --> 00:05:00,345
比较它们对一个特定数据集的效果

82
00:05:00,345 --> 00:05:02,940
来看这个例子

83
00:05:02,939 --> 00:05:06,389
我们进行 K 均值聚类 得分为 0.801

84
00:05:06,389 --> 00:05:09,336
单连接算法的得分也一样

85
00:05:09,336 --> 00:05:11,729
全连接算法 Ward 最小方差法 DBSCAN

86
00:05:11,730 --> 00:05:15,435
其结果和得分都一样

87
00:05:15,435 --> 00:05:17,399
但我们来看看这个数据集

88
00:05:17,399 --> 00:05:22,199
进行 K 均值聚类 其得分为 0.637

89
00:05:22,199 --> 00:05:25,289
看起来还可以 但我们来看看单连接算法

90
00:05:25,290 --> 00:05:29,610
其得分接近于 0 

91
00:05:29,610 --> 00:05:34,580
一个聚类几乎占完了整个数据集 还不错

92
00:05:34,579 --> 00:05:36,639
我们可以放心了

93
00:05:36,639 --> 00:05:39,819
全连接算法也还不错

94
00:05:39,819 --> 00:05:42,519
Ward 最小方差法得分更好 DBSCAN...

95
00:05:42,519 --> 00:05:45,654
所以这个例子告诉我们 

96
00:05:45,654 --> 00:05:49,044
当我们使用 DBSCAN 时

97
00:05:49,045 --> 00:05:51,265
我们不应该使用轮廓系数

98
00:05:51,264 --> 00:05:56,860
更具体的说 它没有噪音的概念

99
00:05:56,860 --> 00:06:02,139
而 DBSCAN 并不总是倾向于

100
00:06:02,139 --> 00:06:04,870
这些紧凑的聚类

101
00:06:04,870 --> 00:06:09,879
这些聚类可以通过轮廓系数奖励聚类算法来获得结果

102
00:06:09,879 --> 00:06:12,310
我们可以发现与 GMM 类似的东西

103
00:06:12,310 --> 00:06:17,245
我认为这是一种很优美的分割方式

104
00:06:17,245 --> 00:06:19,240
尽管这是第四好的得分

105
00:06:19,240 --> 00:06:21,985
但是我非常喜欢这种分割

106
00:06:21,985 --> 00:06:26,410
最后一个例子显示了轮廓系数的一些缺点

107
00:06:26,410 --> 00:06:28,510
如果我们想

108
00:06:28,509 --> 00:06:33,939
使用两个环形的数据集 并与不同的聚类算法进行比较

109
00:06:33,939 --> 00:06:38,245
我们对其进行 K 均值聚类 得分为 0.35 

110
00:06:38,245 --> 00:06:40,795
但 Single Link 将其完美地分割 结果如下

111
00:06:40,795 --> 00:06:43,465
但其得分比 K 均值的低

112
00:06:43,464 --> 00:06:47,364
这是因为轮廓系数并不会奖励这样地分割数据集 

113
00:06:47,365 --> 00:06:51,879
或者这样的聚类

114
00:06:51,879 --> 00:06:55,540
它想找到从其它类分割开的

115
00:06:55,540 --> 00:06:59,740
那些紧凑 密集的环形类

116
00:06:59,740 --> 00:07:02,905
所以 如果你的数据外形或你想分割成的聚类外形是这样的

117
00:07:02,904 --> 00:07:06,849
使用轮廓系数的效果并不是很好

118
00:07:06,850 --> 00:07:12,355
尽管全连接的聚类很糟糕 但其得分稍好一点

119
00:07:12,355 --> 00:07:15,730
Ward 也是这样 DBSCAN 分割得很完美

120
00:07:15,730 --> 00:07:19,660
分割得非常漂亮 得分接近于 0

121
00:07:19,660 --> 00:07:21,860
这又是另一个证明我们处理 DBSCAN 时

122
00:07:21,860 --> 00:07:25,235
不应使用轮廓系数的例子

123
00:07:25,235 --> 00:07:28,475
GMM 和 K 均值的得分很相近

124
00:07:28,475 --> 00:07:34,310
所以处理 DBSCAN 时 不要使用轮廓系数

125
00:07:34,310 --> 00:07:36,290
但还有另一个算法

126
00:07:36,290 --> 00:07:38,840
在下面链接的论文里

127
00:07:38,839 --> 00:07:44,074
介绍了基于密度的聚类评价的内部指标

128
00:07:44,074 --> 00:07:47,014
其名为 DBCV 即 density-based clustering validation

129
00:07:47,014 --> 00:07:54,199
它在处理 DBSCAN 时的效果更好

