1
00:00:00,000 --> 00:00:04,769
The third step of the expectation maximization algorithm for

2
00:00:04,769 --> 00:00:10,251
GMM clustering is to take whatever what was produced in step two,

3
00:00:10,252 --> 00:00:15,319
which is the memberships of all the points to all the clusters,

4
00:00:15,319 --> 00:00:16,920
the two clusters that we have,

5
00:00:16,920 --> 00:00:22,480
and use those to come up with new parameters for the Gaussians.

6
00:00:22,480 --> 00:00:25,530
And so, the entire purpose of step three is

7
00:00:25,530 --> 00:00:28,560
to use whatever is step two produced as input,

8
00:00:28,559 --> 00:00:31,469
and the goal is to fill up this small table,

9
00:00:31,469 --> 00:00:33,179
which is to say for Cluster A,

10
00:00:33,179 --> 00:00:36,750
we need the new mean and a new variance,

11
00:00:36,750 --> 00:00:38,715
and we do the same with Cluster B.

12
00:00:38,715 --> 00:00:43,935
So, all that it is is just coming up with these four values here.

13
00:00:43,935 --> 00:00:46,770
So, let's go and do that right now.

14
00:00:46,770 --> 00:00:49,710
And so, the new mean for Cluster A,

15
00:00:49,710 --> 00:00:52,259
given the result of step two,

16
00:00:52,259 --> 00:00:56,804
comes from calculating the weighted average of all of these points.

17
00:00:56,804 --> 00:01:02,144
The weighted average does not only account for the parameters of each point,

18
00:01:02,145 --> 00:01:05,825
but also account for how much it belongs.

19
00:01:05,825 --> 00:01:07,745
So it's membership to Cluster A,

20
00:01:07,745 --> 00:01:09,745
and that's these terms here.

21
00:01:09,745 --> 00:01:11,189
And so let's calculate that.

22
00:01:11,189 --> 00:01:13,784
So, this is the table resulting from step two,

23
00:01:13,784 --> 00:01:18,959
where we filled up all the memberships for all the points in both clusters.

24
00:01:18,959 --> 00:01:21,839
And the numbers are slightly different here because this is

25
00:01:21,840 --> 00:01:27,155
the correct calculation where we accounted for the mixing coefficient.

26
00:01:27,155 --> 00:01:29,730
This value here is just arbitrary.

27
00:01:29,730 --> 00:01:34,350
I added it just to showcase that it's not always 99 percent accuracy.

28
00:01:34,349 --> 00:01:39,144
It can be 55 and 40 and 30 and anything between zero and one.

29
00:01:39,144 --> 00:01:43,439
And so, this is how we calculate the weighted average.

30
00:01:43,439 --> 00:01:47,564
And it's basically going to each row here and multiplying

31
00:01:47,564 --> 00:01:53,549
this cell by an array or a vector containing these two values,

32
00:01:53,549 --> 00:01:55,064
and that would be this term.

33
00:01:55,064 --> 00:01:57,138
And then we do that for each row,

34
00:01:57,138 --> 00:02:00,314
summing them up and then that would be this entire term.

35
00:02:00,314 --> 00:02:03,599
And then, we sum up the values of this column,

36
00:02:03,599 --> 00:02:06,044
and that would be what we divide by.

37
00:02:06,045 --> 00:02:08,340
And then we calculate that,

38
00:02:08,340 --> 00:02:11,295
and we get the you mean for Cluster A.

39
00:02:11,294 --> 00:02:15,030
We do the same for a Cluster B, except here,

40
00:02:15,030 --> 00:02:18,405
we'll be looking at the weighted average of these points here,

41
00:02:18,405 --> 00:02:21,175
and the calculation is almost the same.

42
00:02:21,175 --> 00:02:22,875
And so, we have the new values.

43
00:02:22,875 --> 00:02:24,694
To calculate the new variance,

44
00:02:24,694 --> 00:02:26,310
we do something similar.

45
00:02:26,310 --> 00:02:28,229
So this term here,

46
00:02:28,229 --> 00:02:35,429
this looks eerily similar to the regular calculation of how variance is calculated,

47
00:02:35,430 --> 00:02:37,830
except here, we're doing a weighted version of it.

48
00:02:37,830 --> 00:02:44,070
So we're multiplying here by the membership to each cluster.

49
00:02:44,069 --> 00:02:48,344
And so, if we plug in the data that we got from step two,

50
00:02:48,344 --> 00:02:53,655
we can get a new value for the new variance for Cluster A,

51
00:02:53,655 --> 00:02:57,150
and we can do the same with Cluster B as well.

52
00:02:57,150 --> 00:02:59,409
And so now step three is complete.

53
00:02:59,409 --> 00:03:02,574
We have the two new Gaussians,

54
00:03:02,574 --> 00:03:05,375
and we can compare them with the old Gaussians.

55
00:03:05,375 --> 00:03:07,080
Let's look at this graph here.

56
00:03:07,080 --> 00:03:08,450
So these are the old Gaussians.

57
00:03:08,449 --> 00:03:12,491
And so, this is an overlay of the old and the new Gaussians.

58
00:03:12,491 --> 00:03:15,149
And we can see that the green Gaussian started

59
00:03:15,150 --> 00:03:18,105
moving to the top and right here a little bit.

60
00:03:18,104 --> 00:03:20,209
So this is one step.

61
00:03:20,210 --> 00:03:21,300
Now, if it doesn't converge,

62
00:03:21,300 --> 00:03:23,550
we'll continue to do five, 10, 20,

63
00:03:23,550 --> 00:03:27,510
30 steps like this until we converge into

64
00:03:27,509 --> 00:03:32,909
a Gaussian mixture that produces a good result for the problem.

65
00:03:32,909 --> 00:03:37,210
The fourth step is to evaluate the log-likelihood.

66
00:03:37,210 --> 00:03:42,890
And the log-likelihood sums for all clusters here.

67
00:03:42,889 --> 00:03:46,657
This is the mixing coefficient related to this cluster,

68
00:03:46,657 --> 00:03:49,934
and then this is the normal distribution of each point,

69
00:03:49,935 --> 00:03:53,039
given the parameter of the cluster,

70
00:03:53,039 --> 00:03:57,689
and this term as a whole is called log-likelihood.

71
00:03:57,689 --> 00:04:02,264
And basically, what this says is that the higher this number is,

72
00:04:02,264 --> 00:04:06,884
the more sure we are that the mixer model that we've generated

73
00:04:06,884 --> 00:04:11,844
is responsible for creating the data or fits the dataset that we have.

74
00:04:11,844 --> 00:04:16,214
And so, the purpose here is to maximize this value, this term.

75
00:04:16,214 --> 00:04:21,301
We do that by choosing the parameters of the Gaussians,

76
00:04:21,302 --> 00:04:24,135
including the mixing coefficient,

77
00:04:24,134 --> 00:04:27,824
the mean, and the variance of each Gaussian.

78
00:04:27,824 --> 00:04:30,104
The better parameters we pick,

79
00:04:30,105 --> 00:04:32,830
the higher the value of this entire term will be.

80
00:04:32,829 --> 00:04:35,954
And then we do that until the algorithm converges,

81
00:04:35,954 --> 00:04:42,714
until it reaches a maximum or it starts to increase by a tiny fraction each step.

82
00:04:42,714 --> 00:04:45,558
And so, that's where we can stop the algorithm and say,

83
00:04:45,559 --> 00:04:47,750
"Alright. We have converged.

84
00:04:47,750 --> 00:04:51,269
Let's choose those Gaussians as

85
00:04:51,269 --> 00:04:55,745
the components of a Gaussian mixture model and that's cluster based on them."

86
00:04:55,745 --> 00:04:57,540
So, this is the final step of

87
00:04:57,540 --> 00:05:02,615
the expectation maximization algorithm to do GMM clustering.

88
00:05:02,615 --> 00:05:05,060
In the next video, we'll look at a visual example,

89
00:05:05,060 --> 00:05:08,939
where we'll visualize step number one and two and three,

90
00:05:08,939 --> 00:05:12,000
the entire process until convergence.

