1
00:00:00,000 --> 00:00:04,304
回到具有权重和输入的神经网络

2
00:00:04,304 --> 00:00:08,745
具有上标 1 的权重属于第一层级

3
00:00:08,743 --> 00:00:12,618
具有上标 2 的权重属于第二层级

4
00:00:12,618 --> 00:00:15,384
偏差不再叫做 b

5
00:00:15,384 --> 00:00:18,434
现在叫做 W31 W32 等等

6
00:00:18,434 --> 00:00:22,679
这样是为了用矩阵表示所有内容

7
00:00:22,678 --> 00:00:25,189
输入会怎样？

8
00:00:25,190 --> 00:00:27,859
我们开始执行前向反馈流程

9
00:00:27,859 --> 00:00:30,169
在第一层级

10
00:00:30,170 --> 00:00:33,590
我们用输入乘以权重

11
00:00:33,590 --> 00:00:37,720
得出 h1 它是输入和权重的线性方程

12
00:00:37,719 --> 00:00:42,149
同样 使用这个公式得出 h2

13
00:00:42,149 --> 00:00:43,399
在第二层级

14
00:00:43,399 --> 00:00:46,460
我们使用 h1 h2 和偏差

15
00:00:46,460 --> 00:00:48,649
并应用 s 型函数

16
00:00:48,649 --> 00:00:51,739
然后对它们应用线性方程

17
00:00:51,740 --> 00:00:55,445
将它们乘以权重并相加 得出值 h

18
00:00:55,445 --> 00:00:58,969
最后 在第三层级

19
00:00:58,969 --> 00:01:03,039
我们对 h 应用 s 型函数 得出预测

20
00:01:03,039 --> 00:01:06,879
即在 0 到 1 之间的概率 用 ŷ 表示

21
00:01:06,879 --> 00:01:10,640
我们可以写成更简练的形式

22
00:01:10,640 --> 00:01:15,530
第一层级对应的矩阵是 W(1)

23
00:01:15,530 --> 00:01:20,060
第二层级对应的矩阵是 W(2)

24
00:01:20,060 --> 00:01:23,605
预测 ŷ 是 W(2) 的 s 型函数

25
00:01:23,605 --> 00:01:28,734
与 W(1) 的 s 型函数相结合

26
00:01:28,733 --> 00:01:33,539
应用到输入 x 上 这就是前向反馈

27
00:01:33,540 --> 00:01:39,015
现在我们将形成反向传播 正好是前向反馈的逆过程

28
00:01:39,015 --> 00:01:43,019
我们将使用链式法则计算这些误差函数

29
00:01:43,019 --> 00:01:48,024
相对于标签中每个权重的导数

30
00:01:48,025 --> 00:01:51,120
我们将使用链式法则计算误差函数

31
00:01:51,120 --> 00:01:55,719
相对于标签中每个权重的导数

32
00:01:55,718 --> 00:01:58,678
我们的误差函数是这个公式

33
00:01:58,680 --> 00:02:02,870
即预测函数 ŷ

34
00:02:02,870 --> 00:02:07,579
但是预测是所有权重 Wij 的函数

35
00:02:07,578 --> 00:02:13,560
那么误差函数可以看做所有 Wij 的函数

36
00:02:13,560 --> 00:02:16,965
因此梯度就是误差函数 E

37
00:02:16,965 --> 00:02:23,520
相对于每个权重的偏导数形成的向量

38
00:02:23,520 --> 00:02:25,870
我们来计算其中一个导数

39
00:02:25,870 --> 00:02:31,465
我们计算下 E 相对于 W11(1) 的导数

40
00:02:31,465 --> 00:02:33,450
因为预测就是函数组合

41
00:02:33,449 --> 00:02:35,639
根据链式法则

42
00:02:35,639 --> 00:02:41,519
我们知道这一部分的导数就是所有偏导数的积

43
00:02:41,520 --> 00:02:44,820
这里 导数 E 相对于 W11

44
00:02:44,818 --> 00:02:49,448
是 E 相对于 ŷ 的导数

45
00:02:49,449 --> 00:02:52,875
乘以 ŷ 相对于 h 的导数

46
00:02:52,875 --> 00:02:59,650
乘以 h 相对于 h1 的导数 乘以 hi 相对于 W11 的导数

47
00:02:59,650 --> 00:03:03,780
看起来似乎有点复杂

48
00:03:03,780 --> 00:03:06,360
但是我们能够通过将四个偏导数相乘

49
00:03:06,360 --> 00:03:10,230
得出这么一个复杂的组合函数 简直太神奇

50
00:03:10,229 --> 00:03:12,368
我们已经计算出了第一部分

51
00:03:12,370 --> 00:03:14,955
即 E 相对于 ŷ 的导数

52
00:03:14,955 --> 00:03:17,840
结果是 ŷ - y

53
00:03:17,840 --> 00:03:20,378
我们计算下其他几部分

54
00:03:20,378 --> 00:03:24,899
稍微放大下 只看多层感知器的一个部分

55
00:03:24,900 --> 00:03:31,085
输入是值 h1 和 h2 它们是来自前面的值

56
00:03:31,085 --> 00:03:34,895
对 h1 h2 及偏置单元对应的 1

57
00:03:34,895 --> 00:03:39,034
应用 s 型函数和线性方程后

58
00:03:39,033 --> 00:03:40,968
得到结果 h

59
00:03:40,968 --> 00:03:45,068
h 相对于 h1 的导数是多少？

60
00:03:45,068 --> 00:03:51,113
h 是三项的和 其中只有一项包含 h1

61
00:03:51,115 --> 00:03:55,830
所以第二和第三项得出导数 0

62
00:03:55,830 --> 00:04:03,330
第一项得出 W11(2) 因为这是常量

63
00:04:03,330 --> 00:04:09,475
该常量乘以 s 型函数相对于 h1 的导数

64
00:04:09,473 --> 00:04:12,603
我们在下面的讲师注释中计算了这部分

65
00:04:12,604 --> 00:04:16,865
s 型函数拥有完美的导数

66
00:04:16,865 --> 00:04:20,764
即 σ(h) 的导数正好是

67
00:04:20,764 --> 00:04:23,598
σ(h*(1-σ(h))

68
00:04:23,600 --> 00:04:27,860
你可以在下方的讲师注释中看到演算过程

69
00:04:27,860 --> 00:04:30,650
你还可以在测验中将公式编写为代码

70
00:04:30,649 --> 00:04:34,699
因为最终我们会将这些公式编写成代码 然后一直使用它们

71
00:04:34,699 --> 00:04:37,120
讲完了 神经网络就是这么训练的

