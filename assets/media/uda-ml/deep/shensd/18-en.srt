1
00:00:00,000 --> 00:00:04,674
Well, the first observation is that both equations give us the same line.

2
00:00:04,674 --> 00:00:08,894
The line with equation x_1 plus x_2 equals zero.

3
00:00:08,894 --> 00:00:11,417
And the reason for this is that solution two

4
00:00:11,417 --> 00:00:15,160
is really just a scalar multiple of solution one.

5
00:00:15,160 --> 00:00:18,905
So let's see. Recall that the prediction is a sigmoid of the linear function.

6
00:00:18,905 --> 00:00:21,105
So in the first case, for the point 1, 1,

7
00:00:21,105 --> 00:00:23,280
it would be sigmoid of one plus one,

8
00:00:23,280 --> 00:00:27,580
which is sigmoid of two, which is 0.88.

9
00:00:27,579 --> 00:00:30,945
This is not bad since the point is blue so it has a label of one.

10
00:00:30,945 --> 00:00:35,738
For the point -1, -1, the prediction is sigmoid of -1 plus -1,

11
00:00:35,738 --> 00:00:41,134
which is sigmoid of -2, which is 0.12.

12
00:00:41,134 --> 00:00:45,349
This is also not bad since the point has a label of zero since it's red.

13
00:00:45,350 --> 00:00:48,020
Now, let's see what happens with the second model.

14
00:00:48,020 --> 00:00:53,540
The point 1,1, has a prediction sigmoid of 10 times one plus 10 times one,

15
00:00:53,539 --> 00:00:55,810
which is sigmoid of 20.

16
00:00:55,810 --> 00:01:04,210
This is 0.9999999979, which is really close to 1,

17
00:01:04,209 --> 00:01:06,069
so it's a great prediction.

18
00:01:06,069 --> 00:01:07,728
And the point -1, -1,

19
00:01:07,728 --> 00:01:13,849
has prediction sigmoid of 10 times -1 plus 10 times -1,

20
00:01:13,849 --> 00:01:16,459
which is sigmoid of -20,

21
00:01:16,459 --> 00:01:23,694
and that is 0.0000000021.

22
00:01:23,694 --> 00:01:25,514
That's really, really close to zero,

23
00:01:25,515 --> 00:01:27,320
so it's a great prediction.

24
00:01:27,319 --> 00:01:30,109
So the answer to the quiz is the second model.

25
00:01:30,109 --> 00:01:32,109
The second model is super accurate.

26
00:01:32,109 --> 00:01:33,909
This means it's better, right?

27
00:01:33,909 --> 00:01:35,179
Well, after the last section,

28
00:01:35,180 --> 00:01:39,040
you may be a bit reluctant since this hints a bit towards overfitting.

29
00:01:39,040 --> 00:01:40,945
And your hunch is correct.

30
00:01:40,944 --> 00:01:43,504
The problem is overfitting but in a subtle way.

31
00:01:43,504 --> 00:01:46,759
Here's what's happening and here's why the first model is better,

32
00:01:46,760 --> 00:01:49,040
even if it gives a larger error.

33
00:01:49,040 --> 00:01:54,250
When we apply sigmoid to small values such as x_1 plus x_2,

34
00:01:54,250 --> 00:01:56,375
we get the function on the left,

35
00:01:56,375 --> 00:01:59,921
which has a nice slope to the gradient descent.

36
00:01:59,921 --> 00:02:08,004
When we multiply the linear function by 10 and take sigmoid of 10_x_1 plus 10_x_2,

37
00:02:08,004 --> 00:02:11,909
our predictions are much better since they're closer to zero and one,

38
00:02:11,909 --> 00:02:16,199
but the function becomes much steeper and it's much harder to do

39
00:02:16,199 --> 00:02:20,789
gradient descent here since the derivatives are mostly close to zero,

40
00:02:20,789 --> 00:02:24,659
and they're very large when we get to the middle of the curve.

41
00:02:24,659 --> 00:02:27,930
Therefore, in order to do gradient descent properly,

42
00:02:27,930 --> 00:02:33,270
we want a model like the one in the left more than a model like the one in the right.

43
00:02:33,270 --> 00:02:35,219
In a conceptual way,

44
00:02:35,219 --> 00:02:38,134
the model in the right is too certain,

45
00:02:38,134 --> 00:02:41,299
and it gives little room for applying gradient descent.

46
00:02:41,300 --> 00:02:42,540
Also, as we can imagine,

47
00:02:42,539 --> 00:02:44,836
the points that are classified incorrectly in the model

48
00:02:44,836 --> 00:02:47,579
in the right will generate large errors,

49
00:02:47,580 --> 00:02:51,230
and it would be hard to tune the model to correct them.

50
00:02:51,229 --> 00:02:54,569
These can be summarized in the quote by the famous philosopher

51
00:02:54,569 --> 00:02:58,173
and mathematician, Bertrand Russell.

52
00:02:58,173 --> 00:02:59,310
The whole problem with

53
00:02:59,310 --> 00:03:03,944
artificial intelligence is that bad models are so certain of themselves,

54
00:03:03,944 --> 00:03:07,629
and good models are so full of doubts.

55
00:03:07,629 --> 00:03:08,680
Now the question is,

56
00:03:08,680 --> 00:03:12,670
how do we prevent this type of overfitting from happening?

57
00:03:12,669 --> 00:03:16,919
This seems to not be easy since the bad model gives smaller errors.

58
00:03:16,919 --> 00:03:20,859
Well, all we have to do is we have to tweak the error function a bit.

59
00:03:20,860 --> 00:03:24,190
Basically, we want to punish high coefficients.

60
00:03:24,189 --> 00:03:29,409
So what we do is we take the old error function and add a term,

61
00:03:29,409 --> 00:03:32,560
which is big when the weights are big.

62
00:03:32,560 --> 00:03:34,569
There are two ways to do this.

63
00:03:34,569 --> 00:03:39,049
One way is to add the sums of absolute values of the weights

64
00:03:39,050 --> 00:03:41,010
times a constant lambda.

65
00:03:41,009 --> 00:03:46,699
The other one is to add the sum of the squares of the weights times that same constant.

66
00:03:46,699 --> 00:03:51,500
As you can see, these two are large if the weights are large.

67
00:03:51,500 --> 00:03:56,009
The lambda parameter will tell us how much we want to penalize the coefficients.

68
00:03:56,009 --> 00:03:57,409
If lambda is large,

69
00:03:57,409 --> 00:03:58,895
we penalize them a lot,

70
00:03:58,895 --> 00:04:00,240
and if Lambda is small,

71
00:04:00,240 --> 00:04:02,420
then we don't penalize them much.

72
00:04:02,419 --> 00:04:05,664
And finally, if we decide to go for the absolute values,

73
00:04:05,664 --> 00:04:08,509
we're doing L1 regularization.

74
00:04:08,509 --> 00:04:10,949
And if we decide to go for the squares,

75
00:04:10,949 --> 00:04:13,634
then we're doing L2 regularization.

76
00:04:13,634 --> 00:04:15,280
Both are very popular,

77
00:04:15,280 --> 00:04:16,725
and depending on our goals,

78
00:04:16,725 --> 00:04:21,840
our application will be applying one or the other.

79
00:04:21,839 --> 00:04:27,185
Here are some general guidelines for deciding between L1 and L2 regularization.

80
00:04:27,185 --> 00:04:32,165
When we apply L1, we tend to end up with sparse vectors.

81
00:04:32,165 --> 00:04:36,355
That means small weights will tend to go to zero.

82
00:04:36,355 --> 00:04:40,140
So if you want to reduce the number of weights and end up with a small set,

83
00:04:40,139 --> 00:04:42,334
we can use L1.

84
00:04:42,334 --> 00:04:45,109
This is also good for feature selections since sometimes,

85
00:04:45,110 --> 00:04:47,970
we have a problem with hundreds of features and

86
00:04:47,970 --> 00:04:51,995
L1 regularization will help us select which ones are important.

87
00:04:51,995 --> 00:04:54,735
And it will turn the rest into zeroes.

88
00:04:54,735 --> 00:04:56,915
L2, on the other hand,

89
00:04:56,915 --> 00:04:59,840
tends not to favor sparse vectors since it tries

90
00:04:59,839 --> 00:05:03,185
to maintain all the weights homogeneously small.

91
00:05:03,185 --> 00:05:06,319
This one normally gives better results for training models so

92
00:05:06,319 --> 00:05:09,790
it's the one we'll use the most. Now, let's think a bit.

93
00:05:09,790 --> 00:05:14,105
Why would L1 regularization produce vectors with sparse weights and

94
00:05:14,105 --> 00:05:18,800
L2 regularization will produce vectors with small homogeneous weights?

95
00:05:18,800 --> 00:05:20,785
Well, here's an idea of why.

96
00:05:20,785 --> 00:05:22,478
If we take the vector 1, 0,

97
00:05:22,478 --> 00:05:26,389
the sums of the absolute values of the weights are one,

98
00:05:26,389 --> 00:05:30,779
and the sums of the squares of the weights are also one.

99
00:05:30,779 --> 00:05:35,424
But if we take the vector 0.5, 0.5,

100
00:05:35,425 --> 00:05:38,875
the sums of the absolute values of the weights is still one,

101
00:05:38,875 --> 00:05:46,410
but the sums of the squares is 0.25 plus 0.25, which is 0.5.

102
00:05:46,410 --> 00:05:50,930
Thus, L2 regularization will prefer the vector 0.5,

103
00:05:50,930 --> 00:05:52,900
0.5, over the vector 1,

104
00:05:52,899 --> 00:05:57,954
0, since this one produces a smaller sum of squares and in turn,

105
00:05:57,954 --> 00:06:00,000
a smaller error function.

