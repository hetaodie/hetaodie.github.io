1
00:00:00,000 --> 00:00:04,674
第一个观点是两个方程给我们相同的直线

2
00:00:04,674 --> 00:00:08,894
这条直线方程是 X1 + X2 = 0

3
00:00:08,894 --> 00:00:11,417
原因在于第二个解法

4
00:00:11,417 --> 00:00:15,160
实际上只是第一个解法的纯量倍数

5
00:00:15,160 --> 00:00:18,905
那么我们来看一下 回想一下 预测是一个 sigmoid 线性函数

6
00:00:18,905 --> 00:00:21,105
所以在第一个例子中 对于点 (1,1)

7
00:00:21,105 --> 00:00:23,280
可以是 sigmoid (1+1)

8
00:00:23,280 --> 00:00:27,580
即 sigmoid 2 等于 0.88

9
00:00:27,579 --> 00:00:30,945
因为这个点是蓝色的 所以结果不算差 所以标签为 1

10
00:00:30,945 --> 00:00:35,738
对于点 (-1,-1) 预测是 sigmoid (-1-1)

11
00:00:35,738 --> 00:00:41,134
即 sigmoid -2 等于 0.12

12
00:00:41,134 --> 00:00:45,349
因为这个点标签为 0 是红色的 所以结果不算差

13
00:00:45,350 --> 00:00:48,020
我们来看一下第二个模型

14
00:00:48,020 --> 00:00:53,540
点 (1,1) 预测结果是 sigmoid (10+10)

15
00:00:53,539 --> 00:00:55,810
即 sigmoid 20

16
00:00:55,810 --> 00:01:04,210
结果为 0.9999999979 非常接近 1

17
00:01:04,209 --> 00:01:06,069
所以这是个很好的预测

18
00:01:06,069 --> 00:01:07,728
对于点 (-1,-1)

19
00:01:07,728 --> 00:01:13,849
预测结果是 sigmoid (-10-10)

20
00:01:13,849 --> 00:01:16,459
即 sigmoid -20

21
00:01:16,459 --> 00:01:23,694
等于 0.0000000021

22
00:01:23,694 --> 00:01:25,514
这非常接近 0

23
00:01:25,515 --> 00:01:27,320
所以这是个很好的预测

24
00:01:27,319 --> 00:01:30,109
所以这个测试的答案是第二个模型

25
00:01:30,109 --> 00:01:32,109
第二个模型非常准确

26
00:01:32,109 --> 00:01:33,909
也意味着更好

27
00:01:33,909 --> 00:01:35,179
在最后一部分

28
00:01:35,180 --> 00:01:39,040
因为这可能有点过度拟合 所以你觉得有点勉强

29
00:01:39,040 --> 00:01:40,945
而且你的预感是正确的

30
00:01:40,944 --> 00:01:43,504
这个问题有点过度拟合

31
00:01:43,504 --> 00:01:46,759
这是整个过程 以及第一个模型更好的原因

32
00:01:46,760 --> 00:01:49,040
即使得出更大的误差

33
00:01:49,040 --> 00:01:54,250
我们把 sigmoid 运用到较小值中 如 X1 + X2

34
00:01:54,250 --> 00:01:56,375
我们得到左侧的函数

35
00:01:56,375 --> 00:01:59,921
包含梯度下降法的较好斜率

36
00:01:59,921 --> 00:02:08,004
我们把线性函数乘以 10 后得到 sigmoid (10X1+10X2)

37
00:02:08,004 --> 00:02:11,909
由于结果更趋近于 0 和 1 所以预测更好一些

38
00:02:11,909 --> 00:02:16,199
但是函数更加陡峭 这里也更难使用

39
00:02:16,199 --> 00:02:20,789
梯度下降法 因为导数几乎接近 0

40
00:02:20,789 --> 00:02:24,659
到达曲线中部时 导数非常大

41
00:02:24,659 --> 00:02:27,930
所以 为了合理使用梯度下降法

42
00:02:27,930 --> 00:02:33,270
相比右侧的模型来说 我们更想用左侧的模型

43
00:02:33,270 --> 00:02:35,219
从概念上来说

44
00:02:35,219 --> 00:02:38,134
右侧的模型非常稳定

45
00:02:38,134 --> 00:02:41,299
很难应用梯度下降法

46
00:02:41,300 --> 00:02:42,540
同时正如我们想象到的

47
00:02:42,539 --> 00:02:44,836
这些点在右侧模型中分类错误

48
00:02:44,836 --> 00:02:47,579
会导致更大的偏差

49
00:02:47,580 --> 00:02:51,230
这样会很难调优模型或纠正模型

50
00:02:51,229 --> 00:02:54,569
这可以引用著名哲学家和数学家伯特兰·罗素

51
00:02:54,569 --> 00:02:58,173
的名言进行总结

52
00:02:58,173 --> 00:02:59,310
人工智能的问题

53
00:02:59,310 --> 00:03:03,944
在于错误模型对自身非常确定

54
00:03:03,944 --> 00:03:07,629
而好的模型充满疑问

55
00:03:07,629 --> 00:03:08,680
现在问题是

56
00:03:08,680 --> 00:03:12,670
我们如何避免这种过度拟合的发生呢？

57
00:03:12,669 --> 00:03:16,919
由于错误模型提供了更小误差 所以这并不简单

58
00:03:16,919 --> 00:03:20,859
我们要做的是对误差函数稍作调整

59
00:03:20,860 --> 00:03:24,190
大体来说 我们想要惩罚高系数

60
00:03:24,189 --> 00:03:29,409
那么我们要做的是 利用原来的误差函数 增加一项

61
00:03:29,409 --> 00:03:32,560
如果权重大 那么这项也大

62
00:03:32,560 --> 00:03:34,569
两种方法可以做到这些

63
00:03:34,569 --> 00:03:39,049
第一种方法是加上权重绝对值的总和

64
00:03:39,050 --> 00:03:41,010
再乘以常数 λ

65
00:03:41,009 --> 00:03:46,699
另一种方法是加上权重平方总和 再乘以相同的常数

66
00:03:46,699 --> 00:03:51,500
可以看到 如果权重大 这两个函数都大

67
00:03:51,500 --> 00:03:56,009
参数 λ 告诉我们惩罚系数的多少

68
00:03:56,009 --> 00:03:57,409
如果 λ 很大

69
00:03:57,409 --> 00:03:58,895
会惩罚很严重

70
00:03:58,895 --> 00:04:00,240
如果 λ 很小

71
00:04:00,240 --> 00:04:02,420
那么不需要惩罚很严重

72
00:04:02,419 --> 00:04:05,664
最后如果我们想要得到绝对值

73
00:04:05,664 --> 00:04:08,509
可以使用 L1 正则化

74
00:04:08,509 --> 00:04:10,949
如果想要得到平方

75
00:04:10,949 --> 00:04:13,634
可以使用 L2 正则化

76
00:04:13,634 --> 00:04:15,280
两者都比较通用

77
00:04:15,280 --> 00:04:16,725
取决于我们的目标

78
00:04:16,725 --> 00:04:21,840
我们可以应用其中一个或另一个

79
00:04:21,839 --> 00:04:27,185
对于使用 L1 和 L2 正则化 这里有一些通用规则

80
00:04:27,185 --> 00:04:32,165
使用 L1 正则化时 我们希望得到稀疏向量

81
00:04:32,165 --> 00:04:36,355
它表示较小权重趋向于 0

82
00:04:36,355 --> 00:04:40,140
所以如果你想降低权重值 最终得到较小的数

83
00:04:40,139 --> 00:04:42,334
我们可以使用 L1

84
00:04:42,334 --> 00:04:45,109
这也利于特征选择 因为有时候

85
00:04:45,110 --> 00:04:47,970
我们会遇到几百种特征问题

86
00:04:47,970 --> 00:04:51,995
L1 正则化可以帮我们选择哪一些更重要

87
00:04:51,995 --> 00:04:54,735
然后将其余的变为 0

88
00:04:54,735 --> 00:04:56,915
而 L2 正则化

89
00:04:56,915 --> 00:04:59,840
不支持稀疏向量

90
00:04:59,839 --> 00:05:03,185
因为它确保所有权重一致较小

91
00:05:03,185 --> 00:05:06,319
这个一般可以训练模型 得出更好结果

92
00:05:06,319 --> 00:05:09,790
所以这会是我们最常用的 现在我们来想一下

93
00:05:09,790 --> 00:05:14,105
为什么 L1 正则化得出稀疏权重的向量

94
00:05:14,105 --> 00:05:18,800
而 L2 正则化得出较小齐权的向量？

95
00:05:18,800 --> 00:05:20,785
原因是这样的

96
00:05:20,785 --> 00:05:22,478
如果我们向量 (1,0)

97
00:05:22,478 --> 00:05:26,389
权重绝对值的总和是 1

98
00:05:26,389 --> 00:05:30,779
权重平方的总和也是 1

99
00:05:30,779 --> 00:05:35,424
如果得到向量 (0.5, 0.5)

100
00:05:35,425 --> 00:05:38,875
权重绝对值的总和仍然是 1

101
00:05:38,875 --> 00:05:46,410
平方总和是 0.25 加 0.25 等于 0.5

102
00:05:46,410 --> 00:05:50,930
那么 L2 正则化更倾向于向量 (0.5, 0.5)

103
00:05:50,930 --> 00:05:52,900
而不是向量 (1,0)

104
00:05:52,899 --> 00:05:57,954
因为前者可以得出更小的平方总和

105
00:05:57,954 --> 00:06:00,000
以及更小的误差函数

