1
00:00:00,000 --> 00:00:03,689
在上个视频中 我们逐步讲解了

2
00:00:03,689 --> 00:00:06,980
将完全连接层替换为局部连接层这一概念

3
00:00:06,980 --> 00:00:10,769
这种局部连接层包含更少的权重

4
00:00:10,769 --> 00:00:12,884
并在空间内共享

5
00:00:12,884 --> 00:00:17,160
在此视频中我们通过图形的形式阐述了这一概念

6
00:00:17,160 --> 00:00:20,010
并定义了卷积层

7
00:00:20,010 --> 00:00:25,179
它是一种隐藏层 你在卷积神经网络中将会遇到

8
00:00:25,178 --> 00:00:29,283
首先 我们将图片拆分为更小的部分

9
00:00:29,285 --> 00:00:30,495
在上个视频中

10
00:00:30,495 --> 00:00:34,000
我们将图片分成了四个相同大小的区域

11
00:00:34,000 --> 00:00:38,115
标为红色 绿色 黄色和蓝色

12
00:00:38,115 --> 00:00:42,024
在拆分图片以构建卷积层的过程中

13
00:00:42,024 --> 00:00:47,353
我们首先选择定义卷积窗的宽度和高度

14
00:00:47,353 --> 00:00:49,875
然后在图片像素矩阵上

15
00:00:49,875 --> 00:00:54,224
水平和垂直地滑动该卷积窗

16
00:00:54,222 --> 00:00:59,265
在每个位置 卷积窗都在图片中指定一个小部分

17
00:00:59,265 --> 00:01:04,954
并定义一组连接到单个隐藏节点的像素

18
00:01:04,953 --> 00:01:08,264
我们将此隐藏层称为卷积层

19
00:01:08,265 --> 00:01:12,495
我们深入了解下区域性输入节点集合

20
00:01:12,495 --> 00:01:17,355
对卷积层中的节点的值有何影响

21
00:01:17,355 --> 00:01:20,504
我们可以通过在箭头上方写上值

22
00:01:20,504 --> 00:01:24,209
表示连接这些节点的权重

23
00:01:24,209 --> 00:01:29,715
然后 为了获取该图片输入的卷积层中节点的值

24
00:01:29,715 --> 00:01:34,379
我们就像处理神经网络一样操作

25
00:01:34,379 --> 00:01:39,810
我们将输入节点与对应的权重相乘 然后对结果求和

26
00:01:39,810 --> 00:01:42,734
求和后的结果是 0

27
00:01:42,733 --> 00:01:45,707
和神经网络一样 存在偏差

28
00:01:45,709 --> 00:01:48,489
暂时假设该偏差是 0

29
00:01:48,489 --> 00:01:53,694
我们始终会向卷积层添加一个 ReLu 激活函数

30
00:01:53,694 --> 00:01:55,719
ReLu 激活函数使正值保持不变

31
00:01:55,718 --> 00:02:00,323
并将所有负值变成 0

32
00:02:00,325 --> 00:02:04,090
这里 0 保持不变

33
00:02:04,090 --> 00:02:09,438
因此 我们可以代入卷积层第一个节点的值

34
00:02:09,437 --> 00:02:14,287
然后按照完全相同的方式计算所有其他节点的值

35
00:02:14,288 --> 00:02:18,823
首先将每个输入节点与权重相乘

36
00:02:18,824 --> 00:02:21,145
这个节点的值是 -2

37
00:02:21,145 --> 00:02:23,820
偏差依然是 0

38
00:02:23,818 --> 00:02:25,703
所以不用加上任何值

39
00:02:25,705 --> 00:02:29,629
但是现在应用 ReLu 激活函数的话

40
00:02:29,627 --> 00:02:33,632
-2 变成了 0

41
00:02:33,633 --> 00:02:35,948
我们可以重复这一流程

42
00:02:35,949 --> 00:02:39,719
算出卷积层所有其他节点的值

43
00:02:39,717 --> 00:02:43,477
我们会发现将权重放在网格里

44
00:02:43,479 --> 00:02:47,199
比写在箭头上方更方便

45
00:02:47,199 --> 00:02:50,400
以这种方式将权重放入网格中后

46
00:02:50,400 --> 00:02:52,849
我们将该网格称为过滤器

47
00:02:52,848 --> 00:02:57,489
它的大小始终与卷积窗的大小一样

48
00:02:57,490 --> 00:03:02,620
这里 我们的过滤器宽和高都为 3

49
00:03:02,620 --> 00:03:05,349
现在计算隐藏层中节点的值这一流程

50
00:03:05,348 --> 00:03:08,843
更直观了

51
00:03:08,842 --> 00:03:11,091
我们看看计算卷积层中

52
00:03:11,092 --> 00:03:14,843
第一个节点的值效果如何

53
00:03:14,842 --> 00:03:17,500
同样 我们首先将每个输入节点

54
00:03:17,500 --> 00:03:21,907
与对应的权重相乘 然后对结果求和

55
00:03:21,907 --> 00:03:28,113
求和之后是 0 因此应用 ReLu 激活函数后还是 0

56
00:03:28,115 --> 00:03:31,930
但是对于这个大的值 3 会怎样？

57
00:03:31,930 --> 00:03:35,615
我们看看相应的图片部分

58
00:03:35,615 --> 00:03:38,905
你会发现过滤器中的正值

59
00:03:38,905 --> 00:03:44,435
正好对应的是这个图片区域最大的值

60
00:03:44,435 --> 00:03:46,365
反之亦然

61
00:03:46,365 --> 00:03:51,715
过滤器中的负值对应的是图片中最小的值

62
00:03:51,715 --> 00:03:57,098
实际上 因为我们对图片中的像素进行了调整 使其位于 0 到 1 之间

63
00:03:57,098 --> 00:04:01,454
1 表示白色 0 表示黑色

64
00:04:01,455 --> 00:04:04,479
这个 3 是我们可以从此过滤器的卷积层中

65
00:04:04,479 --> 00:04:07,789
获得的最大值

66
00:04:07,788 --> 00:04:12,608
这就是该区域的规律 对角线白色长条

67
00:04:12,609 --> 00:04:17,709
是唯一可以产生这个最大值的像素分布

68
00:04:17,709 --> 00:04:21,338
我们看到这里也出现了 3

69
00:04:21,338 --> 00:04:25,740
并且可以验证图片的相应区域是一样的

70
00:04:25,740 --> 00:04:27,819
但是再看看过滤器

71
00:04:27,819 --> 00:04:32,199
我们用浅色表示更大的数字

72
00:04:32,199 --> 00:04:34,000
可以看出规律是相符的

73
00:04:34,000 --> 00:04:37,509
每个都描绘了一个浅色斜条纹

74
00:04:37,509 --> 00:04:43,350
实际上 当我们使用卷积神经网络时  我们经常会可视化过滤器

75
00:04:43,350 --> 00:04:46,120
该可视化图表会告诉我们

76
00:04:46,120 --> 00:04:49,194
过滤器会检测到什么样的规律

77
00:04:49,194 --> 00:04:51,084
就像在神经网络中一样

78
00:04:51,084 --> 00:04:55,060
我们不会提前设置这些权重

79
00:04:55,060 --> 00:05:00,310
而是由网络去判断哪些权重可以最小化损失函数

80
00:05:00,310 --> 00:05:04,810
到目前为止 我们的网络只有一个过滤器

81
00:05:04,810 --> 00:05:10,088
因此我们只能获得是否检测到了一个规律的信息

82
00:05:10,088 --> 00:05:12,430
如果我们想检测更多规律

83
00:05:12,430 --> 00:05:14,845
则需要使用更多过滤器

84
00:05:14,845 --> 00:05:16,293
在下个视频中

85
00:05:16,293 --> 00:05:21,040
我们将继续讲解卷积层

86
00:05:21,040 --> 00:05:26,000
并讲解为何要在图片中寻找更多规律？

