1
00:00:00,000 --> 00:00:04,099
现在快速总结下上几节课的内容

2
00:00:04,099 --> 00:00:05,644
这是两个模型

3
00:00:05,644 --> 00:00:06,915
左侧是坏模型

4
00:00:06,915 --> 00:00:09,080
右侧是好模型

5
00:00:09,080 --> 00:00:13,469
我们算出每个模型的交叉熵

6
00:00:13,470 --> 00:00:18,969
即点是本身颜色的概率对数负值之和

7
00:00:18,969 --> 00:00:22,190
并得出结论：右侧的模型更好

8
00:00:22,190 --> 00:00:25,890
因为交叉熵更小

9
00:00:25,890 --> 00:00:29,410
我们计算下误差方程公式

10
00:00:29,410 --> 00:00:31,675
拆分为两种情形

11
00:00:31,675 --> 00:00:34,280
第一种是 y = 1

12
00:00:34,280 --> 00:00:36,564
首先是点是蓝色的情形

13
00:00:36,564 --> 00:00:42,490
模型告诉我们蓝色的概率是预测 ŷ

14
00:00:42,490 --> 00:00:47,865
对于这两个点 概率是 0.6 和 0.2

15
00:00:47,865 --> 00:00:50,929
可以看出 蓝色区域的点是蓝色的概率

16
00:00:50,929 --> 00:00:54,905
比红色区域的点概率要高

17
00:00:54,905 --> 00:01:00,579
我们的误差是该概率的对数负值

18
00:01:00,579 --> 00:01:04,025
也就是 -ln(ŷ)

19
00:01:04,025 --> 00:01:07,510
在图中是 -ln(0.6)

20
00:01:07,510 --> 00:01:09,700
和 -ln(0.2)

21
00:01:09,700 --> 00:01:12,010
如果 y = 0

22
00:01:12,010 --> 00:01:13,765
即点是红色

23
00:01:13,765 --> 00:01:18,000
那么我们需要计算点是红色的概率

24
00:01:18,000 --> 00:01:19,810
点是红色的概率是

25
00:01:19,810 --> 00:01:23,049
1 减去点是蓝色的概率

26
00:01:23,049 --> 00:01:27,790
即 1-ŷ

27
00:01:27,790 --> 00:01:31,269
所以误差是该概率对数的负值

28
00:01:31,269 --> 00:01:36,334
即 -ln(1-ŷ)

29
00:01:36,334 --> 00:01:39,150
这里是 -ln(0.1)

30
00:01:39,150 --> 00:01:42,099
和 -ln(0.7)

31
00:01:42,099 --> 00:01:46,620
我们得出结论：如果点是蓝色 则误差是 -ln(ŷ)

32
00:01:46,620 --> 00:01:50,765
如果点是红色 则误差是 -ln(1-ŷ)

33
00:01:50,765 --> 00:01:53,640
我们可以将这两个公式总结为一个

34
00:01:53,640 --> 00:01:57,655
Error = -(1-y)(ln(1-ŷ))-yln(ŷ)

35
00:01:57,655 --> 00:02:02,058

36
00:02:02,058 --> 00:02:03,789
该公式为何可行？

37
00:02:03,789 --> 00:02:05,739
因为如果点是蓝色

38
00:02:05,739 --> 00:02:11,490
那么 y = 1 则 1-y=0 使第一项变成 0

39
00:02:11,490 --> 00:02:15,930
第二项就是 ln(ŷ)

40
00:02:15,930 --> 00:02:20,185
类似地 如果点是红色 则 y=0

41
00:02:20,185 --> 00:02:22,435
公式的第二项变成 0

42
00:02:22,435 --> 00:02:27,069
第一项是 ln(1-ŷ)

43
00:02:27,068 --> 00:02:30,079
现在误差函数的公式变成

44
00:02:30,080 --> 00:02:33,060
所有点的误差函数之和

45
00:02:33,060 --> 00:02:35,550
即这里的和

46
00:02:35,550 --> 00:02:38,564
将是这里的 4.8

47
00:02:38,562 --> 00:02:41,757
按照惯例 我们将考虑平均值

48
00:02:41,758 --> 00:02:45,338
而不是和 它已经在这里除以 m

49
00:02:45,340 --> 00:02:49,080
这样使 4.8 变成 1.2

50
00:02:49,080 --> 00:02:53,405
接下来我们将使用这个公式作为误差函数

51
00:02:53,405 --> 00:02:58,875
因为 ŷ 是线性方程 Wx + b 的 s 型函数

52
00:02:58,875 --> 00:03:01,919
那么误差的总公式涉及的是

53
00:03:01,919 --> 00:03:05,129
W 和 b 即模型的权重

54
00:03:05,127 --> 00:03:08,227
也就是这里的和

55
00:03:08,229 --> 00:03:14,465
这里 i 是点 x(i) 的下角标

56
00:03:14,465 --> 00:03:16,009
我们已经得出公式

57
00:03:16,008 --> 00:03:17,394
现在的目标是简化它

58
00:03:17,395 --> 00:03:19,155
这将是接下来的任务

59
00:03:19,155 --> 00:03:20,384
顺便提醒下

60
00:03:20,383 --> 00:03:23,239
我们针对的是二元分类问题

61
00:03:23,240 --> 00:03:25,520
如果是有多个类别的分类问题

62
00:03:25,520 --> 00:03:29,419
那么误差将是多类别交叉熵

63
00:03:29,419 --> 00:03:33,530
这里的公式针对的是每个数据点

64
00:03:33,530 --> 00:03:38,995
我们将标记类别的积乘以预测对数 然后对所有这些值求平均值

65
00:03:38,995 --> 00:03:41,569
同样可以自己计算验证下 当只有两个类别时

66
00:03:41,568 --> 00:03:45,000
这两个公式实际上是一样的

