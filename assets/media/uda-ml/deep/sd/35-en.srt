1
00:00:00,000 --> 00:00:03,979
So let's compare the perceptron algorithm and the gradient descent algorithm.

2
00:00:03,979 --> 00:00:05,844
In the gradient descent algorithm,

3
00:00:05,844 --> 00:00:14,153
we take the weights and change them from Wi to Wᵢ + α(ŷ - y)xᵢ.

4
00:00:14,153 --> 00:00:17,230
In the perceptron algorithm not every point changes the weights,

5
00:00:17,230 --> 00:00:18,870
only the misclassified ones.

6
00:00:18,870 --> 00:00:23,565
Here, if x is misclassified we'll change the weights by

7
00:00:23,565 --> 00:00:29,789
adding xᵢ to wᵢ if the point label is positive and subtracting if it's negative.

8
00:00:29,789 --> 00:00:32,539
Now the question is, are these two things the same?

9
00:00:32,539 --> 00:00:37,009
Well, let's remember that in the perceptron algorithm the labels are 1 and 0,

10
00:00:37,009 --> 00:00:40,673
and the predictions ŷ are also 1 and 0.

11
00:00:40,673 --> 00:00:43,039
So if the point is correct, classified.

12
00:00:43,039 --> 00:00:48,255
Then y - ŷ is 0 because y is equal to ŷ.

13
00:00:48,255 --> 00:00:50,200
Now if the point is labeled blue,

14
00:00:50,200 --> 00:00:51,884
then y = 1,

15
00:00:51,884 --> 00:00:55,829
and if it's misclassified then the prediction must be ŷ = 0.

16
00:00:55,829 --> 00:00:59,280
So ŷ - y is -1.

17
00:00:59,280 --> 00:01:01,048
Similarly, with points labeled red,

18
00:01:01,048 --> 00:01:02,530
then y = 0 and ŷ = 1.

19
00:01:02,530 --> 00:01:06,769
So ŷ - y = 1.

20
00:01:06,769 --> 00:01:09,629
This may not be super clear right away but if you stare the screen for long

21
00:01:09,629 --> 00:01:13,659
enough you realize that the right and the left are exactly the same thing.

22
00:01:13,659 --> 00:01:15,180
The only difference is that in the left,

23
00:01:15,180 --> 00:01:17,812
ŷ can take any number between 0 and 1,

24
00:01:17,811 --> 00:01:19,649
whereas in the right,

25
00:01:19,650 --> 00:01:23,599
ŷ can take only the values 0 or 1.

26
00:01:23,599 --> 00:01:25,379
It's pretty fascinating, isn't it?

27
00:01:25,379 --> 00:01:28,064
But let's study gradient descent even more carefully.

28
00:01:28,063 --> 00:01:31,828
Both in the perceptron algorithm and in the gradient descent algorithm,

29
00:01:31,828 --> 00:01:35,473
a point that has misclassified tells a line to come closer

30
00:01:35,474 --> 00:01:40,769
because eventually it wants the line to surpass it so it can be in the correct side.

31
00:01:40,769 --> 00:01:43,724
Now what happens if the point is correctly classified?

32
00:01:43,724 --> 00:01:47,269
Well, the perceptron algorithm says do absolutely nothing.

33
00:01:47,269 --> 00:01:52,995
In the gradient descent algorithm you are changing the weights but what is it doing.

34
00:01:52,995 --> 00:01:54,465
Well, if we look carefully,

35
00:01:54,465 --> 00:01:58,935
what the point is telling the line is to go farther away.

36
00:01:58,935 --> 00:02:01,114
And this makes sense, right?

37
00:02:01,114 --> 00:02:03,170
Because if you are correctly classified,

38
00:02:03,170 --> 00:02:05,885
say if you are a blue point in the blue region,

39
00:02:05,885 --> 00:02:08,483
you'd like to be even more into the blue region,

40
00:02:08,483 --> 00:02:13,145
so your prediction is even closer to 1 and your error is even smaller.

41
00:02:13,145 --> 00:02:16,340
Similarly for a red point in the red region.

42
00:02:16,340 --> 00:02:18,979
So it makes sense that the point tells the line to go farther

43
00:02:18,979 --> 00:02:23,164
away and that's precisely what the gradient descent algorithm does.

44
00:02:23,163 --> 00:02:26,250
The misclassified points ask the line to come closer

45
00:02:26,250 --> 00:02:30,693
and the correctly classified points ask the line to go farther away.

46
00:02:30,693 --> 00:02:33,228
The line listens to all the points and take steps in

47
00:02:33,229 --> 00:02:37,000
such a way that it eventually arrives to a pretty good solution.

