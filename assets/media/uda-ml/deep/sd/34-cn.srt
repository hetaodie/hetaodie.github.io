1
00:00:00,000 --> 00:00:03,390
现在我们终于有了为梯度下降算法

2
00:00:03,390 --> 00:00:06,971
编写伪代码的工具 步骤是这样的

3
00:00:06,971 --> 00:00:08,268
第一步

4
00:00:08,268 --> 00:00:13,344
从随机权重 w1 一直到 wn 和 b 开始

5
00:00:13,342 --> 00:00:16,300
得出一条直线 不仅仅是一条直线

6
00:00:16,300 --> 00:00:20,664
而是 σ(Wx+b) 形成的整个概率函数

7
00:00:20,664 --> 00:00:24,316
对于每个点 我们都将计算误差

8
00:00:24,315 --> 00:00:29,163
我们可以看到分类错误的点误差很大 分类正确的点误差很小

9
00:00:29,164 --> 00:00:32,530
对于所有坐标从 x1 到 xn 的点

10
00:00:32,530 --> 00:00:36,804
我们通过加上学习速率 α 乘以

11
00:00:36,804 --> 00:00:42,588
误差函数相对于 wi 的偏导数来更新 wi

12
00:00:42,588 --> 00:00:45,210
并通过加上 α 乘以

13
00:00:45,210 --> 00:00:48,090
误差函数相对于 b 的偏导数来更新 b

14
00:00:48,090 --> 00:00:52,649
并得出新的权重 wi’ 和新的偏差 b’

15
00:00:52,649 --> 00:00:56,215
我们已经计算过这些偏导数

16
00:00:56,215 --> 00:00:59,310
知道对于 wi 的偏导数是 （y^-y）xi

17
00:00:59,310 --> 00:01:04,765
对于 b 的偏导数是 y^-y

18
00:01:04,765 --> 00:01:10,390
我们就是这么更新权重的

19
00:01:10,390 --> 00:01:13,730
重复这一流程 直到误差很小

20
00:01:13,730 --> 00:01:15,754
或者重复固定的次数

21
00:01:15,754 --> 00:01:18,829
重复次数称为 epoch 稍后将介绍这一概念

22
00:01:18,828 --> 00:01:20,149
这个看起来很熟悉

23
00:01:20,150 --> 00:01:21,974
之前见过吗？

24
00:01:21,974 --> 00:01:24,284
观察每个点

25
00:01:24,284 --> 00:01:26,629
每个点是将成倍数的自己与直线的权重相加

26
00:01:26,629 --> 00:01:31,659
以便使自己离直线更近（如果分类错误的话）

27
00:01:31,659 --> 00:01:34,760
感知器算法差不多就是这么做的

28
00:01:34,760 --> 00:01:35,989
在下个视频中 我们将了解它们的相似之处

29
00:01:35,989 --> 00:01:39,000
因为它们很相似 令人有点怀疑

