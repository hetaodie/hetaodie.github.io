1
00:00:00,000 --> 00:00:04,799
So far we've been working with the state value function for a policy.

2
00:00:04,799 --> 00:00:06,689
For each state s,

3
00:00:06,690 --> 00:00:11,160
it yields the expected discounted return If the agent starts in state

4
00:00:11,160 --> 00:00:16,065
as and then uses the policy to choose its actions for all time steps.

5
00:00:16,065 --> 00:00:18,540
You've seen a few examples and know how to

6
00:00:18,539 --> 00:00:22,030
calculate the state value function corresponding to a policy.

7
00:00:22,030 --> 00:00:24,100
In this concept, we'll define a new type of

8
00:00:24,100 --> 00:00:27,810
value function known as the action-value function.

9
00:00:27,809 --> 00:00:31,550
This value function is denoted with a lowercase q instead of v.

10
00:00:31,550 --> 00:00:35,317
While the state values are a function of the environment state,

11
00:00:35,317 --> 00:00:40,695
the action values are a function of the environment state and the agent's action.

12
00:00:40,695 --> 00:00:43,149
For each state s and action a,

13
00:00:43,149 --> 00:00:45,600
the action value function yields

14
00:00:45,600 --> 00:00:49,740
the expected discounted return If the agent starts in state

15
00:00:49,740 --> 00:00:53,190
s then chooses action a and

16
00:00:53,189 --> 00:00:58,000
then uses a policy to choose its actions for all future time steps.

17
00:00:58,000 --> 00:00:59,479
Just like with the state value function,

18
00:00:59,479 --> 00:01:04,149
it will help your intuition If you calculate this yourself.

19
00:01:04,150 --> 00:01:06,180
In the case of the state value function,

20
00:01:06,180 --> 00:01:10,650
we kept track of the value of each state with the number on the grid.

21
00:01:10,650 --> 00:01:13,725
We'll do something similar with the action-value function,

22
00:01:13,724 --> 00:01:17,153
where we now need up to four values for each state,

23
00:01:17,153 --> 00:01:20,159
each corresponding to a different action.

24
00:01:20,159 --> 00:01:21,840
These four numbers correspond to

25
00:01:21,840 --> 00:01:26,130
the same state but the one on top corresponds to action up;

26
00:01:26,129 --> 00:01:31,829
the one on the right, corresponds to moving right before following the policy and so on.

27
00:01:31,829 --> 00:01:33,359
You'll see this soon.

28
00:01:33,359 --> 00:01:36,194
So with the exception of the terminal state,

29
00:01:36,194 --> 00:01:39,479
I've broken up the figure to leave a space for keeping

30
00:01:39,480 --> 00:01:43,795
track of the value corresponding to each possible state and action.

31
00:01:43,795 --> 00:01:47,212
And let's see if we can calculate some action values.

32
00:01:47,212 --> 00:01:49,575
We'll begin with the state here.

33
00:01:49,575 --> 00:01:54,034
Let's calculate the value corresponding to this state and action down.

34
00:01:54,034 --> 00:01:56,295
So the agent starts in this state,

35
00:01:56,295 --> 00:02:00,615
takes action down and receives a reward of negative one.

36
00:02:00,614 --> 00:02:03,045
Then for every time step in the future,

37
00:02:03,045 --> 00:02:07,320
it just follows the policy until it reaches the terminal state.

38
00:02:07,319 --> 00:02:10,004
We can then add up all the rewards that it encountered

39
00:02:10,004 --> 00:02:13,319
along the way and when we do that, we get zero.

40
00:02:13,319 --> 00:02:16,769
So this zero corresponds to the action value

41
00:02:16,770 --> 00:02:21,110
for this state where the agent started and action down.

42
00:02:21,110 --> 00:02:23,985
Let's try calculating another actual value,

43
00:02:23,985 --> 00:02:28,020
this time corresponding to this state and action up.

44
00:02:28,020 --> 00:02:31,890
So the agent starts in the state and takes action up,

45
00:02:31,889 --> 00:02:35,454
and it received a reward of negative one.

46
00:02:35,455 --> 00:02:39,830
Then it just follows a policy for all future time steps.

47
00:02:39,830 --> 00:02:42,210
We add up the reward it collected along

48
00:02:42,210 --> 00:02:46,055
the way and this yields a cumulative reward of one.

49
00:02:46,055 --> 00:02:51,115
So this one corresponds to the action value for this state and action up.

50
00:02:51,115 --> 00:02:56,439
So you can continue and do this for every state-action pair that makes sense.

51
00:02:56,439 --> 00:02:59,775
And when you do this, you get this action-value function.

52
00:02:59,775 --> 00:03:04,204
I highly encourage you to calculate and check these values yourself.

53
00:03:04,204 --> 00:03:09,780
Before moving on, it's important to revisit some information that you learned earlier.

54
00:03:09,780 --> 00:03:16,862
Remember that we have the notation v_star to refer to the optimal state value function.

55
00:03:16,862 --> 00:03:22,080
Similarly, we'll refer to the optimal action value function as q_star.

