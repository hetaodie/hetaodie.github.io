1
00:00:00,000 --> 00:00:03,759
为了理解如何寻找最佳策略

2
00:00:03,759 --> 00:00:06,344
我们需要通过一个示例来讲解

3
00:00:06,344 --> 00:00:11,445
假设有一个非常非常小的世界 里面住着一个智能体

4
00:00:11,445 --> 00:00:15,769
这个小世界主要由 9 块草坪组成

5
00:00:15,769 --> 00:00:20,094
但是其中的两个位置有大山

6
00:00:20,094 --> 00:00:21,599
将这个世界的所有 9 个可能的位置

7
00:00:21,600 --> 00:00:25,760
看做状态和环境

8
00:00:25,760 --> 00:00:27,660
在每个时间点

9
00:00:27,660 --> 00:00:30,240
假设智能体只能上下左右移动

10
00:00:30,239 --> 00:00:35,600
并且只能采取不离开网格的动作

11
00:00:35,600 --> 00:00:39,899
图中的箭头展示了可能的动作

12
00:00:39,899 --> 00:00:42,420
假设智能体的目标是

13
00:00:42,420 --> 00:00:46,664
尽快抵达右下角

14
00:00:46,664 --> 00:00:49,034
我们将此任务看做阶段性任务

15
00:00:49,034 --> 00:00:52,369
每当智能体抵达目的地时 该阶段结束

16
00:00:52,369 --> 00:00:56,530
因此不用担心离开该目标状态

17
00:00:56,530 --> 00:01:02,590
此外 假设对于大多数变动 智能体都收到奖励 -1

18
00:01:02,590 --> 00:01:06,075
但是如果遇到大山

19
00:01:06,075 --> 00:01:08,460
则奖励为 -3

20
00:01:08,459 --> 00:01:10,214
如果遇到目标状态

21
00:01:10,215 --> 00:01:12,125
则奖励为 5

22
00:01:12,125 --> 00:01:14,670
我们将奖励信号看做

23
00:01:14,670 --> 00:01:18,570
每一个时间步智能体没有抵达目标就会受到惩罚

24
00:01:18,569 --> 00:01:20,159
可以将大山看做

25
00:01:20,159 --> 00:01:22,319
具有非常大的惩罚力度

26
00:01:22,319 --> 00:01:25,494
因为跨过大山需要的时间更长

27
00:01:25,495 --> 00:01:30,300
奖励机制鼓励智能体尽快抵达目的地

28
00:01:30,299 --> 00:01:31,769
抵达目的地后

29
00:01:31,769 --> 00:01:33,089
就会获得奖励 5

30
00:01:33,090 --> 00:01:34,665
这一阶段结束

31
00:01:34,665 --> 00:01:38,060
我们将通过该示例讲解如何寻找最佳策略

