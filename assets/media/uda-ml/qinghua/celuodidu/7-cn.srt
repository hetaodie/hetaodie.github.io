1
00:00:00,000 --> 00:00:03,734
到目前为止 我们讨论了仅使用策略的优势部分

2
00:00:03,734 --> 00:00:07,424
作为目标函数的基于策略的方法

3
00:00:07,424 --> 00:00:10,934
即我们预期从中获得多少奖励

4
00:00:10,935 --> 00:00:15,690
意思是我们不关心该策略采取了什么操作

5
00:00:15,689 --> 00:00:17,054
经历了什么状态

6
00:00:17,054 --> 00:00:18,870
频率有多大 等等

7
00:00:18,870 --> 00:00:22,650
我们只关心整体预期奖励

8
00:00:22,649 --> 00:00:25,244
这样可以形成

9
00:00:25,245 --> 00:00:28,019
在目标函数空间内非常接近

10
00:00:28,019 --> 00:00:30,884
但是行为截然不同的两个策略

11
00:00:30,885 --> 00:00:34,725
我们来举一个有四条腿的行走机器人例子

12
00:00:34,725 --> 00:00:37,545
一个策略可能使机器人这样行走

13
00:00:37,545 --> 00:00:42,164
另一个策略可能使机器人的步法完全不一样

14
00:00:42,164 --> 00:00:45,269
两个策略都是合理的解决方案

15
00:00:45,270 --> 00:00:48,570
假设策略 A 是当前策略

16
00:00:48,570 --> 00:00:50,984
策略 B 是梯度目标指向的策略

17
00:00:50,984 --> 00:00:54,570
一个更好的策略

18
00:00:54,570 --> 00:01:00,375
我们的优化算法将尝试更新参数以从 A 移到 B

19
00:01:00,375 --> 00:01:01,634
但是并非一直这样

20
00:01:01,634 --> 00:01:04,219
因为我们使用了学习步骤 α

21
00:01:04,219 --> 00:01:08,724
因此我们进入一组中间策略参数

22
00:01:08,724 --> 00:01:11,964
但是可能会导致很糟糕的性能

23
00:01:11,965 --> 00:01:15,859
这在行走等复杂的控制任务中很常见

24
00:01:15,859 --> 00:01:20,439
不同动作之间的坐标和时间非常重要

25
00:01:20,439 --> 00:01:25,554
算法依然在学习改善策略并获得合理的结果

26
00:01:25,555 --> 00:01:30,250
但是学习流程本身可能会非常不稳定并且效率很低

27
00:01:30,250 --> 00:01:33,864
此外 在某些应用中 例如机器人

28
00:01:33,864 --> 00:01:37,007
我们会处理电子机械组件

29
00:01:37,007 --> 00:01:41,545
显著改变策略参数可能会破坏装置

30
00:01:41,545 --> 00:01:44,350
因为这些原因

31
00:01:44,349 --> 00:01:47,769
除了目标函数之外 我们还应该注意策略参数

32
00:01:47,769 --> 00:01:52,164
实际上 我们可能需要对策略参数进行一些限制

33
00:01:52,165 --> 00:01:54,550
只能以某种方式更改参数

34
00:01:54,549 --> 00:01:57,507
或只能按照一定的数量更改参数

35
00:01:57,507 --> 00:02:01,899
我们可以通过添加限制条件 在基于梯度的算法中实现这一点

36
00:02:01,900 --> 00:02:07,175
要求两个策略之间的差异不能超过某个阈值 δ

37
00:02:07,174 --> 00:02:10,139
另一个添加这种限制条件的方式是

38
00:02:10,139 --> 00:02:12,779
向目标函数添加惩罚项

39
00:02:12,780 --> 00:02:15,150
即该差异的某个乘数

40
00:02:15,150 --> 00:02:18,765
如果我们尝试优化这个组合的目标函数

41
00:02:18,764 --> 00:02:21,944
那么我们将能够获得更好的策略

42
00:02:21,944 --> 00:02:24,375
但是在每一步差别并不是很大

43
00:02:24,375 --> 00:02:27,810
这样可以使学习算法很稳定

44
00:02:27,810 --> 00:02:31,335
有点像对机器学习算法进行正则化

45
00:02:31,335 --> 00:02:33,330
但是对于这两种方法

46
00:02:33,330 --> 00:02:35,760
无论是添加限制条件还是惩罚项

47
00:02:35,759 --> 00:02:40,125
我们都需要某种方式来量化两个策略之间的差异

48
00:02:40,125 --> 00:02:45,169
计算两个策略之间的差异的一种简单方式是

49
00:02:45,169 --> 00:02:50,674
算出两组参数向量之间的差异并对该差异取范数 例如欧几里得范数

50
00:02:50,675 --> 00:02:54,950
如果你的策略是某个复杂的参数函数

51
00:02:54,949 --> 00:02:59,569
当你使用深度神经网络或其他机器学习模型时就是这种情况

52
00:02:59,569 --> 00:03:02,389
那么参数差异可能无法准确地反映

53
00:03:02,389 --> 00:03:05,629
策略行为之间的差异

54
00:03:05,629 --> 00:03:08,889
以及它们最终生成的动作之间的差异

55
00:03:08,889 --> 00:03:11,284
为了得出一个更好的差异衡量方式

56
00:03:11,284 --> 00:03:16,340
我们需要想出一个用到不同动作的概率分布的策略

57
00:03:16,340 --> 00:03:21,575
当动作空间是连续空间时 这个思考角度尤其有用

58
00:03:21,574 --> 00:03:25,339
思考下拖车问题的连续版本

59
00:03:25,340 --> 00:03:28,969
不再有向左和向右的二元动作

60
00:03:28,969 --> 00:03:33,590
你可以应用在 -1 到 +1 之间的任何大小的力

61
00:03:33,590 --> 00:03:36,950
例如 -0.5 或 +0.8

62
00:03:36,949 --> 00:03:41,750
我们的策略是这个动作范围的概率分布

63
00:03:41,750 --> 00:03:45,229
注意 不同状态之间的分布可能不相同

64
00:03:45,229 --> 00:03:47,014
如果是连续状态

65
00:03:47,014 --> 00:03:48,709
我们可以将这些分布统一成

66
00:03:48,710 --> 00:03:54,135
一个状态和动作组合范围的连续分布

67
00:03:54,134 --> 00:03:57,182
假设有两个这样的策略

68
00:03:57,182 --> 00:03:59,889
每个都是状态和动作分布

69
00:03:59,889 --> 00:04:03,684
如何比较这两个分布？

70
00:04:03,685 --> 00:04:09,370
一种统计学衡量方法是 KL 散度

71
00:04:09,370 --> 00:04:13,045
p 相对于 q 的 KL 散度

72
00:04:13,044 --> 00:04:15,819
定义为 p(x) 的积分

73
00:04:15,819 --> 00:04:19,120
乘以 p(x)/q(x) 的对数

74
00:04:19,120 --> 00:04:26,319
我们可以将 log p(x)/q(x) 变换成 log p(x) - log q(x)

75
00:04:26,319 --> 00:04:29,435
这样可能更容易理解

76
00:04:29,435 --> 00:04:31,766
这是原始分布

77
00:04:31,766 --> 00:04:34,754
p(x) 和 q(x)

78
00:04:34,754 --> 00:04:38,250
我们取这两个概率的对数

79
00:04:38,250 --> 00:04:40,814
然后在每个点 x

80
00:04:40,814 --> 00:04:44,730
我们将这两个对数概率相减

81
00:04:44,730 --> 00:04:47,550
最后 我们用某个概率值缩放该差值

82
00:04:47,550 --> 00:04:52,845
并在整个 x 的范围内求和

83
00:04:52,845 --> 00:04:55,380
从图中可以看出

84
00:04:55,379 --> 00:04:59,819
该区域是这两个对数概率分布的非重叠区域

85
00:04:59,819 --> 00:05:02,625
但是使用其中一个概率作为参考

86
00:05:02,625 --> 00:05:05,685
在每个位置 x 求和

87
00:05:05,685 --> 00:05:09,915
意味着 p 相对于 q 的 KL 散度

88
00:05:09,915 --> 00:05:16,395
不等于 q 相对于 p 的散度 这是一个非对称距离衡量法

89
00:05:16,394 --> 00:05:19,409
我们将其应用到策略梯度的方式是

90
00:05:19,410 --> 00:05:23,362
计算当前策略 θ 与任何新候选策略 θ′

91
00:05:23,362 --> 00:05:26,279
之间的 KL 散度

92
00:05:26,279 --> 00:05:31,995
然后使用这个差异项作为目标函数的限制条件

93
00:05:31,995 --> 00:05:35,935
该项会惩罚使行为变化很大的新策略

94
00:05:35,935 --> 00:05:40,810
因此 新策略必须带来更多价值 才能被视为更好的策略

95
00:05:40,810 --> 00:05:44,470
对于总体策略梯度算法

96
00:05:44,470 --> 00:05:47,470
这有助于创建一个更稳定的学习流程

97
00:05:47,470 --> 00:05:50,845
策略不会大幅度跳动

98
00:05:50,845 --> 00:05:55,323
目前已经开发出了多个受限策略梯度变体版本

99
00:05:55,322 --> 00:05:58,699
包括热门的置信域策略优化

100
00:05:58,699 --> 00:06:01,269
简称 TRPO 算法

101
00:06:01,269 --> 00:06:04,555
以及最近的近端策略优化 (PPO)

102
00:06:04,555 --> 00:06:08,290
这是将 PPO 应用到一个非常复杂的环境示例

103
00:06:08,290 --> 00:06:11,545
智能体尝试接触一个目标 即粉色球

104
00:06:11,545 --> 00:06:15,235
同时学习如何行走 跑步 转身

105
00:06:15,235 --> 00:06:16,960
从小挫折中恢复

106
00:06:16,959 --> 00:06:20,544
以及如何在跌倒后站起来

107
00:06:20,545 --> 00:06:23,379
这个示例演示了修改目标函数

108
00:06:23,379 --> 00:06:27,654
使我们能够内置多种限制条件

109
00:06:27,654 --> 00:06:32,809
包括额外的损失函数 从而进一步指导智能体学习规律

