1
00:00:00,000 --> 00:00:02,084
As you may have noticed,

2
00:00:02,084 --> 00:00:06,580
stochastic policy search may or may not always work as expected.

3
00:00:06,580 --> 00:00:09,960
It is sensitive to the choice of individual samples and may

4
00:00:09,960 --> 00:00:14,345
get stuck in local optima or take a long time to converge.

5
00:00:14,345 --> 00:00:17,565
One of the main reasons to use such a randomized approach

6
00:00:17,565 --> 00:00:21,135
is that you don't need to know anything about the objective function,

7
00:00:21,135 --> 00:00:23,760
the underlying policy or it's gradient.

8
00:00:23,760 --> 00:00:25,890
You can treat it like a black box.

9
00:00:25,890 --> 00:00:27,960
Send in a set of parameters,

10
00:00:27,960 --> 00:00:29,580
run the policy in the environment,

11
00:00:29,579 --> 00:00:31,500
and get an objective value.

12
00:00:31,500 --> 00:00:33,914
But if you knew more about the policy,

13
00:00:33,914 --> 00:00:35,759
and the objective function,

14
00:00:35,759 --> 00:00:38,250
and if you could compute the gradient of

15
00:00:38,250 --> 00:00:41,445
the objective function with respect to policy parameters,

16
00:00:41,445 --> 00:00:44,990
that would allow you to take more efficient steps, right?

17
00:00:44,990 --> 00:00:49,829
The gradient always points towards the direction of maximum change.

18
00:00:49,829 --> 00:00:52,549
So, instead of having to evaluate a bunch of

19
00:00:52,549 --> 00:00:55,459
random candidates in the current policies neighborhood,

20
00:00:55,460 --> 00:01:01,250
you can directly compute the next set of policy parameters that seem most promising.

21
00:01:01,250 --> 00:01:03,765
This is the basis of policy gradients.

22
00:01:03,765 --> 00:01:06,409
The general layout of such an approach would be to

23
00:01:06,409 --> 00:01:09,394
find the gradient for the current policy parameters,

24
00:01:09,394 --> 00:01:13,834
update them in the direction of increasing gradient, and iterate.

25
00:01:13,834 --> 00:01:16,429
Know that we still want to be cautious and take

26
00:01:16,430 --> 00:01:20,755
small steps defined by our step size alpha.

27
00:01:20,754 --> 00:01:25,924
This is because both our policy and the environment are most likely stochastic.

28
00:01:25,924 --> 00:01:29,780
Each policy evaluation may not produce consistent results.

29
00:01:29,780 --> 00:01:34,344
This is one reason to evaluate a policy over multiple episodes.

30
00:01:34,344 --> 00:01:37,370
Now, if your objective function is not differentiable,

31
00:01:37,370 --> 00:01:40,370
which is possible if the underlying policy is complicated,

32
00:01:40,370 --> 00:01:43,835
you will not be able to find the gradient directly.

33
00:01:43,834 --> 00:01:48,349
In that case, you can estimate the gradient using finite differences.

34
00:01:48,349 --> 00:01:54,019
Given any policy pi that is defined by an n dimensional parameter vector theta,

35
00:01:54,019 --> 00:01:57,192
find the partial derivative of the objective function

36
00:01:57,192 --> 00:02:00,649
with respect to each dimension k. By adding

37
00:02:00,650 --> 00:02:04,130
a tiny value to that particular dimension and computing

38
00:02:04,129 --> 00:02:07,939
the difference between the resulting policy value and the current one,

39
00:02:07,939 --> 00:02:14,204
collect all the partial derivatives into a single vector to get the combined derivative.

40
00:02:14,205 --> 00:02:17,405
Note that each of these policy evaluations may require

41
00:02:17,405 --> 00:02:20,909
at least to one full episode of interaction in the environment,

42
00:02:20,909 --> 00:02:26,889
and we are doing one evaluation for every parameter dimension at every iteration.

43
00:02:26,889 --> 00:02:30,349
So, this procedure can take a long time.

44
00:02:30,349 --> 00:02:33,829
If you do have access to the underlying policy function,

45
00:02:33,830 --> 00:02:36,153
presumably because you designed it,

46
00:02:36,152 --> 00:02:41,334
then it is more efficient to try and compute the gradient analytically.

47
00:02:41,334 --> 00:02:43,800
But this means we need to compute the gradient of

48
00:02:43,800 --> 00:02:47,880
the expected value of some function. That sounds hard.

49
00:02:47,879 --> 00:02:52,620
Well, it turns out this is a fairly well studied problem.

50
00:02:52,620 --> 00:02:56,625
The reward function we have here is generally referred to as a "score function",

51
00:02:56,625 --> 00:02:59,159
and we can manipulate this expression to

52
00:02:59,159 --> 00:03:02,745
estimate the gradient of the score function more easily.

53
00:03:02,745 --> 00:03:05,625
Let's see how. To simplify things,

54
00:03:05,625 --> 00:03:11,069
we'll consider some arbitrary score function f that is dependent on x.

55
00:03:11,069 --> 00:03:17,144
The values of x are drawn from a probability distribution defined by parameters theta.

56
00:03:17,145 --> 00:03:19,900
This means we can expand the expectation into

57
00:03:19,900 --> 00:03:26,670
an integral or summation of the probability of x given theta times the score function.

58
00:03:26,669 --> 00:03:30,709
Note that here the gradient can be moved into the summation.

59
00:03:30,710 --> 00:03:33,710
This will help us simplify things a bit further.

60
00:03:33,710 --> 00:03:37,310
Now, we can use something called the Likelihood Ratio Trick.

61
00:03:37,310 --> 00:03:42,131
First, we multiply the numerator and denominator by P of X given theta,

62
00:03:42,131 --> 00:03:47,080
and then replace the resulting fraction by the derivative of the log probability.

63
00:03:47,080 --> 00:03:50,105
Trust me, this is a well-known identity.

64
00:03:50,104 --> 00:03:54,644
Finally, we can convert the summation back to an expectation.

65
00:03:54,645 --> 00:03:59,795
Now, know that we only need to compute the derivative of the log probabilities,

66
00:03:59,794 --> 00:04:04,068
not the score function since it does not directly depend on theta.

67
00:04:04,068 --> 00:04:06,284
Back in reinforcement land,

68
00:04:06,284 --> 00:04:09,030
here's what the gradient expression looks like.

69
00:04:09,030 --> 00:04:12,930
With the policy function pi in place of the probability distribution,

70
00:04:12,930 --> 00:04:14,677
and our own score function R,

71
00:04:14,677 --> 00:04:16,844
which can be the sum of all rewards,

72
00:04:16,845 --> 00:04:20,475
discounted rewards or some other value-based function.

73
00:04:20,475 --> 00:04:23,550
You must be wondering like I did when I first saw this,

74
00:04:23,550 --> 00:04:27,134
did we actually gain anything from this whole exercise?

75
00:04:27,134 --> 00:04:30,384
It looks just about as ugly as before.

76
00:04:30,384 --> 00:04:31,990
Let's look at it carefully.

77
00:04:31,990 --> 00:04:34,639
We can compute the score function by interacting

78
00:04:34,639 --> 00:04:37,664
with the environment and summing up the rewards we get.

79
00:04:37,665 --> 00:04:42,575
If we had a policy function implemented using some approximator like a neural net,

80
00:04:42,574 --> 00:04:47,120
then we can compute the log of the output probabilities and the derivative.

81
00:04:47,120 --> 00:04:52,139
Libraries like TensorFlow or PyTorch will happily do this for us.

82
00:04:52,139 --> 00:04:55,439
And finally, we can compute this expectation

83
00:04:55,439 --> 00:04:59,170
stochastically by taking a bunch of different samples.

84
00:04:59,170 --> 00:05:01,629
Using this gradient, we can update

85
00:05:01,629 --> 00:05:05,589
policy parameters to improve the objective function iteratively.

86
00:05:05,589 --> 00:05:10,000
So, we can actually build a concrete algorithm around this.

