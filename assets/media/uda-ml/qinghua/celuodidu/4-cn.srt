1
00:00:00,000 --> 00:00:02,850
拥有目标函数后

2
00:00:02,850 --> 00:00:07,211
现在可以思考如何查找最大化它的策略了

3
00:00:07,211 --> 00:00:10,230
目标函数可能会非常复杂

4
00:00:10,230 --> 00:00:14,070
可以看做有很多峰值和谷值的表面

5
00:00:14,070 --> 00:00:16,875
图中的函数有两个参数

6
00:00:16,875 --> 00:00:20,175
高度表示策略目标值 J(θ)

7
00:00:20,175 --> 00:00:24,317
但是这一概念可以扩展到多个参数

8
00:00:24,317 --> 00:00:27,594
我们对此表面一点也不了解

9
00:00:27,594 --> 00:00:32,128
如何找到目标值最大的位置？

10
00:00:32,128 --> 00:00:37,852
第一个方法是不断到处逼近搜索最佳策略

11
00:00:37,853 --> 00:00:41,585
我们从某个随机策略 π 开始

12
00:00:41,585 --> 00:00:43,570
该策略由参数 θ 定义

13
00:00:43,570 --> 00:00:47,380
通过将该策略应用到环境中评估该策略

14
00:00:47,380 --> 00:00:50,020
这样可以获得一个目标值

15
00:00:50,020 --> 00:00:55,573
可以想象策略位于目标函数表面的某个位置

16
00:00:55,573 --> 00:00:58,450
现在 我们可以稍微更改策略参数

17
00:00:58,450 --> 00:01:01,720
使目标值也更改

18
00:01:01,719 --> 00:01:06,204
可以通过向参数添加一些小的高斯噪点来实现

19
00:01:06,204 --> 00:01:10,539
如果新的策略值比到目前为止的最佳值还要大

20
00:01:10,540 --> 00:01:15,265
则将此策略设为新的最佳策略并迭代

21
00:01:15,265 --> 00:01:18,474
这个一般方法称之为爬山法

22
00:01:18,474 --> 00:01:23,524
相当于沿着目标函数表面走动 直到抵达山丘的顶部

23
00:01:23,525 --> 00:01:25,185
很简单 对吧？

24
00:01:25,185 --> 00:01:28,950
最精彩的是你可以使用任何策略函数

25
00:01:28,950 --> 00:01:32,790
并不一定要可微甚至连续

26
00:01:32,790 --> 00:01:35,385
但是因为采取的是随机步骤

27
00:01:35,385 --> 00:01:39,506
可能并不是爬山的最有效路径

28
00:01:39,506 --> 00:01:42,689
一个小小的改进是在每次迭代时

29
00:01:42,689 --> 00:01:47,730
选择一小部分周围的策略并从中选择最佳策略

30
00:01:47,730 --> 00:01:51,674
通过查看等值线图更容易理解

31
00:01:51,674 --> 00:01:54,509
同样 从随机策略开始

32
00:01:54,510 --> 00:01:57,344
评估它并找到它的位置

33
00:01:57,344 --> 00:02:01,289
通过随机打乱参数生成几个候选策略

34
00:02:01,290 --> 00:02:05,925
并与环境互动以评估每个策略

35
00:02:05,924 --> 00:02:10,155
这样可以了解当前策略的周围情况

36
00:02:10,155 --> 00:02:15,030
现在选择看起来效果最理想的候选策略并迭代

37
00:02:15,030 --> 00:02:19,949
这种变体形式称之为最陡上升爬山法

38
00:02:19,949 --> 00:02:25,125
可以避免选择一个可能会导致次最优解决方案的下个策略

39
00:02:25,125 --> 00:02:27,930
你可能依然会陷入局部最佳状态

40
00:02:27,930 --> 00:02:31,935
但是有一些修改方法可以帮助缓和这种情况

41
00:02:31,935 --> 00:02:36,527
例如 使用随机重新开始法或模拟退火法

42
00:02:36,527 --> 00:02:39,849
模拟退火法使用预定义的计划表

43
00:02:39,849 --> 00:02:42,745
控制如何探索策略空间

44
00:02:42,745 --> 00:02:45,310
从一个很大的噪点参数开始

45
00:02:45,310 --> 00:02:47,754
这时候要探索的临近区域很广泛

46
00:02:47,754 --> 00:02:50,500
我们逐渐减小噪点或半径

47
00:02:50,500 --> 00:02:54,009
并越来越接近最优解决方案

48
00:02:54,009 --> 00:02:56,530
有点像退火和烙铁

49
00:02:56,530 --> 00:03:00,219
通过加热铁块然后逐渐冷却

50
00:03:00,219 --> 00:03:02,650
使铁分子能够进入最优结构

51
00:03:02,650 --> 00:03:07,510
并形成坚硬的金属块 故而得名

52
00:03:07,509 --> 00:03:10,269
我们还可以使方法

53
00:03:10,270 --> 00:03:13,090
更加适应于观察的策略值中的变化

54
00:03:13,090 --> 00:03:17,185
原理是 每当我们发现比之前更好的策略时

55
00:03:17,185 --> 00:03:20,199
我们就有可能更接近最优策略

56
00:03:20,199 --> 00:03:24,939
因此有必要缩小生成下个策略的搜索半径

57
00:03:24,939 --> 00:03:30,400
这就相当于减小或衰减添加的高斯噪点的变化

58
00:03:30,400 --> 00:03:33,564
到目前为止 这就相当于模拟退火法

59
00:03:33,564 --> 00:03:38,590
但是如果我们没有找到更好的策略 则最好增大搜索半径

60
00:03:38,590 --> 00:03:43,543
并继续从当前最佳策略进行探索

61
00:03:43,543 --> 00:03:49,310
对随机性策略搜索进行这一小小的调整使我们不太可能会陷入僵局

62
00:03:49,310 --> 00:03:53,340
尤其是位于目标函数复杂的环境中更是如此

