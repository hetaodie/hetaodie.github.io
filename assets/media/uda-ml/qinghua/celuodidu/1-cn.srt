1
00:00:00,000 --> 00:00:04,960
强化学习的最终目的是学习最优策略

2
00:00:04,960 --> 00:00:07,620
到目前为止 我们查看了各种基于值的方法

3
00:00:07,620 --> 00:00:10,379
我们尝试找到最优值函数

4
00:00:10,380 --> 00:00:13,745
然后隐含地通过该值函数定义策略

5
00:00:13,744 --> 00:00:17,114
例如 使用 Epsilon 贪婪动作选择法

6
00:00:17,114 --> 00:00:19,439
但是 我们可以直接得出最优策略

7
00:00:19,440 --> 00:00:23,970
根本不用管值函数吗？

8
00:00:23,969 --> 00:00:26,519
这正是我们在这节课将讨论的内容

9
00:00:26,519 --> 00:00:29,500
稍后 我们将讨论这两种方法的结合形式

10
00:00:29,500 --> 00:00:31,350
称之为行动者-评论者方法

