1
00:00:00,000 --> 00:00:02,250
经实践证明 策略梯度是一个

2
00:00:02,250 --> 00:00:05,804
非常强大的强化学习方法系列

3
00:00:05,804 --> 00:00:07,469
正如我们在这节课所看到的

4
00:00:07,469 --> 00:00:11,089
与基于值的方法相比 它们具有多个优势

5
00:00:11,089 --> 00:00:13,835
它们可以直接从状态映射到动作

6
00:00:13,835 --> 00:00:16,625
因此不需要处理值函数

7
00:00:16,625 --> 00:00:21,750
你可以使用它们学习连续控制任务

8
00:00:21,750 --> 00:00:23,714
在这种任务中 动作是实数 而不是离散选项

9
00:00:23,714 --> 00:00:28,679
并且它们可以表示作为概率分布的动作选择

10
00:00:28,679 --> 00:00:31,859
因此你可以自动获得真实的随机性策略

11
00:00:31,859 --> 00:00:35,579
基本策略梯度方法出现了多项改进

12
00:00:35,579 --> 00:00:39,559
并且新的方法在不断涌现

13
00:00:39,560 --> 00:00:42,270
这是一个活跃的研究领域

14
00:00:42,270 --> 00:00:45,180
如果你想了解最新的发展动态

15
00:00:45,179 --> 00:00:48,000
可以阅读这一领域最近发表的论文

16
00:00:48,000 --> 00:00:50,219
尝试理解这些算法

17
00:00:50,219 --> 00:00:52,000
并自己实现它们

