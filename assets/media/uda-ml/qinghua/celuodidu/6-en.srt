1
00:00:00,000 --> 00:00:02,940
The first approach we'll use to design

2
00:00:02,940 --> 00:00:06,480
a policy gradient algorithm is Monte Carlo sampling.

3
00:00:06,480 --> 00:00:11,355
We can apply this to tasks that can be split into distinct episodes.

4
00:00:11,355 --> 00:00:15,720
For each episode or trajectory tau generated by interacting with

5
00:00:15,720 --> 00:00:19,905
the policy pi at each time step t within that episode,

6
00:00:19,905 --> 00:00:23,640
we first compute the gradient of the log probabilities produced by

7
00:00:23,640 --> 00:00:28,515
our policy function multiply that by the returns from the rest of the episode,

8
00:00:28,515 --> 00:00:30,045
this is our score function,

9
00:00:30,045 --> 00:00:34,410
and finally update the weights with some small learning rate alpha,

10
00:00:34,409 --> 00:00:38,000
this is called the reinforced algorithm.

