1
00:00:00,000 --> 00:00:02,084
你可能已经注意到了

2
00:00:02,084 --> 00:00:06,580
随机性策略搜索并非始终能达到预期效果

3
00:00:06,580 --> 00:00:09,960
它容易受到所选单个样本的影响

4
00:00:09,960 --> 00:00:14,345
可能会陷入局部最佳状态或需要很长的时间才能收敛

5
00:00:14,345 --> 00:00:17,565
使用此类随机化方法的主要原因之一是

6
00:00:17,565 --> 00:00:21,135
你不需要了解关于目标函数

7
00:00:21,135 --> 00:00:23,760
底层策略或梯度的任何信息

8
00:00:23,760 --> 00:00:25,890
可以当做一个黑匣子

9
00:00:25,890 --> 00:00:27,960
输入一组参数

10
00:00:27,960 --> 00:00:29,580
在环境中运行策略

11
00:00:29,579 --> 00:00:31,500
并获得目标值

12
00:00:31,500 --> 00:00:33,914
但是如果你对策略

13
00:00:33,914 --> 00:00:35,759
和目标函数更了解

14
00:00:35,759 --> 00:00:38,250
并且如果能计算目标函数

15
00:00:38,250 --> 00:00:41,445
相对于策略参数的梯度

16
00:00:41,445 --> 00:00:44,990
这样可以采取更有效的步骤 对吧

17
00:00:44,990 --> 00:00:49,829
梯度始终指向最大变化的方向

18
00:00:49,829 --> 00:00:52,549
因此 不用评估当前策略临近区域的

19
00:00:52,549 --> 00:00:55,459
一堆随机候选策略

20
00:00:55,460 --> 00:01:01,250
你可以直接计算下一组看起来最有希望最佳的策略参数

21
00:01:01,250 --> 00:01:03,765
这就是策略梯度的基础

22
00:01:03,765 --> 00:01:06,409
此类方法的一般流程是

23
00:01:06,409 --> 00:01:09,394
找到当前策略参数的梯度

24
00:01:09,394 --> 00:01:13,834
并以增加梯度的形式更新它们 然后迭代

25
00:01:13,834 --> 00:01:16,429
注意 我们依然需要保持谨慎

26
00:01:16,430 --> 00:01:20,755
并采取由步长 α 定义的小步

27
00:01:20,754 --> 00:01:25,924
因为策略和环境最有可能是随机性

28
00:01:25,924 --> 00:01:29,780
每个策略评估可能无法生成一致的结果

29
00:01:29,780 --> 00:01:34,344
这是通过多个阶段评估策略的原因之一

30
00:01:34,344 --> 00:01:37,370
如果目标函数不可微

31
00:01:37,370 --> 00:01:40,370
底层策略很复杂时会发生这种情况

32
00:01:40,370 --> 00:01:43,835
你将无法直接找到梯度

33
00:01:43,834 --> 00:01:48,349
在这种情况下 你可以使用有限差估算梯度

34
00:01:48,349 --> 00:01:54,019
对于任何一个策略 π 它由 n 维参数向量 θ 定义

35
00:01:54,019 --> 00:01:57,192
找到目标函数相对于

36
00:01:57,192 --> 00:02:00,649
每个维度 k 的偏导数

37
00:02:00,650 --> 00:02:04,130
使该维度与一个很小的值相加

38
00:02:04,129 --> 00:02:07,939
计算形成的策略与当前策略之间的差异

39
00:02:07,939 --> 00:02:14,204
将所有偏导数添加到一个向量中 获得组合导数

40
00:02:14,205 --> 00:02:17,405
注意 每个策略评估都需要

41
00:02:17,405 --> 00:02:20,909
至少与环境完成一次完整的互动阶段

42
00:02:20,909 --> 00:02:26,889
并且在每次迭代时对每个参数维度进行一次评估

43
00:02:26,889 --> 00:02:30,349
因此 这一流程可能会花费很长的时间

44
00:02:30,349 --> 00:02:33,829
如果你可以访问底层策略函数

45
00:02:33,830 --> 00:02:36,153
假设因为你设计了该函数

46
00:02:36,152 --> 00:02:41,334
那么可以更高效地以分析方式尝试和计算梯度

47
00:02:41,334 --> 00:02:43,800
但是这就意味着我们需要计算某个函数的

48
00:02:43,800 --> 00:02:47,880
预期值的梯度 听起来有点难

49
00:02:47,879 --> 00:02:52,620
实际上这是一个得到广泛研究的问题

50
00:02:52,620 --> 00:02:56,625
这里的奖励函数通常称之为得分函数

51
00:02:56,625 --> 00:02:59,159
我们可以操纵该表达式

52
00:02:59,159 --> 00:03:02,745
以便更轻松地估算得分函数的梯度

53
00:03:02,745 --> 00:03:05,625
我们来看看如何计算 为了进行简化

54
00:03:05,625 --> 00:03:11,069
我们将考虑某个依赖于 x 的随机得分函数 f

55
00:03:11,069 --> 00:03:17,144
值 x 来自由参数 θ 定义的概率分布

56
00:03:17,145 --> 00:03:19,900
这意味着我们可以将预期函数展开为

57
00:03:19,900 --> 00:03:26,670
一个积分或 x|θ 的概率乘以得分函数的和

58
00:03:26,669 --> 00:03:30,709
注意 这里的梯度可以移到求和运算中

59
00:03:30,710 --> 00:03:33,710
这样可以进一步简化公式

60
00:03:33,710 --> 00:03:37,310
现在我们可以使用似然比技巧

61
00:03:37,310 --> 00:03:42,131
首先 我们将分子和分母乘以 P[X|θ}

62
00:03:42,131 --> 00:03:47,080
然后将生成的分数替换为对数概率的导数

63
00:03:47,080 --> 00:03:50,105
相信我 这是一个很有名的技巧

64
00:03:50,104 --> 00:03:54,644
最后 我们将求和变回预期函数

65
00:03:54,645 --> 00:03:59,795
注意 我们只需计算对数概率的导数

66
00:03:59,794 --> 00:04:04,068
而不是得分函数 因为它并不直接依赖于 θ

67
00:04:04,068 --> 00:04:06,284
回到强化部分

68
00:04:06,284 --> 00:04:09,030
这是梯度表达式

69
00:04:09,030 --> 00:04:12,930
用策略函数 π 替换概率分布

70
00:04:12,930 --> 00:04:14,677
以及我们自己的得分函数 R

71
00:04:14,677 --> 00:04:16,844
它可以是所有奖励或折扣奖励的和

72
00:04:16,845 --> 00:04:20,475
或者其他基于值的函数

73
00:04:20,475 --> 00:04:23,550
你可能会疑问 就像我第一次见到该表达式一样

74
00:04:23,550 --> 00:04:27,134
我们从这整个练习中获得了任何结果吗？

75
00:04:27,134 --> 00:04:30,384
看起来和之前的一样糟糕

76
00:04:30,384 --> 00:04:31,990
我们来仔细看看它

77
00:04:31,990 --> 00:04:34,639
我们可以通过与环境互动并对获得的奖励求和

78
00:04:34,639 --> 00:04:37,664
计算得分函数

79
00:04:37,665 --> 00:04:42,575
如果我们有一个使用神经网络等估算器实现的策略函数

80
00:04:42,574 --> 00:04:47,120
那么我们可以计算输出概率的对数以及导数

81
00:04:47,120 --> 00:04:52,139
TensorFlow 或 PyTorch 等库可以帮助我们完成这一步

82
00:04:52,139 --> 00:04:55,439
最后 我们可以通过抽取一批不同的样本

83
00:04:55,439 --> 00:04:59,170
以随机方式计算这个预期函数

84
00:04:59,170 --> 00:05:01,629
我们可以使用这个梯度更新策略参数

85
00:05:01,629 --> 00:05:05,589
从而以迭代方式改善目标函数

86
00:05:05,589 --> 00:05:10,000
因此我们可以根据它构建一个具体的算法

