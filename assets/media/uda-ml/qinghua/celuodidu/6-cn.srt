1
00:00:00,000 --> 00:00:02,940
我们设计策略梯度算法时

2
00:00:02,940 --> 00:00:06,480
将用到的第一个方法是蒙特卡洛抽样

3
00:00:06,480 --> 00:00:11,355
我们可以将该方法应用到可以拆分为独特阶段的任务

4
00:00:11,355 --> 00:00:15,720
对于每个阶段或通过与策略 π 互动的轨线 τ

5
00:00:15,720 --> 00:00:19,905
在该阶段中的每个时间步 t

6
00:00:19,905 --> 00:00:23,640
我们首先计算策略函数生成的对数概率的梯度

7
00:00:23,640 --> 00:00:28,515
乘以该阶段剩余时间步的回报

8
00:00:28,515 --> 00:00:30,045
这是我们的得分函数

9
00:00:30,045 --> 00:00:34,410
最后用某个很小的学习速率 α 更新权重

10
00:00:34,409 --> 00:00:38,000
这称之为强化算法

