1
00:00:00,000 --> 00:00:03,270
我们可以将基于策略的方法扩展到

2
00:00:03,270 --> 00:00:08,429
涵盖大型连续性状态空间 方法是使用一个函数来估算策略

3
00:00:08,429 --> 00:00:11,054
就像值函数逼近一样

4
00:00:11,054 --> 00:00:14,459
为此 我们用 θ 参数化策略

5
00:00:14,460 --> 00:00:18,179
θ 可以是神经网络或其他模型的权重

6
00:00:18,179 --> 00:00:23,309
我们的目标变成优化这些参数 以便找到最佳策略

7
00:00:23,309 --> 00:00:28,004
一个简单的近似函数是这些特征的线性组合

8
00:00:28,004 --> 00:00:30,779
但是因为我们希望最终输出

9
00:00:30,780 --> 00:00:34,289
是一个可以从中抽样的概率分布

10
00:00:34,289 --> 00:00:36,549
因此需要进行一些转换

11
00:00:36,549 --> 00:00:41,359
例如 我们可以应用 Softmax 函数

12
00:00:41,359 --> 00:00:44,719
该函数通过除以所有动作的指数之和

13
00:00:44,719 --> 00:00:48,545
指数化线性组合的结果并使其标准化

14
00:00:48,545 --> 00:00:53,300
这样可以形成一组和为 1 的动作概率值

15
00:00:53,299 --> 00:00:57,724
注意 这种转换仅适用于一组离散的动作

16
00:00:57,725 --> 00:01:01,760
例如 OpenAI 中的山地车环境

17
00:01:01,759 --> 00:01:05,644
有两个连续性状态变量 即位置和速度

18
00:01:05,644 --> 00:01:08,973
以及一个离散动作空间

19
00:01:08,974 --> 00:01:10,204
其中包含三个动作 0 表示向左推动

20
00:01:10,204 --> 00:01:13,090
1 表示不推动 2 表示向右推动

21
00:01:13,090 --> 00:01:19,155
你可以使用 Softmax 策略对汽车的行为进行建模并学会优化它

22
00:01:19,155 --> 00:01:21,540
但是还可以有采用单个连续动作变量的版本

23
00:01:21,540 --> 00:01:26,415
负值表示向左推动

24
00:01:26,415 --> 00:01:27,750
0 表示不推动

25
00:01:27,750 --> 00:01:29,275
正值表示向右推动

26
00:01:29,275 --> 00:01:34,635
这样更现实 并且采取的动作更加灵活

27
00:01:34,635 --> 00:01:39,835
但是也更有挑战性 因为动作空间是连续的

28
00:01:39,834 --> 00:01:43,654
当环境具有连续动作空间时

29
00:01:43,655 --> 00:01:45,769
我们可以使用高斯策略

30
00:01:45,769 --> 00:01:48,469
即从高斯或正态分布中抽取动作

31
00:01:48,469 --> 00:01:50,929
该分布的参数

32
00:01:50,930 --> 00:01:53,870
由我们的特征决定

33
00:01:53,870 --> 00:01:56,075
例如 均值 μ 可以是

34
00:01:56,075 --> 00:02:00,140
特征的简单线性组合

35
00:02:00,140 --> 00:02:04,564
σ2 可以是固定的值或以类似的方式参数化

36
00:02:04,564 --> 00:02:09,349
我们可以对任何近似函数应用相同的技巧

37
00:02:09,349 --> 00:02:14,810
这些函数会生成一些值并将这些值变成表示随机性策略的概率

38
00:02:14,810 --> 00:02:18,974
如何判断哪个策略是最佳策略？

39
00:02:18,974 --> 00:02:23,474
我们需要一些目标衡量方法来判断某个策略的效果如何

40
00:02:23,474 --> 00:02:27,329
该衡量方法必须是策略参数的函数

41
00:02:27,330 --> 00:02:29,580
凭直觉判断

42
00:02:29,580 --> 00:02:33,555
该函数需要捕获根据该策略获得的奖励的预期值

43
00:02:33,555 --> 00:02:36,825
我们来介绍下轨线 τ 这一概念

44
00:02:36,824 --> 00:02:40,185
可以将其看做一个完整或部分阶段

45
00:02:40,185 --> 00:02:45,870
因此 可以通过对多个轨线抽样计算预期值

46
00:02:45,870 --> 00:02:49,480
如果你有一个阶段性任务

47
00:02:49,479 --> 00:02:54,744
那么一种方式是使用第一个时间步 G1 的回报均值

48
00:02:54,745 --> 00:02:59,020
即整个阶段的累积折扣回报

49
00:02:59,020 --> 00:03:02,920
等同于起始状态的值

50
00:03:02,919 --> 00:03:07,329
在连续环境中 我们不能依赖于特定的起始状态

51
00:03:07,330 --> 00:03:11,950
因此最好将目标与不依赖于这种状态的衡量方法相关联

52
00:03:11,949 --> 00:03:15,864
一种此类衡量方法是平均或预期状态值

53
00:03:15,865 --> 00:03:20,125
凭直觉来看 你希望选择一个使平均值更高的策略

54
00:03:20,125 --> 00:03:23,280
但是某些状态的发生频率可能高于其他状态

55
00:03:23,280 --> 00:03:26,180
因此现在 每个状态值需要

56
00:03:26,180 --> 00:03:29,375
用相应状态的发生概率进行加权

57
00:03:29,375 --> 00:03:31,955
即平稳概率

58
00:03:31,955 --> 00:03:36,125
计算方法是将状态的发生次数除以

59
00:03:36,125 --> 00:03:39,199
所有状态的总发生次数

60
00:03:39,199 --> 00:03:43,339
相关衡量方法是以类似方式

61
00:03:43,340 --> 00:03:46,250
计算的平均动作值或 Q 值

62
00:03:46,250 --> 00:03:49,849
但是注意直接对策略进行编码的目标是

63
00:03:49,849 --> 00:03:54,419
不需要记录状态值或动作值

64
00:03:54,419 --> 00:03:57,609
因此我们可以直接计算的衡量指标是

65
00:03:57,610 --> 00:04:00,340
每个时间步的平均奖励

66
00:04:00,340 --> 00:04:03,280
有趣的是 所有这些公式在优化策略方面

67
00:04:03,280 --> 00:04:06,580
似乎都可以取得相同的良好效果

68
00:04:06,580 --> 00:04:08,395
选择哪个目标

69
00:04:08,395 --> 00:04:11,335
取决于问题或环境的特性

70
00:04:11,335 --> 00:04:14,349
你可以选择最方便计算

71
00:04:14,349 --> 00:04:17,920
并且最能体现你要实现的目标的公式

