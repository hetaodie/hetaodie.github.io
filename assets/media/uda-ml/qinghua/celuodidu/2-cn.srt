1
00:00:00,000 --> 00:00:04,170
你可能会疑问 既然基于值的方法似乎效果很好

2
00:00:04,170 --> 00:00:08,465
为何要直接查找最优策略

3
00:00:08,465 --> 00:00:11,775
原因有三个方面

4
00:00:11,775 --> 00:00:16,225
简单性 随机性策略和连续动作空间

5
00:00:16,225 --> 00:00:19,546
在 Q 学习等基于值的方法中

6
00:00:19,546 --> 00:00:22,440
我们发明了值函数这一概念

7
00:00:22,440 --> 00:00:26,660
作为查找最优策略的中间步骤

8
00:00:26,660 --> 00:00:29,250
它有助于我们将问题重新描述为

9
00:00:29,250 --> 00:00:32,455
更易于理解和学习的形式

10
00:00:32,455 --> 00:00:36,320
但是如果我们的最终目标是查找最优策略

11
00:00:36,320 --> 00:00:39,090
真的需要该值函数吗？

12
00:00:39,090 --> 00:00:42,725
可以直接估算最优策略吗？

13
00:00:42,725 --> 00:00:45,637
这样的策略看起来如何？

14
00:00:45,637 --> 00:00:47,930
如果我们采用确定性方法

15
00:00:47,930 --> 00:00:53,270
则该策略只需是从状态到动作的映射或函数

16
00:00:53,270 --> 00:00:55,171
对于随机性方法

17
00:00:55,171 --> 00:01:01,235
则为在特定状态下每个动作的条件概率

18
00:01:01,235 --> 00:01:05,870
然后根据该概率分布选择一个动作

19
00:01:05,870 --> 00:01:10,663
这样更简单 因为我们直接处理手头上的问题

20
00:01:10,663 --> 00:01:13,400
并且可以避免处理大量额外的数据

21
00:01:13,400 --> 00:01:17,195
这些数据并非始终有用

22
00:01:17,195 --> 00:01:22,010
例如 状态空间的很大部分可能具有相同的值

23
00:01:22,010 --> 00:01:26,720
以这种方式构建策略使我们能够在可能时进行此类泛化

24
00:01:26,720 --> 00:01:32,745
并更侧重于状态空间的更复杂区域

25
00:01:32,745 --> 00:01:35,860
与基于值的方法相比 基于策略的方法具有的主要优势之一是

26
00:01:35,860 --> 00:01:41,045
它们可以学习真正的随机性策略

27
00:01:41,045 --> 00:01:44,710
这就像从一种特殊的机器中选择一个随机数字

28
00:01:44,710 --> 00:01:47,980
首先 每个数字被选择的概率

29
00:01:47,980 --> 00:01:52,775
取决于可以更改的某些状态变量

30
00:01:52,775 --> 00:01:57,415
相反 当我们对值函数应用 Epsilon 贪婪动作选择法时

31
00:01:57,415 --> 00:02:01,200
的确会增加一些随机性 但并不足够

32
00:02:01,200 --> 00:02:04,480
抛掷硬币 如果正面朝上 遵守确定性策略

33
00:02:04,480 --> 00:02:06,765
因此随机选择一个动作

34
00:02:06,765 --> 00:02:09,730
底层的值函数依然会促使我们

35
00:02:09,730 --> 00:02:13,150
选择特定的动作

36
00:02:13,150 --> 00:02:15,355
我们来看看其中存在的问题

37
00:02:15,355 --> 00:02:19,180
假设你要学习如何玩剪刀石头布

38
00:02:19,180 --> 00:02:22,195
剪刀可以剪开布 布可以包裹石头

39
00:02:22,195 --> 00:02:24,543
石头可以敲碎剪刀

40
00:02:24,543 --> 00:02:28,375
对手同时伸出自己的手

41
00:02:28,375 --> 00:02:32,135
因此你无法使用该信息来决定选择什么动作

42
00:02:32,135 --> 00:02:34,840
实际上 这里的最优策略是

43
00:02:34,840 --> 00:02:38,510
随机均匀地选择一个动作

44
00:02:38,510 --> 00:02:42,982
任何其他策略 例如确定性策略

45
00:02:42,982 --> 00:02:47,800
甚至有一定的不均匀性的随机性策略都会被对手利用

46
00:02:47,800 --> 00:02:52,945
随机性策略有帮助的另一个情形是别名状态

47
00:02:52,945 --> 00:02:58,180
即我们认为相同的两个或多个状态实际上并不相同

48
00:02:58,180 --> 00:03:02,420
没错 从这一角度考虑 它们的环境部分可观察

49
00:03:02,420 --> 00:03:06,425
但是此类情形出现的次数比你想象的要频繁

50
00:03:06,425 --> 00:03:08,815
例如这个网格世界

51
00:03:08,815 --> 00:03:11,253
其中包含平滑的白色单元格

52
00:03:11,253 --> 00:03:13,037
和网状灰色单元格

53
00:03:13,037 --> 00:03:16,396
在底部中间单元格有一根香蕉

54
00:03:16,396 --> 00:03:18,970
在左下角和右下角有一个辣椒

55
00:03:18,970 --> 00:03:23,320
很明显 无论从哪个单元格开始 智能体 George 都需要找到一个可靠的策略

56
00:03:23,320 --> 00:03:28,950
以便抵达香蕉单元格并避免进入辣椒单元格

57
00:03:28,950 --> 00:03:30,490
实际情况是

58
00:03:30,490 --> 00:03:33,100
它所知道的信息是当前单元格是否平滑

59
00:03:33,100 --> 00:03:37,415
两边是否有墙壁

60
00:03:37,415 --> 00:03:39,820
假设这些是可以从环境中获取的

61
00:03:39,820 --> 00:03:42,845
唯一观察结果或特征

62
00:03:42,845 --> 00:03:46,240
它不能了解关于临近单元格的任何信息

63
00:03:46,240 --> 00:03:48,860
当 George 位于顶部中间单元格时

64
00:03:48,860 --> 00:03:53,270
它知道该单元格很平滑并且只有北侧有一面墙

65
00:03:53,270 --> 00:03:57,405
因此它可以向下移动并拿到香蕉

66
00:03:57,405 --> 00:04:01,377
当它不在左上角或右上角的单元格中时

67
00:04:01,377 --> 00:04:03,220
它知道两侧有墙壁

68
00:04:03,220 --> 00:04:07,090
能够知道它处在哪两个极端单元格中

69
00:04:07,090 --> 00:04:12,070
并且能够学会可靠地避免遇到辣椒并进入中间单元格

70
00:04:12,070 --> 00:04:15,475
问题是 当它位于某个网状灰色单元格中时

71
00:04:15,475 --> 00:04:17,730
无法判断位于哪个单元格中

72
00:04:17,730 --> 00:04:20,110
所有特征都一样

73
00:04:20,110 --> 00:04:23,050
如果使用值函数表示法

74
00:04:23,050 --> 00:04:25,838
那么这些单元格的值相等

75
00:04:25,838 --> 00:04:28,870
因为它们都映射到相同的状态

76
00:04:28,870 --> 00:04:32,955
相应的最佳动作肯定也一样

77
00:04:32,955 --> 00:04:35,805
因此 根据经验

78
00:04:35,805 --> 00:04:41,070
George 可能学会从这两个单元格中向右或向左移动

79
00:04:41,070 --> 00:04:45,015
导致整体策略变成这样 这也没关系

80
00:04:45,015 --> 00:04:47,180
但是这个区域存在问题

81
00:04:47,180 --> 00:04:51,585
George 将不断在这两个单元格之间来回移动 永远出不来

82
00:04:51,585 --> 00:04:54,750
借助很小的 Epsilon 贪婪策略

83
00:04:54,750 --> 00:04:57,285
George 也许能够碰巧出来

84
00:04:57,285 --> 00:05:01,250
但是很不高效 可能会花费很长时间

85
00:05:01,250 --> 00:05:03,495
如果 ϵ 很大

86
00:05:03,495 --> 00:05:06,855
可能会导致其他状态的操作很糟糕

87
00:05:06,855 --> 00:05:12,465
可以清晰地看出其他基于值的策略也不太理想

88
00:05:12,465 --> 00:05:14,835
它能采取的最佳做法是

89
00:05:14,835 --> 00:05:18,990
以相同的概率从这些别名状态中向左或向右移动

90
00:05:18,990 --> 00:05:22,340
它更有可能很快离开该陷阱

91
00:05:22,340 --> 00:05:27,675
基于值的方法倾向于学习确定性或近似确定性策略

92
00:05:27,675 --> 00:05:29,610
而在这种情况下基于策略的方法

93
00:05:29,610 --> 00:05:33,805
可以学习期望的随机性策略

94
00:05:33,805 --> 00:05:36,945
探索基于策略的方法的最后一个原因是

95
00:05:36,945 --> 00:05:40,800
它们非常适合连续性动作空间

96
00:05:40,800 --> 00:05:44,865
当我们使用基于值的方法时 即使采用函数逼近器

97
00:05:44,865 --> 00:05:48,840
输出也将由每个动作对应的值组成

98
00:05:48,840 --> 00:05:53,880
如果动作空间是离散的 有一组有限的动作

99
00:05:53,880 --> 00:05:57,770
我们可以轻松地选择值最大的动作

100
00:05:57,770 --> 00:06:00,790
但是如果动作空间是连续的

101
00:06:00,790 --> 00:06:05,470
那么这个最大化运算变成了优化问题本身

102
00:06:05,470 --> 00:06:10,750
就像查找连续性函数的全局最大值 过程不轻松

103
00:06:10,750 --> 00:06:14,035
尤其是如果该函数非凸

104
00:06:14,035 --> 00:06:18,071
更高维度的动作空间存在类似的问题

105
00:06:18,071 --> 00:06:20,705
要评估的潜在动作非常多

106
00:06:20,705 --> 00:06:25,445
如果能将给定状态直接映射到动作 那么会比较理想

107
00:06:25,445 --> 00:06:29,060
即使生成的策略有点复杂

108
00:06:29,060 --> 00:06:33,005
这样可以显著缩短所需的计算时间

109
00:06:33,005 --> 00:06:37,000
这正是基于策略的方法可以达到的效果

