1
00:00:00,000 --> 00:00:03,089
In summary, here's what you learned in this lesson.

2
00:00:03,089 --> 00:00:07,530
Traditional reinforcement learning techniques use a finite MDP to model

3
00:00:07,530 --> 00:00:12,600
an environment which limits us to environments with discrete state and action spaces.

4
00:00:12,599 --> 00:00:16,304
In order to extend our learning algorithms to continuous spaces,

5
00:00:16,304 --> 00:00:18,059
we can do one of two things.

6
00:00:18,059 --> 00:00:23,279
Discretize the state space or directly try to approximate desired value functions.

7
00:00:23,280 --> 00:00:26,655
Discretization can be performed using a constant grid,

8
00:00:26,655 --> 00:00:28,875
tile coding, or course coding.

9
00:00:28,875 --> 00:00:32,899
This indirectly leads to an approximation of the value function.

10
00:00:32,899 --> 00:00:37,195
Directly approximating a continuous value function can be done by first,

11
00:00:37,195 --> 00:00:39,909
defining a feature transformation and

12
00:00:39,909 --> 00:00:43,194
then computing a linear combination of those features.

13
00:00:43,195 --> 00:00:47,755
Using non-linear feature transforms like radial basis functions,

14
00:00:47,755 --> 00:00:49,000
allows us to use

15
00:00:49,000 --> 00:00:54,310
the same linear combination framework to capture some non-linear relationships.

16
00:00:54,310 --> 00:00:59,469
In order to represent non-linear relationships across combinations of features,

17
00:00:59,469 --> 00:01:01,854
we can apply an activation function.

18
00:01:01,854 --> 00:01:07,129
And this sets us up to use deep neural networks for reinforcement learning.

