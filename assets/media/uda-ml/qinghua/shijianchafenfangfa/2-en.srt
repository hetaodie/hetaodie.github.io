1
00:00:00,000 --> 00:00:02,424
We'll continue with the trend of addressing

2
00:00:02,424 --> 00:00:05,494
the prediction problem and reinforcement learning first.

3
00:00:05,495 --> 00:00:07,310
So, given a policy,

4
00:00:07,309 --> 00:00:09,765
how might we estimate its value function?

5
00:00:09,765 --> 00:00:13,169
Let's build up how we did this in the previous lesson.

6
00:00:13,169 --> 00:00:14,550
In our Monte Carlo approach,

7
00:00:14,550 --> 00:00:17,760
the agent interacted with the environment in episodes.

8
00:00:17,760 --> 00:00:19,395
After an episode finished,

9
00:00:19,394 --> 00:00:23,309
we looked at every state-action pair in the sequence.

10
00:00:23,309 --> 00:00:24,974
If it was a first visit,

11
00:00:24,975 --> 00:00:30,205
we calculated the corresponding return and used it to update the action value.

12
00:00:30,204 --> 00:00:34,129
And we did this for many, many episodes.

13
00:00:34,130 --> 00:00:37,160
It's important to note that this algorithm is a solution for

14
00:00:37,159 --> 00:00:42,458
the prediction problem as long as we never change the policy between episodes.

15
00:00:42,459 --> 00:00:45,425
And as long as we run the algorithm for long enough,

16
00:00:45,424 --> 00:00:49,984
we're guaranteed to end with a nice estimate for the action-value function.

17
00:00:49,984 --> 00:00:53,500
But let's move our focus to this update step.

18
00:00:53,500 --> 00:00:58,945
And there is an analogous equation if we instead want to keep track of the state values.

19
00:00:58,945 --> 00:01:01,030
Now for the rest of this video,

20
00:01:01,030 --> 00:01:05,995
what we'll do is adapt this update step to come up with a new algorithm.

21
00:01:05,995 --> 00:01:10,480
Remember that the main idea behind this line is that the value of any state is

22
00:01:10,480 --> 00:01:12,910
defined as the expected return that's likely to

23
00:01:12,909 --> 00:01:15,909
follow that state if the agent follows the policy.

24
00:01:15,909 --> 00:01:20,144
So averaging sampled returns yields a good estimate.

25
00:01:20,144 --> 00:01:25,634
At this point, I'll remind you of the Bellman expectation equation for the state values.

26
00:01:25,635 --> 00:01:28,125
It gives us a way to express the value of

27
00:01:28,125 --> 00:01:32,890
any state in terms of the values of the states that could potentially follow.

28
00:01:32,890 --> 00:01:35,879
And so what if we used the equation just like the one above

29
00:01:35,879 --> 00:01:39,159
it to motivate a slightly different update rule?

30
00:01:39,159 --> 00:01:42,390
So now instead of averaging sampled returns,

31
00:01:42,390 --> 00:01:45,719
we average the sampled value of the sum of

32
00:01:45,719 --> 00:01:50,000
the immediate reward plus the discounted value of the next state.

33
00:01:50,000 --> 00:01:53,084
And you'll notice that we now have an update step that

34
00:01:53,084 --> 00:01:58,199
understands the value of a state in terms of the values of its successor states.

35
00:01:58,200 --> 00:02:00,240
And why would we want to do that anyway?

36
00:02:00,239 --> 00:02:03,314
Well, the first thing to notice is that we've removed

37
00:02:03,314 --> 00:02:06,929
any mention of the return that comes at the end of the episode.

38
00:02:06,930 --> 00:02:10,140
And in fact, this new update step gives us

39
00:02:10,139 --> 00:02:13,514
a way to update the state values after every time step.

40
00:02:13,514 --> 00:02:19,289
To see this, let's consider what would happen at an arbitrary time step t. As always,

41
00:02:19,289 --> 00:02:21,625
we'll use S_sub_t to denote the state.

42
00:02:21,625 --> 00:02:25,550
Say the agent uses the policy to pick its action A_sub_t,

43
00:02:25,550 --> 00:02:30,180
then it receives a reward and next state from the environment.

44
00:02:30,180 --> 00:02:33,960
So then what the prediction algorithm could do is use

45
00:02:33,960 --> 00:02:38,534
this very small time window of information to update value function.

46
00:02:38,534 --> 00:02:44,224
Specifically, we'll update the value of this state at time t. In order to do that,

47
00:02:44,224 --> 00:02:49,104
we begin by looking up the values of the states from time t and time t plus one.

48
00:02:49,104 --> 00:02:51,560
By also plugging in the reward,

49
00:02:51,560 --> 00:02:54,564
we can calculate this entire right hand side,

50
00:02:54,564 --> 00:02:59,050
and that's our new estimate for the value of the state at time t. Now it's

51
00:02:59,050 --> 00:03:01,005
important to realize that we won't have to wait

52
00:03:01,004 --> 00:03:04,079
anymore until the end of the episode to update the values.

53
00:03:04,080 --> 00:03:06,040
And this is the first algorithm you can use for

54
00:03:06,039 --> 00:03:09,334
the prediction problem when working with continuous tasks.

55
00:03:09,335 --> 00:03:11,784
But before detailing the algorithm in full,

56
00:03:11,784 --> 00:03:15,245
let's talk a bit more about what this update step accomplishes.

57
00:03:15,245 --> 00:03:17,640
So at an arbitrary time step t,

58
00:03:17,639 --> 00:03:19,768
before the agent takes an action,

59
00:03:19,769 --> 00:03:21,640
it's best estimate for the value of

60
00:03:21,639 --> 00:03:25,004
the current state is just what's contained in the value function.

61
00:03:25,004 --> 00:03:29,355
But then once it takes an action and receives the reward and next state,

62
00:03:29,355 --> 00:03:30,849
well that's new information.

63
00:03:30,849 --> 00:03:35,819
And we can use it to express an alternative estimate for the value of the same state,

64
00:03:35,819 --> 00:03:38,924
but in terms of the value of the state that followed.

65
00:03:38,925 --> 00:03:42,745
And we refer to this new estimate as the TD target.

66
00:03:42,745 --> 00:03:45,550
So then what this entire update equation does is

67
00:03:45,550 --> 00:03:48,985
find some middle ground between the two estimates.

68
00:03:48,985 --> 00:03:53,335
You will set the value of alpha according to which estimate you trust more.

69
00:03:53,335 --> 00:03:54,905
To see this more clearly,

70
00:03:54,905 --> 00:03:57,210
we'll rewrite the updated equation.

71
00:03:57,210 --> 00:04:01,270
Note that alpha must be set to a number between zero and one.

72
00:04:01,270 --> 00:04:03,320
When alpha is set to one,

73
00:04:03,319 --> 00:04:06,021
the new estimate is just the TD target,

74
00:04:06,021 --> 00:04:09,289
where we completely ignore and replace the previous estimate.

75
00:04:09,289 --> 00:04:11,724
And if we were to set alpha to zero,

76
00:04:11,724 --> 00:04:16,500
we would completely ignore the target and keep the old estimate unchanged.

77
00:04:16,500 --> 00:04:20,814
This is not something that we'd ever want to do because then our agent would never learn.

78
00:04:20,814 --> 00:04:24,910
But it will prove useful to set alpha to a small number that's close to zero.

79
00:04:24,910 --> 00:04:27,700
And in general, the smaller alpha is,

80
00:04:27,699 --> 00:04:30,769
the less we trust the target when performing an update,

81
00:04:30,769 --> 00:04:34,704
and the more we rely on our existing estimate of the state value.

82
00:04:34,704 --> 00:04:39,488
You'll soon get a chance to experiment with setting the value of alpha yourself.

83
00:04:39,488 --> 00:04:43,810
We'll now put this update step back into the context of the full algorithm,

84
00:04:43,810 --> 00:04:48,120
which we'll call One-Step temporal difference or TD for short.

85
00:04:48,120 --> 00:04:51,189
Of course the One-Step just refers to the fact that we

86
00:04:51,189 --> 00:04:54,785
update the value function after any individual step.

87
00:04:54,785 --> 00:04:57,760
You'll also see it referred to as TD zero.

88
00:04:57,759 --> 00:04:59,680
The algorithm is designed to determine

89
00:04:59,680 --> 00:05:02,530
the state value function corresponding to a policy,

90
00:05:02,529 --> 00:05:04,504
which we denote by pi.

91
00:05:04,504 --> 00:05:08,629
We begin by initializing the value of each state to zero.

92
00:05:08,629 --> 00:05:10,149
Then, at every time step,

93
00:05:10,149 --> 00:05:12,139
the agent interacts with the environment,

94
00:05:12,139 --> 00:05:15,310
choosing actions that are dictated by the policy.

95
00:05:15,310 --> 00:05:20,584
And immediately after receiving the reward and next state from the environment,

96
00:05:20,584 --> 00:05:23,959
it updates the value function for the previous state.

97
00:05:23,959 --> 00:05:26,884
So this is the version for continuous tasks.

98
00:05:26,884 --> 00:05:30,724
And as long as the agent interacts with the environment for long enough,

99
00:05:30,725 --> 00:05:34,715
the algorithm should return a nice estimate for the value function.

100
00:05:34,714 --> 00:05:37,868
Okay. And what about episodic tasks?

101
00:05:37,869 --> 00:05:39,605
Well in that case,

102
00:05:39,605 --> 00:05:44,773
we need only check at every time step if the most recent state is a terminal state.

103
00:05:44,773 --> 00:05:50,570
And if so, we run the update step one last time to update the preceding state.

104
00:05:50,569 --> 00:05:53,189
Then, we start a new episode,

105
00:05:53,189 --> 00:05:56,560
but as you can see the idea is basically the same.

