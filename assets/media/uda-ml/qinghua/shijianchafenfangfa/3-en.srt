1
00:00:00,000 --> 00:00:01,409
Earlier in this lesson,

2
00:00:01,409 --> 00:00:07,044
we detailed an algorithm to calculate the state value function corresponding to a policy.

3
00:00:07,044 --> 00:00:09,000
Now, we'll adopt that algorithm to

4
00:00:09,000 --> 00:00:12,589
instead return an estimate of the action-value function.

5
00:00:12,589 --> 00:00:17,589
So let's recall exactly how one-step temporal difference works.

6
00:00:17,589 --> 00:00:20,170
The agent interacts with the environment.

7
00:00:20,170 --> 00:00:21,560
At time step zero,

8
00:00:21,559 --> 00:00:24,179
it receives some state S_sub_zero.

9
00:00:24,179 --> 00:00:28,045
Then, it uses the policy to pick an action.

10
00:00:28,045 --> 00:00:33,414
Immediately afterwards, the agent receives a reward and next state.

11
00:00:33,414 --> 00:00:36,659
At this point, the agent uses its experience to

12
00:00:36,659 --> 00:00:40,429
update its estimate for the value of the state from time zero.

13
00:00:40,429 --> 00:00:42,359
At the next point in time,

14
00:00:42,359 --> 00:00:44,714
the agent chooses an action, again,

15
00:00:44,715 --> 00:00:46,275
by consulting the policy,

16
00:00:46,274 --> 00:00:49,229
then it receives a reward and next state.

17
00:00:49,229 --> 00:00:54,504
Then, it uses that information to update the value of the state from time one.

18
00:00:54,505 --> 00:00:56,984
Then, the process continues where the agent

19
00:00:56,984 --> 00:01:00,339
always consults the same policy to pick an action,

20
00:01:00,340 --> 00:01:02,985
receives a reward and next state,

21
00:01:02,984 --> 00:01:05,534
and then updates the value function.

22
00:01:05,534 --> 00:01:07,254
So the question is,

23
00:01:07,254 --> 00:01:12,879
how might we adapt this process to instead return an estimate of the action values?

24
00:01:12,879 --> 00:01:19,194
Well, instead of having an updated equation that relates the values of successive states,

25
00:01:19,194 --> 00:01:22,809
what we'll instead need to do is have an update equation

26
00:01:22,810 --> 00:01:27,400
that relates the values of successive state-action pairs.

27
00:01:27,400 --> 00:01:32,658
Then, instead of updating the values after each state is received,

28
00:01:32,658 --> 00:01:37,215
the agent will instead update the values after each action is chosen.

29
00:01:37,215 --> 00:01:39,079
But that's the only difference,

30
00:01:39,079 --> 00:01:42,819
and if the agent interacts with the environment for long enough,

31
00:01:42,819 --> 00:01:46,269
it will have a pretty good estimate of the action-value function.

32
00:01:46,269 --> 00:01:48,034
In the upcoming concepts,

33
00:01:48,034 --> 00:01:53,000
you'll learn more about how to use this algorithm in the search for an optimal policy.

