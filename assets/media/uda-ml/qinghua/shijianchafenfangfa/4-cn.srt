1
00:00:00,000 --> 00:00:02,265
我们已经解决了预测问题

2
00:00:02,265 --> 00:00:04,320
可以讨论控制问题了

3
00:00:04,320 --> 00:00:07,695
智能体如何确定最优策略？

4
00:00:07,695 --> 00:00:12,115
我们将采用估算动作值函数的算法

5
00:00:12,115 --> 00:00:14,655
在此算法中 选择每个动作后

6
00:00:14,654 --> 00:00:16,934
智能体都更新估值

7
00:00:16,934 --> 00:00:19,140
需要注意的是

8
00:00:19,140 --> 00:00:22,820
智能体在每个时间步都使用相同的策略来选择动作

9
00:00:22,820 --> 00:00:25,995
但是现在 为了调整该算法以便生成控制算法

10
00:00:25,995 --> 00:00:27,990
我们将逐渐更改该策略

11
00:00:27,989 --> 00:00:31,199
使其在每个时间步都越来越完善

12
00:00:31,199 --> 00:00:33,810
我们将使用的方法之一

13
00:00:33,810 --> 00:00:36,480
与蒙特卡洛方法非常相似

14
00:00:36,479 --> 00:00:41,779
即在每个时间步使用一个针对当前动作估值的

15
00:00:41,780 --> 00:00:44,850
Epsilon 贪婪策略选择一个动作

16
00:00:44,850 --> 00:00:46,259
在初始时间步

17
00:00:46,259 --> 00:00:48,795
我们先将 ε 设为 1

18
00:00:48,795 --> 00:00:54,734
然后根据对等概率随机策略选择 A0 和 A1

19
00:00:54,734 --> 00:00:58,420
在选择某个动作之后的未来所有时间步

20
00:00:58,420 --> 00:01:00,600
我们都更新动作值函数

21
00:01:00,600 --> 00:01:03,675
并构建相应的 Epsilon 贪婪策略

22
00:01:03,674 --> 00:01:07,349
只要我们为 ε 指定合适的值

23
00:01:07,349 --> 00:01:11,614
该算法就肯定会收敛于最优策略

24
00:01:11,614 --> 00:01:14,104
该算法的名称叫做 Sarsa 0

25
00:01:14,105 --> 00:01:16,424
简称为 Sarsa

26
00:01:16,424 --> 00:01:23,564
得名原因是每个动作值更新都使用状态动作奖励

27
00:01:23,564 --> 00:01:27,000
后续状态 后续动作 互动元组

