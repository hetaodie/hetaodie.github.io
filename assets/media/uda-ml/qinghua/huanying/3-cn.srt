1
00:00:00,000 --> 00:00:02,700
在整个课程中 我们将了解

2
00:00:02,700 --> 00:00:05,400
互动学习概念

3
00:00:05,400 --> 00:00:07,980
对于强化学习

4
00:00:07,980 --> 00:00:12,060
我们将学习者或决策制定者称作智能体

5
00:00:12,060 --> 00:00:14,865
但是我非常喜欢狗狗

6
00:00:14,865 --> 00:00:17,789
因此我们将智能体看做一只

7
00:00:17,789 --> 00:00:21,899
刚刚出生的小狗 对世界一点也不了解

8
00:00:21,899 --> 00:00:23,324
当它第一次睁开眼睛时

9
00:00:23,324 --> 00:00:26,414
首先看到的是它的主人

10
00:00:26,414 --> 00:00:31,799
假设主人向小狗发出行为指令

11
00:00:31,800 --> 00:00:34,950
小狗看着主人的手势并根据观察的结果

12
00:00:34,950 --> 00:00:38,130
做出如何响应的决定

13
00:00:38,130 --> 00:00:42,075
当然 它还没有兴趣做出正确的响应

14
00:00:42,075 --> 00:00:46,645
主要是不惹麻烦 甚至获得奖赏

15
00:00:46,645 --> 00:00:50,850
刚出生的小狗非常复杂

16
00:00:50,850 --> 00:00:55,649
根本无法计算它在任何时间可以采取的动作数量

17
00:00:55,649 --> 00:00:58,409
此外 它尚不知道任何动作

18
00:00:58,409 --> 00:01:01,904
或者这些动作带来的影响

19
00:01:01,905 --> 00:01:05,579
我们需要尝试这些指令并看看发生的结果

20
00:01:05,579 --> 00:01:08,564
小狗会如何对主人的指令做出响应？

21
00:01:08,564 --> 00:01:12,969
此刻 它不可能对一种动作更偏爱

22
00:01:12,969 --> 00:01:15,659
假设它只是随机选择一种动作

23
00:01:15,659 --> 00:01:20,444
当然 它根本不知道自己的动作有什么含义

24
00:01:20,444 --> 00:01:23,069
在这种情况下 它选择坐下

25
00:01:23,069 --> 00:01:24,689
完成这一动作后

26
00:01:24,689 --> 00:01:26,500
它等待主人的响应

27
00:01:26,500 --> 00:01:29,364
可以想象等待非常长

28
00:01:29,364 --> 00:01:33,409
因为完全不清楚下一步会发生什么

29
00:01:33,409 --> 00:01:34,994
完成动作后

30
00:01:34,995 --> 00:01:37,255
它从主人那获得了反馈

31
00:01:37,254 --> 00:01:40,799
在此示例中 它获得了一次奖赏

32
00:01:40,799 --> 00:01:45,129
这很令他兴奋 它表现得不错

33
00:01:45,129 --> 00:01:46,814
我们假设

34
00:01:46,814 --> 00:01:49,829
它的唯一目标是最大化奖励

35
00:01:49,829 --> 00:01:52,974
它希望一直表现得很正面

36
00:01:52,974 --> 00:01:57,125
暂时是这种假设 以后也一直是这种假设

37
00:01:57,125 --> 00:01:59,069
现在主人发出了新的指令

38
00:01:59,069 --> 00:02:04,079
它需要选择下一个响应

39
00:02:04,079 --> 00:02:06,844
你认为呢？坐下似乎很合适

40
00:02:06,844 --> 00:02:08,984
但是现在指令不一样了

41
00:02:08,985 --> 00:02:12,660
因此不同的动作可能更合适

42
00:02:12,659 --> 00:02:15,944
它决定再次随机选择一个动作

43
00:02:15,944 --> 00:02:20,764
在此示例中 它决定奔跑并等待主人的响应

44
00:02:20,764 --> 00:02:25,554
糟糕 主人给出了负面反馈

45
00:02:25,555 --> 00:02:29,085
现在不清楚的是 奔跑是一般的负面动作

46
00:02:29,085 --> 00:02:33,719
还是不适合这个特定指令

47
00:02:33,719 --> 00:02:36,870
它只能通过系统地尝试和测试假设

48
00:02:36,870 --> 00:02:42,300
与主人进行更多的互动来发现结果

49
00:02:42,300 --> 00:02:45,689
这一流程继续下去

50
00:02:45,689 --> 00:02:48,449
每一刻 小狗都采取一种动作

51
00:02:48,449 --> 00:02:52,909
同时获得反馈并获得主人的响应

52
00:02:52,909 --> 00:02:54,210
在每一刻

53
00:02:54,210 --> 00:03:00,355
它都尽量采取合适的动作 使主人开心

54
00:03:00,354 --> 00:03:04,694
咋一看 当我们以这种方式看待互动时

55
00:03:04,694 --> 00:03:07,009
一切似乎都很简单

56
00:03:07,009 --> 00:03:11,269
毕竟小狗与主人的互动非常具有系统性

57
00:03:11,270 --> 00:03:15,284
虽然它可能需要一段时间才明白所发生的情况

58
00:03:15,284 --> 00:03:18,775
但是它应该最终会明白的

59
00:03:18,775 --> 00:03:20,985
在你学习这门课程的过程中

60
00:03:20,985 --> 00:03:25,460
你会发现这种小狗情形非常复杂

61
00:03:25,460 --> 00:03:27,900
例如 它需要在以下两种情形之间找到平衡点

62
00:03:27,900 --> 00:03:32,159
一方面 探索如何选择动作的潜在假设

63
00:03:32,159 --> 00:03:33,750
另一方面

64
00:03:33,750 --> 00:03:38,835
探索已有的可行有限知识

65
00:03:38,835 --> 00:03:43,590
也就是说 它应该满足于一个奖赏 还是希望获得更多的奖赏

66
00:03:43,590 --> 00:03:47,490
这是强化学习中的一个非常重要的概念

67
00:03:47,490 --> 00:03:49,740
虽然对于如何平衡这两项要求

68
00:03:49,740 --> 00:03:52,215
有一些广泛采用的技巧

69
00:03:52,215 --> 00:03:56,655
但是这个问题推动了一个完整的强化学习分支领域的出现

70
00:03:56,655 --> 00:04:00,419
并且成为一个热点研究领域

71
00:04:00,419 --> 00:04:02,549
另一个值得注意的要点是

72
00:04:02,550 --> 00:04:05,700
如果小狗真的是强化学习智能体

73
00:04:05,699 --> 00:04:09,714
那么它不仅关心现在可以获得的奖励

74
00:04:09,715 --> 00:04:12,210
而且要最大化在一生中

75
00:04:12,210 --> 00:04:16,650
可以获得的奖励数量

76
00:04:16,649 --> 00:04:18,929
因此发现在短期内效果不错的策略

77
00:04:18,930 --> 00:04:22,035
可能相对来说比较简单

78
00:04:22,035 --> 00:04:24,630
但是如果要得出长期效果不错的策略

79
00:04:24,629 --> 00:04:28,300
小狗就必须更加聪明

80
00:04:28,300 --> 00:04:34,439
通常 这种通过经验学习规律的概念从学术上来说比较有趣

81
00:04:34,439 --> 00:04:36,569
例如 如果你训练过小狗

82
00:04:36,569 --> 00:04:39,774
可能就会发现过程并不简单

83
00:04:39,774 --> 00:04:43,349
它的第一反应可能是每当我坐下来就能获得奖励

84
00:04:43,350 --> 00:04:48,635
而不是每当主人说坐下来 我坐下来

85
00:04:48,634 --> 00:04:50,687
就能获得奖励

86
00:04:50,687 --> 00:04:54,432
虽然你的智能体通常会犯错

87
00:04:54,432 --> 00:04:58,000
但是向你保证 它会学会更多知识的

