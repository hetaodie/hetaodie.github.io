1
00:00:00,000 --> 00:00:02,700
Throughout this course, we'll concern ourselves

2
00:00:02,700 --> 00:00:05,400
with the idea of learning from interaction.

3
00:00:05,400 --> 00:00:07,980
In the field of reinforcement learning,

4
00:00:07,980 --> 00:00:12,060
we refer to the learner or decision maker as the agent.

5
00:00:12,060 --> 00:00:14,865
But I really like dogs, so we'll instead,

6
00:00:14,865 --> 00:00:17,789
think of the agent as a small puppy born into

7
00:00:17,789 --> 00:00:21,899
the world without any understanding of how anything works.

8
00:00:21,899 --> 00:00:23,324
And when he opens his eyes,

9
00:00:23,324 --> 00:00:26,414
the first thing he observes is his owner.

10
00:00:26,414 --> 00:00:31,799
Now, say the owner communicates to the puppy how she would like it to behave.

11
00:00:31,800 --> 00:00:34,950
The puppy looks at the command and based on that observation,

12
00:00:34,950 --> 00:00:38,130
he's expected to choose how to respond.

13
00:00:38,130 --> 00:00:42,075
Of course, he hasn't invested interest in responding appropriately,

14
00:00:42,075 --> 00:00:46,645
mostly to stay out of trouble but maybe to even get a treat.

15
00:00:46,645 --> 00:00:50,850
Now, newborn puppies are complicated creatures and it's

16
00:00:50,850 --> 00:00:55,649
impossible to count the number of actions that he could take at any point in time.

17
00:00:55,649 --> 00:00:58,409
Furthermore, he doesn't know what any of the actions

18
00:00:58,409 --> 00:01:01,904
do yet or what effect will have on the world.

19
00:01:01,905 --> 00:01:05,579
So we'll have to try them out and see what happens.

20
00:01:05,579 --> 00:01:08,564
So how will he respond to his owners command?

21
00:01:08,564 --> 00:01:12,969
At this point, he has no reason to favor any action over the others.

22
00:01:12,969 --> 00:01:15,659
And so, let's say he just picks one at random.

23
00:01:15,659 --> 00:01:20,444
Of course, with full understanding that he has no idea what he's doing.

24
00:01:20,444 --> 00:01:23,069
In this case, he decides to sit.

25
00:01:23,069 --> 00:01:24,689
After taking this action,

26
00:01:24,689 --> 00:01:26,500
he waits for a response.

27
00:01:26,500 --> 00:01:29,364
And I have to imagine the wait is pretty scary

28
00:01:29,364 --> 00:01:33,409
as it's entirely uncertain what will happen next.

29
00:01:33,409 --> 00:01:34,994
In response to his action,

30
00:01:34,995 --> 00:01:37,255
he receives feedback from his owner.

31
00:01:37,254 --> 00:01:40,799
In this case, he receives a single treat.

32
00:01:40,799 --> 00:01:45,129
That's encouraging and he's off to a great start.

33
00:01:45,129 --> 00:01:46,814
We'll assume that in general,

34
00:01:46,814 --> 00:01:49,829
his only goal is to maximize reward.

35
00:01:49,829 --> 00:01:52,974
He'd like it to be as positive as possible.

36
00:01:52,974 --> 00:01:57,125
That holds for now and at every point in the future.

37
00:01:57,125 --> 00:01:59,069
But now the owner has given

38
00:01:59,069 --> 00:02:04,079
a new command and it's time for him to choose his next response.

39
00:02:04,079 --> 00:02:06,844
What do you think? Sitting seemed to perform well.

40
00:02:06,844 --> 00:02:08,984
But now, the command looks different.

41
00:02:08,985 --> 00:02:12,660
So maybe a different action is more appropriate.

42
00:02:12,659 --> 00:02:15,944
He decides to again pick an action randomly.

43
00:02:15,944 --> 00:02:20,764
In this case, he decides to run and waits for a response.

44
00:02:20,764 --> 00:02:25,554
And, oh no, he received discouraging feedback from his owner.

45
00:02:25,555 --> 00:02:29,085
Now, it's unclear if this is just a bad action in general

46
00:02:29,085 --> 00:02:33,719
or if he should just not run in response to this particular observation.

47
00:02:33,719 --> 00:02:36,870
He can only discover this through interacting more with

48
00:02:36,870 --> 00:02:42,300
his owner through systematically proposing and testing hypotheses.

49
00:02:42,300 --> 00:02:45,689
And so the process continues where at each point in time,

50
00:02:45,689 --> 00:02:48,449
the puppy takes an action and then simultaneously

51
00:02:48,449 --> 00:02:52,909
receives feedback and an updated observation from his owner.

52
00:02:52,909 --> 00:02:54,210
At each point in time,

53
00:02:54,210 --> 00:03:00,355
he does his best to execute the appropriate action to make his owner happy.

54
00:03:00,354 --> 00:03:04,694
Now at first glance, it could seem that when we view interaction along these lines,

55
00:03:04,694 --> 00:03:07,009
everything is pretty simple.

56
00:03:07,009 --> 00:03:11,269
After all, the way the puppy interacts with his owner is quite systematic.

57
00:03:11,270 --> 00:03:15,284
And while it may take him some time to get an idea of what's happening,

58
00:03:15,284 --> 00:03:18,775
he should be able to figure it out eventually.

59
00:03:18,775 --> 00:03:20,985
But as you progress through this course,

60
00:03:20,985 --> 00:03:25,460
you'll see that this puppy situation is surprisingly complex.

61
00:03:25,460 --> 00:03:27,900
For instance, he'll have to find the balance with on

62
00:03:27,900 --> 00:03:32,159
the one hand exploring potential hypotheses for how to choose actions.

63
00:03:32,159 --> 00:03:33,750
And on the other,

64
00:03:33,750 --> 00:03:38,835
exploiting his limited knowledge about what he already knows should work well.

65
00:03:38,835 --> 00:03:43,590
That is, should he settle for just one treat or should he aim higher?

66
00:03:43,590 --> 00:03:47,490
This is a very important concept in reinforcement learning.

67
00:03:47,490 --> 00:03:49,740
While there are some widely adopted techniques

68
00:03:49,740 --> 00:03:52,215
for balancing these two competing requirements,

69
00:03:52,215 --> 00:03:56,655
this question has inspired an entire subfield of reinforcement learning,

70
00:03:56,655 --> 00:04:00,419
and remains an area of active research.

71
00:04:00,419 --> 00:04:02,549
Another thing that's important to note is that if

72
00:04:02,550 --> 00:04:05,700
our puppy is truly a reinforcement learning agent,

73
00:04:05,699 --> 00:04:09,714
he's not just concerned with the reward he can get now.

74
00:04:09,715 --> 00:04:12,210
Instead, his goal is to maximize

75
00:04:12,210 --> 00:04:16,650
the total number of treats he can get over his entire lifetime.

76
00:04:16,649 --> 00:04:18,929
So while it might be relatively simple to

77
00:04:18,930 --> 00:04:22,035
derive a strategy that works well in the short term,

78
00:04:22,035 --> 00:04:24,630
he'll have to be a bit more clever if he wants to

79
00:04:24,629 --> 00:04:28,300
find strategies that take longer to pay off.

80
00:04:28,300 --> 00:04:34,439
In general, this idea of learning from experience is intellectually quite interesting.

81
00:04:34,439 --> 00:04:36,569
For instance, if you've ever tried to train a dog

82
00:04:36,569 --> 00:04:39,774
perhaps you've noticed that it's not so easy.

83
00:04:39,774 --> 00:04:43,349
His first instinct might be whenever I sit I receive

84
00:04:43,350 --> 00:04:48,635
a treat rather than whenever my owner says sit and then I sit,

85
00:04:48,634 --> 00:04:50,687
I receive a treat.

86
00:04:50,687 --> 00:04:54,432
Even though it seems like a reasonable mistake to make,

87
00:04:54,432 --> 00:04:58,000
we guarantee you that your agents will know better.

