1
00:00:00,000 --> 00:00:05,655
Let's begin with a very small world and an agent who lives in it.

2
00:00:05,655 --> 00:00:09,359
The world is primarily composed of nice patches of grass,

3
00:00:09,359 --> 00:00:13,169
but one of the four locations in the world has a large mountain.

4
00:00:13,169 --> 00:00:16,530
We can think of each of these four possible locations in

5
00:00:16,530 --> 00:00:19,780
the world as states in the environment.

6
00:00:19,780 --> 00:00:21,054
At each point in time,

7
00:00:21,054 --> 00:00:23,684
let's say the agent can only move up, down,

8
00:00:23,684 --> 00:00:28,829
left or right and can only take actions that lead it to not fall off the grid.

9
00:00:28,829 --> 00:00:33,085
Here, the arrows show the possible movements that we allow the agent.

10
00:00:33,085 --> 00:00:35,850
let's also say the goal of the agent is to get to

11
00:00:35,850 --> 00:00:39,550
the bottom right hand corner of the world as quickly as possible.

12
00:00:39,549 --> 00:00:42,029
We'll think of this as an episodic task where

13
00:00:42,030 --> 00:00:45,798
an episode finishes when the agent reaches the goal,

14
00:00:45,798 --> 00:00:49,535
so we won't have to worry about transitions away from the school state.

15
00:00:49,534 --> 00:00:55,429
Furthermore, say that the agent receives a reward of a negative one for most transitions,

16
00:00:55,429 --> 00:00:58,560
but if an action leads it to encounter a mountain,

17
00:00:58,560 --> 00:01:02,980
it receives a reward of negative three and if it reaches the goal state,

18
00:01:02,979 --> 00:01:05,125
it gets a reward of five.

19
00:01:05,125 --> 00:01:06,870
In the dynamic programming setting,

20
00:01:06,870 --> 00:01:09,450
the agent knows this rewards structure and

21
00:01:09,450 --> 00:01:12,590
it knows how transitions happen between states.

22
00:01:12,590 --> 00:01:17,445
So the agent already knows everything about how the environment operates.

23
00:01:17,444 --> 00:01:19,054
So now what?

24
00:01:19,055 --> 00:01:23,010
How can the agent use this information to find the optimal policy?

