1
00:00:00,000 --> 00:00:03,930
Q 学习是一种 TD 学习的离线策略变体

2
00:00:03,930 --> 00:00:08,085
我们看看如何调整它 以便用于函数逼近

3
00:00:08,085 --> 00:00:12,975
和 SARSA 一样 我们随机地初始化参数

4
00:00:12,974 --> 00:00:17,655
并根据 Q 值定义一个 Epsilon 贪婪策略 π

5
00:00:17,655 --> 00:00:21,765
在多个阶段中 我们使用该 Epsilon 贪婪策略

6
00:00:21,765 --> 00:00:26,385
不断采取动作并观察奖励和下个状态

7
00:00:26,385 --> 00:00:29,280
主要区别在于更新步骤

8
00:00:29,280 --> 00:00:33,855
我们不再根据相同的 Epsilon 贪婪策略选择下个动作

9
00:00:33,854 --> 00:00:36,219
而是贪婪地选择一个动作

10
00:00:36,219 --> 00:00:40,009
该动作将最大化后续预期值

11
00:00:40,009 --> 00:00:43,039
注意 我们并非实际地采取该动作

12
00:00:43,039 --> 00:00:45,875
而是用于执行更新步骤

13
00:00:45,875 --> 00:00:49,039
实际上 我们甚至不需要选择该动作

14
00:00:49,039 --> 00:00:52,774
我们可以在下个状态使用最大 Q 值

15
00:00:52,774 --> 00:00:56,539
因此 Q 学习被视为离线策略方法

16
00:00:56,539 --> 00:00:58,939
我们根据一个策略采取动作

17
00:00:58,939 --> 00:01:01,265
即 Epsilon 贪婪策略 π

18
00:01:01,265 --> 00:01:05,555
并根据另一个策略更新值 即贪婪策略

19
00:01:05,555 --> 00:01:09,560
虽然这两个策略都根据相同的底层 Q 值进行定义

20
00:01:09,560 --> 00:01:12,405
但这两个策略实际上是不同的策略

21
00:01:12,405 --> 00:01:18,010
对于阶段性任务 Q 学习算法看起来就是这样

22
00:01:18,010 --> 00:01:22,840
我们也可以将同一算法用于连续性任务

23
00:01:22,840 --> 00:01:26,049
将整个无止境的序列当做一个很长的阶段

24
00:01:26,049 --> 00:01:30,265
或者稍加修改 取消阶段这一概念

25
00:01:30,265 --> 00:01:32,125
这两种形式是等效的

26
00:01:32,125 --> 00:01:36,219
在这两种情况下 我们可能需要额外的条件

27
00:01:36,219 --> 00:01:41,459
来判断我们是否完全学会了任务或完全出错了

28
00:01:41,459 --> 00:01:45,854
我们来比较下 SARSA 和 Q 学习

29
00:01:45,855 --> 00:01:51,765
SARSA 是一个异同策略算法 遵守的是要学习的同一策略

30
00:01:51,765 --> 00:01:55,439
通常更适合在线学习

31
00:01:55,439 --> 00:01:56,849
在任何时间点

32
00:01:56,849 --> 00:02:01,890
你都使用通过与环境互动获得的最新策略

33
00:02:01,890 --> 00:02:07,034
但是 如果你使用 Epsilon 贪婪动作选择法

34
00:02:07,034 --> 00:02:09,804
以便促进探索效果

35
00:02:09,804 --> 00:02:14,224
那么这种随机性还会影响到学习的 Q 值

36
00:02:14,224 --> 00:02:18,310
另一方面 Q 学习是一种新策略方法

37
00:02:18,310 --> 00:02:20,530
即选择动作时遵守的策略

38
00:02:20,530 --> 00:02:23,604
与要学习的策略不同

39
00:02:23,604 --> 00:02:26,484
这样可能会导致糟糕的在线效果

40
00:02:26,485 --> 00:02:28,240
因为你要学习的策略

41
00:02:28,240 --> 00:02:31,165
与遵守的策略之间不相关

42
00:02:31,164 --> 00:02:34,479
不过比较好的一点是动作选择的 Epsilon 贪婪特性

43
00:02:34,479 --> 00:02:38,649
不会影响到学习的 Q 值

44
00:02:38,650 --> 00:02:41,800
因此这两种方法各有利弊

45
00:02:41,800 --> 00:02:46,135
要选择哪个方法 取决于环境特性

46
00:02:46,134 --> 00:02:51,579
以及你是更关心在线性能还是学习准确性

47
00:02:51,580 --> 00:02:56,110
Q 学习等新策略方法

48
00:02:56,110 --> 00:02:58,780
之所以变得如此热门

49
00:02:58,780 --> 00:03:02,860
是因为它们使智能体在环境中采取的动作与学习流程不再相关

50
00:03:02,860 --> 00:03:07,690
这样我们便有机会构建学习算法的不同变体

51
00:03:07,689 --> 00:03:10,900
例如 你可以在采取动作时遵守更加探索性的策略

52
00:03:10,900 --> 00:03:14,245
并学习最优值函数

53
00:03:14,245 --> 00:03:17,020
没错 在线性能会很糟糕

54
00:03:17,020 --> 00:03:22,750
但是在某个时间点 我们可以停止探索并遵守最优策略 以获得更好的结果

55
00:03:22,750 --> 00:03:27,960
实际上 采取动作时遵守的策略不一定必须只限于智能体

56
00:03:27,960 --> 00:03:30,899
人类可以演示所采取的动作

57
00:03:30,899 --> 00:03:34,939
智能体可以通过观察这些动作的效果学习规律

58
00:03:34,939 --> 00:03:38,659
并且离线学习或批量学习过程更加轻松

59
00:03:38,659 --> 00:03:42,799
因为不需要在每个时间步更新策略

60
00:03:42,800 --> 00:03:45,380
这对可靠地训练强化学习神经网络来说非常关键

61
00:03:45,379 --> 00:03:49,740
很快我们就会感受到这一点

