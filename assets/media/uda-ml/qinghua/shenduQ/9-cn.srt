1
00:00:00,000 --> 00:00:02,160
现在我们已经准备好了解

2
00:00:02,160 --> 00:00:06,179
深度 Q 学习算法并自己实现它

3
00:00:06,179 --> 00:00:10,500
该算法中有两个交错的主要流程

4
00:00:10,500 --> 00:00:14,279
一个流程是对环境取样 方法是执行动作

5
00:00:14,279 --> 00:00:18,224
并以回放存储器的形式存储观察的经验元组

6
00:00:18,225 --> 00:00:22,440
另一个流程是随机地从该存储器中选择一小批元组

7
00:00:22,440 --> 00:00:27,425
并使用梯度下降更新步骤从该批次中学习规律

8
00:00:27,425 --> 00:00:31,480
这两个流程并非直接相互依赖

9
00:00:31,480 --> 00:00:35,500
因此 你可以完成多个取样步骤 然后完成一个学习步骤

10
00:00:35,500 --> 00:00:39,295
或者具有不同随机批次的多个学习步骤

11
00:00:39,295 --> 00:00:43,130
该算法的剩余步骤旨在支持这些步骤

12
00:00:43,130 --> 00:00:46,320
一开始你需要初始化空的回放存储器

13
00:00:46,320 --> 00:00:49,612
注意存储器是有限的

14
00:00:49,612 --> 00:00:52,128
因此你可能需要使用循环 Q

15
00:00:52,128 --> 00:00:56,429
保留最近 n 个经验元组

16
00:00:56,429 --> 00:01:01,530
然后 你还需要初始化神经网络的参数或权重

17
00:01:01,530 --> 00:01:05,010
你可以采取一些最佳做法

18
00:01:05,010 --> 00:01:08,295
例如 从正态分布中随机抽取一些权重

19
00:01:08,295 --> 00:01:12,900
方差等于每个神经元输入数量的两倍

20
00:01:12,900 --> 00:01:16,020
Keras 和 TensorFlow 等现代深度学习库

21
00:01:16,019 --> 00:01:19,170
通常都包含这些初始化方法

22
00:01:19,170 --> 00:01:21,840
因此你不需要自己实现这些方法

23
00:01:21,840 --> 00:01:24,734
要采用固定 Q 目标技巧

24
00:01:24,734 --> 00:01:31,064
你需要第二组参数 w- 可以初始化为 w

25
00:01:31,064 --> 00:01:35,924
注意 这一特定算法专门用于视频游戏

26
00:01:35,924 --> 00:01:41,009
对于每个阶段和该阶段中的每个时间步 t

27
00:01:41,010 --> 00:01:43,650
你会观察原始屏幕图像或输入帧 xt

28
00:01:43,650 --> 00:01:47,520
并需要将其转换为灰阶形式 剪切为正方形 等等

29
00:01:47,519 --> 00:01:50,009
此外 为了捕获时间关系

30
00:01:50,010 --> 00:01:55,344
你可以堆叠一些输入帧以构建每个状态向量

31
00:01:55,344 --> 00:02:00,233
我们用函数 phi 表示这个预处理和堆叠操作

32
00:02:00,233 --> 00:02:05,479
该函数会接收一系列的帧并形成组合表示法

33
00:02:05,480 --> 00:02:08,599
注意 如果我们要堆叠帧 例如四个帧

34
00:02:08,599 --> 00:02:12,169
则需要对前三个时间步进行特殊处理

35
00:02:12,169 --> 00:02:15,439
例如 我们可以将这些丢失的帧当做空帧

36
00:02:15,439 --> 00:02:17,780
使用第一帧的副本

37
00:02:17,780 --> 00:02:23,555
或跳过存储经验帧这一步 直到获得完整的序列

38
00:02:23,555 --> 00:02:27,500
在现实中 你无法立即运行学习步骤

39
00:02:27,500 --> 00:02:32,055
需要等待 直到存储器中有足够数量的元组

40
00:02:32,055 --> 00:02:35,575
注意 在每个阶段之后 我们不清空存储器

41
00:02:35,574 --> 00:02:40,839
这样可以跨阶段地回忆和形成批量经验

42
00:02:40,840 --> 00:02:45,610
DQN 论文中用到了很多其他技巧和优化方式

43
00:02:45,610 --> 00:02:47,170
例如奖励裁剪 误差裁剪

44
00:02:47,169 --> 00:02:51,219
将过去的动作存储为状态向量的一部分

45
00:02:51,219 --> 00:02:52,914
处理终止状态

46
00:02:52,914 --> 00:02:54,729
挖掘一段时间内的 ϵ 等

47
00:02:54,729 --> 00:02:57,384
建议你在自己实现该算法之前

48
00:02:57,384 --> 00:03:02,155
阅读这篇论文 尤其是方法部分

49
00:03:02,155 --> 00:03:05,379
注意 你可能需要选择应用哪些技巧

50
00:03:05,379 --> 00:03:08,780
并针对不同类型的环境调整这些技巧

