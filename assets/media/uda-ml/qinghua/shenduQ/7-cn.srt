1
00:00:00,000 --> 00:00:04,230
经验回放和使用它来训练

2
00:00:04,230 --> 00:00:08,914
强化学习神经网络并不是什么新的理念

3
00:00:08,914 --> 00:00:14,414
它一开始用于更高效地使用观察的经验

4
00:00:14,414 --> 00:00:17,429
对于基本的在线 Q 学习算法

5
00:00:17,429 --> 00:00:20,609
我们与环境进行互动并且在每个时间步

6
00:00:20,609 --> 00:00:24,600
获得一个动作 状态 奖励 下个状态元组

7
00:00:24,600 --> 00:00:27,435
从中学习规律 然后丢弃

8
00:00:27,434 --> 00:00:31,189
在下个时间步移到下个元组

9
00:00:31,190 --> 00:00:32,925
似乎有点浪费

10
00:00:32,924 --> 00:00:38,399
如果我们将这些经验元组存储在某个地方 我们可能会从中学习更多规律

11
00:00:38,399 --> 00:00:44,164
此外 某些状态很罕见 某些动作的代价很高

12
00:00:44,164 --> 00:00:47,875
因此最好能够回顾这些经验

13
00:00:47,875 --> 00:00:51,450
这正是回放缓冲区的作用

14
00:00:51,450 --> 00:00:55,920
当我们与环境互动时 我们将每个经验元组存储在这个缓冲区中

15
00:00:55,920 --> 00:01:01,300
然后从中抽取一小批元组以进行学习

16
00:01:01,299 --> 00:01:06,185
因此 我们能够从单个元组中多次学习规律

17
00:01:06,185 --> 00:01:11,469
回顾罕见的状态并更充分地利用经验

18
00:01:11,469 --> 00:01:14,909
经验回放还能够解决另一个关键问题

19
00:01:14,909 --> 00:01:18,944
这正是 DQN 所利用的方面

20
00:01:18,944 --> 00:01:21,494
如果你思考获得的经验

21
00:01:21,495 --> 00:01:26,939
就会发现每个动作 AT 都以某种方式影响下个状态 ST

22
00:01:26,939 --> 00:01:32,099
意味着一系列经验元组可能关系非常紧密

23
00:01:32,099 --> 00:01:36,089
按顺序从这些经验中学习规律的简单 Q 学习方法

24
00:01:36,090 --> 00:01:41,585
面临受到这种联系影响的风险

25
00:01:41,585 --> 00:01:45,674
借助经验回放 我们可以随机地从缓冲区中取样

26
00:01:45,674 --> 00:01:50,519
不一定必须是存储元组的同一顺序

27
00:01:50,519 --> 00:01:53,969
这有助于打破相互之间的联系

28
00:01:53,969 --> 00:01:58,525
并最终防止动作值严重振动或发散

29
00:01:58,525 --> 00:02:01,743
理解起来可能有点困难

30
00:02:01,742 --> 00:02:04,114
我们来看一个示例

31
00:02:04,114 --> 00:02:09,710
我正在学习打网球并对着墙壁不断练习

32
00:02:09,710 --> 00:02:13,295
我更擅长于正手击球 而不是反手击球

33
00:02:13,294 --> 00:02:15,619
我可以比较直地击中墙壁

34
00:02:15,620 --> 00:02:18,515
因此球不断回到位于右手边的同一区域

35
00:02:18,514 --> 00:02:22,189
我可以不断正手击球

36
00:02:22,189 --> 00:02:26,525
如果我是一个在线 Q 学习智能体 学习如何玩网球

37
00:02:26,525 --> 00:02:28,534
我可能会学会以下规律

38
00:02:28,534 --> 00:02:30,724
当球朝着我的右侧飞来时

39
00:02:30,724 --> 00:02:32,780
我应该用正手击球

40
00:02:32,780 --> 00:02:38,194
一开始不太确定 但是当我不断练习击球时 越来越自信可以这么操作

41
00:02:38,194 --> 00:02:41,639
很棒！我的正手击球技巧很厉害

42
00:02:41,639 --> 00:02:45,199
但是我没有探索剩下的状态空间

43
00:02:45,199 --> 00:02:47,294
解决方法是使用 Epsilon 贪婪策略

44
00:02:47,294 --> 00:02:51,559
以很小的概率随机采取动作

45
00:02:51,560 --> 00:02:57,020
我会尝试不同的状态和动作组合 有时候会出错

46
00:02:57,020 --> 00:03:00,980
但是我最终会得出最佳整体策略

47
00:03:00,979 --> 00:03:03,229
当球朝着我的右侧飞来时 用正手击球

48
00:03:03,229 --> 00:03:06,724
朝着我的左侧飞来时 使用反手

49
00:03:06,724 --> 00:03:11,120
很棒 这个学习策略似乎很适合 Q 表格

50
00:03:11,120 --> 00:03:17,034
我们认为这个非常简单的状态空间只有两个离散状态

51
00:03:17,034 --> 00:03:23,344
但是对于连续状态空间 情况会变得很糟糕 我们来看看

52
00:03:23,344 --> 00:03:29,090
首先 球可以从最左端和最右端之间的任何方向飞来

53
00:03:29,090 --> 00:03:31,340
如果我尝试将这个范围

54
00:03:31,340 --> 00:03:34,784
离散化为很小的分桶 则会有太多可能性

55
00:03:34,784 --> 00:03:37,699
如果我学习的策略存在漏洞呢

56
00:03:37,699 --> 00:03:41,984
我在练习时可能没有遇到一些状态或情况

57
00:03:41,985 --> 00:03:47,260
相反 使用函数逼近器会更合适

58
00:03:47,259 --> 00:03:53,379
例如线性组合径向基函数核或 Q 网络 它们可以泛化空间内的学习效果

59
00:03:53,379 --> 00:03:58,354
现在 每次球飞向右侧时 我成功地用正手击中

60
00:03:58,354 --> 00:04:00,814
值函数稍微发生了变化

61
00:04:00,814 --> 00:04:05,000
它对球的精确区域更加确定

62
00:04:05,000 --> 00:04:10,710
但是也提高了整个状态空间内的正手击球值

63
00:04:10,710 --> 00:04:13,925
效果是偏离精确位置的幅度越来越小

64
00:04:13,925 --> 00:04:16,185
但是会逐渐累积起来

65
00:04:16,185 --> 00:04:20,120
这正是在玩球时尝试学习所发生的情况

66
00:04:20,120 --> 00:04:23,235
即按顺序处理每个经验元组

67
00:04:23,235 --> 00:04:26,375
例如 如果正手击球很直

68
00:04:26,375 --> 00:04:29,819
我很可能会在同一位置遇到球

69
00:04:29,819 --> 00:04:32,954
这样会生成一个与上个状态非常相似的状态

70
00:04:32,954 --> 00:04:36,019
因此我再次使用正手 如果成功了

71
00:04:36,019 --> 00:04:39,529
则增加了我认为正手很合适的信念

72
00:04:39,529 --> 00:04:42,559
我很容易陷入这种循环中

73
00:04:42,560 --> 00:04:47,540
最终 如果一段时间内没有见到太多朝着左侧飞来的球

74
00:04:47,540 --> 00:04:50,420
那么在整个状态空间内 正手击球的值

75
00:04:50,420 --> 00:04:54,074
比反手击球的值更大

76
00:04:54,074 --> 00:04:58,865
我的策略变成无论看到球朝着哪个方向飞来 我都选择正手

77
00:04:58,865 --> 00:05:02,925
太糟糕 如何解决这一问题？

78
00:05:02,925 --> 00:05:07,100
首先是停止一边练习一边学习

79
00:05:07,100 --> 00:05:10,400
这时候最适合尝试不同的击球方式

80
00:05:10,399 --> 00:05:14,489
随机地击球并探索状态空间

81
00:05:14,490 --> 00:05:17,569
然后必须记住我的互动

82
00:05:17,569 --> 00:05:21,034
在哪些情形下哪些击球方式效果很好 等等

83
00:05:21,035 --> 00:05:24,463
当我短暂休息或回家休息的时候

84
00:05:24,463 --> 00:05:29,180
最适合回忆这些经验并从中学习规律

85
00:05:29,180 --> 00:05:33,995
主要优势是现在拥有一个更全面的示例集合

86
00:05:33,995 --> 00:05:35,954
有时候球朝着右侧飞来

87
00:05:35,954 --> 00:05:37,654
有时候朝着左侧飞来

88
00:05:37,654 --> 00:05:40,434
有时候正手击球 有时候反手击球

89
00:05:40,435 --> 00:05:43,939
我可以通过这些示例总结规律

90
00:05:43,939 --> 00:05:47,089
按照任何顺序回顾它们

91
00:05:47,089 --> 00:05:50,239
这样可以避免固定在一个状态空间区域

92
00:05:50,240 --> 00:05:54,819
或不断加强相同的动作

93
00:05:54,819 --> 00:05:56,314
在一轮学习之后

94
00:05:56,314 --> 00:05:59,464
我可以使用更新的值函数继续玩球

95
00:05:59,464 --> 00:06:04,389
再次收集一系列的经验并批量学习

96
00:06:04,389 --> 00:06:09,125
这样 经验回放可以帮助我们学习更鲁棒的策略

97
00:06:09,125 --> 00:06:12,439
这种策略不会受到一系列观察经验元组

98
00:06:12,439 --> 00:06:17,295
之间的固有联系的影响

99
00:06:17,295 --> 00:06:18,425
仔细思考就会发现

100
00:06:18,425 --> 00:06:20,920
这种方法基本上是构建一个样本数据库

101
00:06:20,920 --> 00:06:24,210
然后从中学习映射

102
00:06:24,209 --> 00:06:27,219
这样的话 经验回放可以帮助我们

103
00:06:27,220 --> 00:06:30,370
将强化学习问题（或至少值学习部分）

104
00:06:30,370 --> 00:06:34,090
简化为监督学习情形

105
00:06:34,089 --> 00:06:35,814
这很机智

106
00:06:35,814 --> 00:06:38,769
然后 我们可以将在监督学习环境中

107
00:06:38,769 --> 00:06:40,379
开发的学习技巧和最佳做法

108
00:06:40,379 --> 00:06:44,589
应用到强化学习中

109
00:06:44,589 --> 00:06:48,009
甚至可以完善这一理念

110
00:06:48,009 --> 00:06:53,000
例如 使罕见或更重要的经验元组优先级更高

