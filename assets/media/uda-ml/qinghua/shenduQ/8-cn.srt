1
00:00:00,000 --> 00:00:04,915
经验回放可以帮助我们解决一种类型的联系

2
00:00:04,915 --> 00:00:08,550
即元组之间的连续经验

3
00:00:08,550 --> 00:00:13,285
Q 学习还容易受到一种联系的影响

4
00:00:13,285 --> 00:00:18,745
Q 学习是一种时间差分 (TD) 学习 对吧

5
00:00:18,745 --> 00:00:21,220
这里 R 加上 γ

6
00:00:21,219 --> 00:00:25,585
乘以下个状态的最大潜在值称为 TD 目标

7
00:00:25,585 --> 00:00:28,179
我们的目标是缩小该目标

8
00:00:28,179 --> 00:00:32,104
和当前预测 Q 值之间的差异

9
00:00:32,104 --> 00:00:34,944
这个差异是 TD 误差

10
00:00:34,945 --> 00:00:37,869
这里的 TD 目标应该取代的是

11
00:00:37,869 --> 00:00:40,939
真值函数 qπ(S,A)

12
00:00:40,939 --> 00:00:44,414
我们不知道真值函数是什么

13
00:00:44,414 --> 00:00:49,164
我们一开始使用 qπ 定义平方误差损失

14
00:00:49,164 --> 00:00:55,564
并针对 w 差分化 以便获得梯度下降更新规则

15
00:00:55,564 --> 00:01:01,007
qπ 并不依赖于我们的函数逼近器或其参数

16
00:01:01,008 --> 00:01:04,885
因此形成一个简单的导数 即更新规则

17
00:01:04,885 --> 00:01:10,300
但是 TD 目标依赖于这些参数

18
00:01:10,299 --> 00:01:13,239
因此直接将真值函数 qπ 替换成这样的目标

19
00:01:13,239 --> 00:01:17,494
在数学上不成立

20
00:01:17,495 --> 00:01:20,484
在现实中 可以忽略该问题

21
00:01:20,484 --> 00:01:24,099
因为每个更新都会使参数出现小小的变化

22
00:01:24,099 --> 00:01:26,619
我们基本上是朝着正确的方向前进

23
00:01:26,620 --> 00:01:32,938
如果将 α 设为 1 并跳转到目标 那么可能会越过目标

24
00:01:32,938 --> 00:01:35,515
并进入错误的区域

25
00:01:35,515 --> 00:01:39,250
此外 如果我们使用查询表或字典 则不是什么大问题

26
00:01:39,250 --> 00:01:44,319
因为每个状态动作对的 Q 值会单独存储

27
00:01:44,319 --> 00:01:49,429
但是当我们使用函数逼近时 会显著影响到学习效果

28
00:01:49,430 --> 00:01:55,040
因为所有 Q 值都通过函数参数固有地联系到一起

29
00:01:55,040 --> 00:01:59,650
你可能会问“经验回放不会解决该问题吗？” 

30
00:01:59,650 --> 00:02:04,109
它解决的是有点相似 但是稍微不同的问题

31
00:02:04,109 --> 00:02:06,519
对于经验回放 我们通过不按顺序随机地取样元组

32
00:02:06,519 --> 00:02:11,414
打破了连续经验元组之间的联系

33
00:02:11,414 --> 00:02:17,394
这里的问题是 目标和我们要更改的参数之间有联系

34
00:02:17,395 --> 00:02:21,040
这就像追赶一个移动的目标

35
00:02:21,039 --> 00:02:23,534
实际上 更糟糕

36
00:02:23,534 --> 00:02:25,710
就像骑在驴背上训练驴沿直线行走

37
00:02:25,710 --> 00:02:29,490
但是杆子上的胡萝卜会随之摆动

38
00:02:29,490 --> 00:02:32,600
没错 驴子可能会向前行走

39
00:02:32,599 --> 00:02:36,534
胡萝卜通常都被甩到前方 使驴子够不着

40
00:02:36,534 --> 00:02:38,444
但是和常识相反

41
00:02:38,444 --> 00:02:41,174
实际效果并不是你想象的那样

42
00:02:41,175 --> 00:02:43,620
胡萝卜更有可能四处随机甩动

43
00:02:43,620 --> 00:02:46,650
使驴子每一步走起来都很不平稳

44
00:02:46,650 --> 00:02:50,039
并以不可预测的复杂形式

45
00:02:50,039 --> 00:02:54,299
影响到下个目标位置

46
00:02:54,300 --> 00:02:56,460
驴子很有可能会到处跳动

47
00:02:56,460 --> 00:03:00,260
最终放弃行走

48
00:03:00,259 --> 00:03:02,669
你应该从驴背上下来

49
00:03:02,669 --> 00:03:05,934
站在一个位置并用杆子摆动胡萝卜

50
00:03:05,935 --> 00:03:07,935
当驴子抵达该位置后

51
00:03:07,935 --> 00:03:09,449
再向前移动几步

52
00:03:09,449 --> 00:03:11,729
重新用杆子摆动胡萝卜并重复这一流程

53
00:03:11,729 --> 00:03:15,929
这样做本质上是将目标位置与驴子的动作拆分开来

54
00:03:15,930 --> 00:03:20,985
为驴子提供一个更加稳定的学习环境

55
00:03:20,985 --> 00:03:25,200
我们也可以在 Q 学习中进行几乎一样的操作

56
00:03:25,199 --> 00:03:29,359
将用于生成目标的函数参数固定起来

57
00:03:29,360 --> 00:03:33,060
用 w- 表示的固定参数

58
00:03:33,060 --> 00:03:37,949
本质上是在学习过程中不会更改的 w 副本

59
00:03:37,949 --> 00:03:41,339
在现实中 我们将 w 替换为 w-

60
00:03:41,340 --> 00:03:46,235
用它来生成目标和更改 w 并持续一定数量的学习步骤

61
00:03:46,235 --> 00:03:49,140
然后使用最新的 w 更新 w-

62
00:03:49,139 --> 00:03:52,689
再次学习一定数量的步骤 以此类推

63
00:03:52,689 --> 00:03:55,919
这样可以使目标和参数拆分开来

64
00:03:55,919 --> 00:03:58,454
使学习算法更加稳定

65
00:03:58,455 --> 00:04:02,680
并且不太可能会发散或振荡

