1
00:00:00,000 --> 00:00:06,145
Several improvements to the original Deep Q-Learning algorithm have been suggested.

2
00:00:06,145 --> 00:00:09,761
We look at three of the more prominent ones,

3
00:00:09,761 --> 00:00:15,225
Double DQNs, Prioritized Replay and Dueling Networks.

4
00:00:15,225 --> 00:00:17,590
The first problem we are going to address is

5
00:00:17,590 --> 00:00:20,920
the overestimation of action values that Q-Learning is prone to.

6
00:00:20,920 --> 00:00:23,740
Let's look back at the update rule for

7
00:00:23,740 --> 00:00:27,370
Q-Learning with function approximation and focus on the TD target.

8
00:00:27,370 --> 00:00:31,000
Here the max operation is necessary

9
00:00:31,000 --> 00:00:35,080
to find the best possible value we could get from the next state.

10
00:00:35,080 --> 00:00:36,880
To understand this better,

11
00:00:36,880 --> 00:00:41,230
let's rewrite the target and expand the max operation.

12
00:00:41,230 --> 00:00:46,270
It is just a more efficient way of saying that we want to obtain the Q-value for

13
00:00:46,270 --> 00:00:50,050
the state S prime and the action that results in

14
00:00:50,050 --> 00:00:54,645
the maximum Q-value among all possible actions from that state.

15
00:00:54,645 --> 00:00:56,260
When we write it this way,

16
00:00:56,260 --> 00:01:00,595
we can see that it's possible for the arg max operation to make a mistake,

17
00:01:00,595 --> 00:01:02,730
especially in the early stages.

18
00:01:02,730 --> 00:01:06,550
Why? Because the Q-values are still evolving and we may

19
00:01:06,550 --> 00:01:10,890
not have gathered enough information to figure out the best action.

20
00:01:10,890 --> 00:01:14,230
The accuracy of our Q-values depends a lot on

21
00:01:14,230 --> 00:01:18,725
what actions have been tried and what neighboring states have been explored.

22
00:01:18,725 --> 00:01:23,542
In fact, it has been shown that this results in an overestimation of Q-values

23
00:01:23,542 --> 00:01:28,760
since we always pick the maximum among a set of noisy numbers.

24
00:01:28,760 --> 00:01:32,685
So, maybe we shouldn't blindly trust these values.

25
00:01:32,685 --> 00:01:36,620
What can we do to make our estimation more robust?

26
00:01:36,620 --> 00:01:41,920
One idea that has been shown to work very well in practice is called Double Q-Learning.

27
00:01:41,920 --> 00:01:45,830
Where we select the best action using one set of parameters,

28
00:01:45,830 --> 00:01:50,990
W, but evaluate it using a different set of parameters W prime.

29
00:01:50,990 --> 00:01:52,630
It's basically like having

30
00:01:52,630 --> 00:01:57,100
two separate function approximaters that must agree on the best action.

31
00:01:57,100 --> 00:02:01,678
If W picks an action that is not the best according to W prime,

32
00:02:01,678 --> 00:02:04,285
then the Q-value returned is not that high.

33
00:02:04,285 --> 00:02:08,125
In the long run, this prevents the algorithm from propagating

34
00:02:08,125 --> 00:02:11,140
incidental higher rewards that may have been obtained

35
00:02:11,140 --> 00:02:15,065
by chance and don't reflect long term returns.

36
00:02:15,065 --> 00:02:16,510
Now, you may be thinking,

37
00:02:16,510 --> 00:02:19,730
where do we get this second set of parameters from?

38
00:02:19,730 --> 00:02:23,680
In the original formulation of Double Q-Learning,

39
00:02:23,680 --> 00:02:27,910
you would basically maintain two value functions and randomly choose one of

40
00:02:27,910 --> 00:02:32,620
them to update at each step using the other only for evaluating actions.

41
00:02:32,620 --> 00:02:36,250
But when using DQNs with fixed Q targets,

42
00:02:36,250 --> 00:02:39,100
we already have an alternate set of parameters.

43
00:02:39,100 --> 00:02:41,110
Remember W minus.

44
00:02:41,110 --> 00:02:46,090
It turns out that since W minus is kept frozen for a while it is different enough

45
00:02:46,090 --> 00:02:51,420
from W that it can be reused for this purpose, and that's it.

46
00:02:51,420 --> 00:02:54,850
This simple modification keeps Q-values in check,

47
00:02:54,850 --> 00:03:00,235
preventing them from exploding in the early stages of learning or fluctuating later on.

48
00:03:00,235 --> 00:03:04,270
The resulting policies have also been shown to perform significantly

49
00:03:04,270 --> 00:03:08,970
better than Vanilla DQNs. All right.

50
00:03:08,970 --> 00:03:12,935
The next issue we'll look at is related to experience replay.

51
00:03:12,935 --> 00:03:15,770
Recall the basic idea behind it.

52
00:03:15,770 --> 00:03:18,635
We interact with the environment to collect experience tuples,

53
00:03:18,635 --> 00:03:24,030
save them in a buffer and then later we randomly sample a batch to learn from.

54
00:03:24,030 --> 00:03:26,610
This helps us break the correlation between

55
00:03:26,610 --> 00:03:31,215
consecutive experiences and stabilizes our learning algorithm.

56
00:03:31,215 --> 00:03:32,460
So far so good,

57
00:03:32,460 --> 00:03:36,975
but some of these experiences may be more important for learning than others.

58
00:03:36,975 --> 00:03:41,520
Moreover, these important experiences might occur infrequently.

59
00:03:41,520 --> 00:03:44,700
If we sample the batch as uniformly then

60
00:03:44,700 --> 00:03:48,520
these experiences have a very small chance of getting selected.

61
00:03:48,520 --> 00:03:51,945
And since buffers are practically limited in capacity,

62
00:03:51,945 --> 00:03:55,465
older important experiences may get lost.

63
00:03:55,465 --> 00:03:59,220
This is where the idea of prioritized experience replay comes in.

64
00:03:59,220 --> 00:04:04,260
But what criteria should we use to assign priorities to each tuple?

65
00:04:04,260 --> 00:04:07,465
One approach is to use the TD error delta.

66
00:04:07,465 --> 00:04:08,715
The bigger the error,

67
00:04:08,715 --> 00:04:11,905
the more we expect to learn from that tuple.

68
00:04:11,905 --> 00:04:15,735
So let's take the magnitude of this error as a measure of priority

69
00:04:15,735 --> 00:04:20,565
and store it along with each corresponding tuple in the replay buffer.

70
00:04:20,565 --> 00:04:26,430
When creating batches, we can use this value to compute a sampling probability,

71
00:04:26,430 --> 00:04:32,433
select any tuple i with a probability equal to its priority value Pi,

72
00:04:32,433 --> 00:04:36,390
normalized by the sum of all priority values in the replay buffer.

73
00:04:36,390 --> 00:04:38,250
And when a tuple is picked,

74
00:04:38,250 --> 00:04:44,835
we can update its priority with a newly computed TD error using the latest Q-values.

75
00:04:44,835 --> 00:04:48,060
This seems to work fairly well and has been shown to

76
00:04:48,060 --> 00:04:52,680
reduce the number of batch updates needed to learn a value function.

77
00:04:52,680 --> 00:04:55,170
There are a couple of things we can improve.

78
00:04:55,170 --> 00:04:58,405
First, note that if the TD error is zero,

79
00:04:58,405 --> 00:05:00,450
then the priority value of the tuple and

80
00:05:00,450 --> 00:05:04,125
hence its probability of being picked will also be zero.

81
00:05:04,125 --> 00:05:07,350
Zero or very low TD error doesn't

82
00:05:07,350 --> 00:05:11,115
necessarily mean we have nothing more to learn from such a tuple.

83
00:05:11,115 --> 00:05:13,380
It might be the case that our estimate was

84
00:05:13,380 --> 00:05:17,555
close due to the limited samples we visited till that point.

85
00:05:17,555 --> 00:05:21,555
So, to prevent such tuples from being starved for selection,

86
00:05:21,555 --> 00:05:26,190
we can add a small constant e to every parity value.

87
00:05:26,190 --> 00:05:31,680
Another issue along similar lines is that greedily using this priority values may lead to

88
00:05:31,680 --> 00:05:34,830
a small subset of experiences being replayed over and

89
00:05:34,830 --> 00:05:39,130
over resulting in a sort of over-fitting to that subset.

90
00:05:39,130 --> 00:05:44,680
To avoid this, we can reintroduce some element of uniform random sampling.

91
00:05:44,680 --> 00:05:47,345
This adds another hyper parameter a,

92
00:05:47,345 --> 00:05:52,120
which we use to redefine the sampling probability as priority Pi,

93
00:05:52,120 --> 00:05:56,025
to the power of a divided by the sum of all priorities,

94
00:05:56,025 --> 00:05:59,120
Pk, each raised to the power a.

95
00:05:59,120 --> 00:06:01,920
We can control how much we want to use

96
00:06:01,920 --> 00:06:05,383
priority versus randomness by varying this parameter.

97
00:06:05,383 --> 00:06:06,890
Where a equals zero,

98
00:06:06,890 --> 00:06:13,452
corresponds to pure uniform randomness and a equals one, only uses priorities.

99
00:06:13,452 --> 00:06:15,996
When we use prioritized experience replay,

100
00:06:15,996 --> 00:06:19,320
we have to make one adjustment to our update rule.

101
00:06:19,320 --> 00:06:22,200
Remember that our original Q-Learning update is

102
00:06:22,200 --> 00:06:25,680
derived from an expectation over all experiences.

103
00:06:25,680 --> 00:06:28,094
When using a stochastic update rule,

104
00:06:28,094 --> 00:06:30,240
the way we sample these experiences

105
00:06:30,240 --> 00:06:33,920
must match the underlying distribution they came from.

106
00:06:33,920 --> 00:06:39,195
This is preserved when we sample experienced tuples uniformly from the replay buffer.

107
00:06:39,195 --> 00:06:42,240
But this assumption is violated when we use

108
00:06:42,240 --> 00:06:46,170
a non-uniform sampling for example using priorities.

109
00:06:46,170 --> 00:06:48,960
The Q-values we learn will be biased according to

110
00:06:48,960 --> 00:06:53,170
these priority values which we only wanted to use for sampling.

111
00:06:53,170 --> 00:06:55,350
To correct for this bias,

112
00:06:55,350 --> 00:07:00,330
we need to introduce an important sampling weight equal to one over n,

113
00:07:00,330 --> 00:07:03,330
where n is the size of this replay buffer,

114
00:07:03,330 --> 00:07:06,790
times one over the sampling probability, Pi.

115
00:07:06,790 --> 00:07:09,660
We can add another hyper parameter, b,

116
00:07:09,660 --> 00:07:12,500
and raise each important sampling weight to b,

117
00:07:12,500 --> 00:07:16,150
to control how much these weights affect learning.

118
00:07:16,150 --> 00:07:20,670
In fact, these weights are more important towards the end of learning when

119
00:07:20,670 --> 00:07:27,330
your Q-values begin to converge so you can increase b from a low value to one over time.

120
00:07:27,330 --> 00:07:30,900
Again, these details may be hard to understand at first,

121
00:07:30,900 --> 00:07:33,420
but each small improvement can contribute

122
00:07:33,420 --> 00:07:37,005
a lot towards more stable and efficient learning.

123
00:07:37,005 --> 00:07:43,310
So make sure you give the prioritized experience replay paper a good read.

124
00:07:43,310 --> 00:07:46,350
The final enhancement of DQNs that we'll briefly

125
00:07:46,350 --> 00:07:50,340
look at is appropriately titled, Dueling Networks.

126
00:07:50,340 --> 00:07:52,650
Here is a typical DQN architecture.

127
00:07:52,650 --> 00:07:55,290
A sequence of convolutional layers followed by

128
00:07:55,290 --> 00:07:59,355
a couple of fully connected layers that produce Q-values.

129
00:07:59,355 --> 00:08:03,555
The core idea of Dueling Networks is to use two streams.

130
00:08:03,555 --> 00:08:06,660
One that estimates the state value function,

131
00:08:06,660 --> 00:08:10,855
and one that estimates the advantage for each action.

132
00:08:10,855 --> 00:08:15,372
These streams may share some layers in the beginning such as convolutional layers,

133
00:08:15,372 --> 00:08:18,450
then branch off with their own fully connected layers,

134
00:08:18,450 --> 00:08:21,060
and finally the desired Q-values are

135
00:08:21,060 --> 00:08:24,785
obtained by combining the state and advantage values.

136
00:08:24,785 --> 00:08:27,600
The intuition behind this is that the value of

137
00:08:27,600 --> 00:08:31,740
most states don't vary a lot across actions.

138
00:08:31,740 --> 00:08:35,220
So it makes sense to try and directly estimate them.

139
00:08:35,220 --> 00:08:39,740
But we still need to capture the difference actions make in each state.

140
00:08:39,740 --> 00:08:42,630
This is where the advantage function comes in.

141
00:08:42,630 --> 00:08:47,375
Some modifications are necessary to adapt Q-Learning to this architecture,

142
00:08:47,375 --> 00:08:50,085
which you can find in the Dueling networks paper.

143
00:08:50,085 --> 00:08:53,305
Along with double DQNs and prioritized replay,

144
00:08:53,305 --> 00:08:59,000
this technique has resulted in significant improvement over vanilla DQNs.

