1
00:00:00,000 --> 00:00:06,145
该论文提供了多个针对原始深度 Q 学习算法的改进建议

2
00:00:06,145 --> 00:00:09,761
我们来看看其中三个最重要的改进

3
00:00:09,761 --> 00:00:15,225
双 DQN 优先回放和对抗网络

4
00:00:15,225 --> 00:00:17,590
我们要解决的第一个问题是

5
00:00:17,590 --> 00:00:20,920
Q 学习容易出现的过高估计动作值问题

6
00:00:20,920 --> 00:00:23,740
我们来回顾下具有函数逼近的

7
00:00:23,740 --> 00:00:27,370
Q 学习更新规则 并重点看看 TD 目标

8
00:00:27,370 --> 00:00:31,000
这里的 max 运算有必要用到

9
00:00:31,000 --> 00:00:35,080
以便计算可以从下个状态获得的最佳潜在值

10
00:00:35,080 --> 00:00:36,880
为了很好地理解这一点

11
00:00:36,880 --> 00:00:41,230
我们重写目标并展开 max 运算

12
00:00:41,230 --> 00:00:46,270
可以更有效地理解为

13
00:00:46,270 --> 00:00:50,050
我们想要获得状态 S′ 的 Q 值

14
00:00:50,050 --> 00:00:54,645
以及从该状态的所有潜在动作中实现最大 Q 值的动作

15
00:00:54,645 --> 00:00:56,260
写成这样时

16
00:00:56,260 --> 00:01:00,595
可以看出 arg max 运算有可能会出错

17
00:01:00,595 --> 00:01:02,730
尤其是在早期阶段

18
00:01:02,730 --> 00:01:06,550
为何？因为 Q 值依然在变化

19
00:01:06,550 --> 00:01:10,890
我们可能没有收集足够的信息来判断最佳动作是什么

20
00:01:10,890 --> 00:01:14,230
Q 值的准确性在很大程度上取决于

21
00:01:14,230 --> 00:01:18,725
尝试了哪些动作以及探索了哪些周围的状态

22
00:01:18,725 --> 00:01:23,542
实际上 事实证明这一部分会导致过高估计 Q 值

23
00:01:23,542 --> 00:01:28,760
因为我们始终在一组杂乱数字中选择最大值

24
00:01:28,760 --> 00:01:32,685
或许我们不应该盲目地信任这些值

25
00:01:32,685 --> 00:01:36,620
如何使我们的估算更可靠？

26
00:01:36,620 --> 00:01:41,920
有一个经实践证明效果很好的方法是双 Q 学习

27
00:01:41,920 --> 00:01:45,830
我们使用一组参数 w 选择最佳动作

28
00:01:45,830 --> 00:01:50,990
但是使用另一组参数 w′ 评估动作

29
00:01:50,990 --> 00:01:52,630
相当于有两个单独的函数逼近器

30
00:01:52,630 --> 00:01:57,100
它们必须在最佳动作上达成一致

31
00:01:57,100 --> 00:02:01,678
如果根据 W′ W 选择的动作并不是最佳动作

32
00:02:01,678 --> 00:02:04,285
那么返回的 Q 值不会那么高

33
00:02:04,285 --> 00:02:08,125
长期下来 可以避免该算法

34
00:02:08,125 --> 00:02:11,140
传播偶尔获得的更高奖励

35
00:02:11,140 --> 00:02:15,065
这些奖励并不能反映长期回报

36
00:02:15,065 --> 00:02:16,510
你可能会问

37
00:02:16,510 --> 00:02:19,730
我们从哪获得第二组参数？

38
00:02:19,730 --> 00:02:23,680
在原始的双 Q 学习公式中

39
00:02:23,680 --> 00:02:27,910
你会保留两个值函数 在每一步随机地选择一个函数进行更新

40
00:02:27,910 --> 00:02:32,620
并仅使用另一个评估动作

41
00:02:32,620 --> 00:02:36,250
但是将 DQN 与固定 Q 目标结合使用时

42
00:02:36,250 --> 00:02:39,100
我们已经有一组替代的参数

43
00:02:39,100 --> 00:02:41,110
还记得 W- 吗？

44
00:02:41,110 --> 00:02:46,090
实际上 W- 已经保留不用有一段时间了

45
00:02:46,090 --> 00:02:51,420
它与 W 之间的区别已经足够大 可以重新用于该目的 原理就是这样

46
00:02:51,420 --> 00:02:54,850
这个简单的修改可以使 Q 值变得可靠起来

47
00:02:54,850 --> 00:03:00,235
防止它们在学习的早期阶段爆发或在后期阶段出现波动

48
00:03:00,235 --> 00:03:04,270
经证明 形成的策略效果

49
00:03:04,270 --> 00:03:08,970
比 Vanilla DQN 强很多

50
00:03:08,970 --> 00:03:12,935
我们要讨论的下个问题与经验回放有关

51
00:03:12,935 --> 00:03:15,770
回忆下经验回放的基本原理

52
00:03:15,770 --> 00:03:18,635
我们与环境互动以便收集经验元组

53
00:03:18,635 --> 00:03:24,030
将元组保存在缓冲区 然后随机抽取一个批次以从中学习规律

54
00:03:24,030 --> 00:03:26,610
这样有助于打破连续经验之间的关系

55
00:03:26,610 --> 00:03:31,215
并使学习算法稳定下来

56
00:03:31,215 --> 00:03:32,460
到目前为止 没什么问题

57
00:03:32,460 --> 00:03:36,975
但是某些经验可能比其他经验更重要 更需要学习

58
00:03:36,975 --> 00:03:41,520
此外 这些重要的经验可能很少发生

59
00:03:41,520 --> 00:03:44,700
如果我们均匀地按批次抽样

60
00:03:44,700 --> 00:03:48,520
那么这些经验被选中的概率很小

61
00:03:48,520 --> 00:03:51,945
因为缓冲区实际上容量有限

62
00:03:51,945 --> 00:03:55,465
因此更早的重要经验可能会丢失

63
00:03:55,465 --> 00:03:59,220
这时候就要用到优先经验回放了

64
00:03:59,220 --> 00:04:04,260
但是我们根据什么条件来为每个元组分配优先级呢？

65
00:04:04,260 --> 00:04:07,465
一种方法是使用 TD 误差增量

66
00:04:07,465 --> 00:04:08,715
误差越大

67
00:04:08,715 --> 00:04:11,905
我们从该元组中预计学到的规律就越多

68
00:04:11,905 --> 00:04:15,735
我们将此误差的大小作为衡量优先级的条件

69
00:04:15,735 --> 00:04:20,565
并将其与每个相应的元组一起存储在回放缓冲区

70
00:04:20,565 --> 00:04:26,430
在创建批次时 我们可以使用该值计算抽样概率

71
00:04:26,430 --> 00:04:32,433
选择任何元组 i 的概率等于其优先级值 pi

72
00:04:32,433 --> 00:04:36,390
并用回放缓冲区中的所有优先级值之和标准化

73
00:04:36,390 --> 00:04:38,250
选择某个元组后

74
00:04:38,250 --> 00:04:44,835
我们可以使用最新的 Q 值将其优先级更新为新计算的 TD 误差

75
00:04:44,835 --> 00:04:48,060
似乎效果不错

76
00:04:48,060 --> 00:04:52,680
经证明 可以减少学习值函数所需的批次更新数量

77
00:04:52,680 --> 00:04:55,170
有几个方面可以加以改进

78
00:04:55,170 --> 00:04:58,405
首先 如果 TD 误差为 0

79
00:04:58,405 --> 00:05:00,450
那么该元组的优先级值

80
00:05:00,450 --> 00:05:04,125
及其被选中的概率也为 0

81
00:05:04,125 --> 00:05:07,350
0 或非常低的 TD 误差

82
00:05:07,350 --> 00:05:11,115
并不一定就表明我们从此类元组中学不到任何规律

83
00:05:11,115 --> 00:05:13,380
可能是因为到该时间点为止

84
00:05:13,380 --> 00:05:17,555
经历的样本有限 估值很接近真实的值

85
00:05:17,555 --> 00:05:21,555
为了防止此类元组不被选中

86
00:05:21,555 --> 00:05:26,190
我们可以在每个平衡值后面加上一个很小的常量 e

87
00:05:26,190 --> 00:05:31,680
另一个类似的问题是贪婪地使用这些优先级值

88
00:05:31,680 --> 00:05:34,830
可能会导致一小部分经验子集不断被回放

89
00:05:34,830 --> 00:05:39,130
导致对该子集的过拟合

90
00:05:39,130 --> 00:05:44,680
为了避免该问题 我们可以重新引入一些均匀随机抽样元素

91
00:05:44,680 --> 00:05:47,345
这样会添加另一个超参数 a

92
00:05:47,345 --> 00:05:52,120
我们用它将抽样概率重新定义为

93
00:05:52,120 --> 00:05:56,025
优先级 pi 的 a 次幂除以所有优先级之和 pk

94
00:05:56,025 --> 00:05:59,120
每个都是 a 次幂

95
00:05:59,120 --> 00:06:01,920
我们可以通过调整该参数

96
00:06:01,920 --> 00:06:05,383
控制采用优先级与随机性的幅度

97
00:06:05,383 --> 00:06:06,890
a 等于 0 表示采用完全均匀随机性方法

98
00:06:06,890 --> 00:06:13,452
a 等于 1 表示仅采用优先级值

99
00:06:13,452 --> 00:06:15,996
当我们使用优先经验回放时

100
00:06:15,996 --> 00:06:19,320
我们需要对更新规则进行一项调整

101
00:06:19,320 --> 00:06:22,200
我们的原始 Q 学习更新

102
00:06:22,200 --> 00:06:25,680
来自于对所有经验的预期结果

103
00:06:25,680 --> 00:06:28,094
在使用随机性更新规则时

104
00:06:28,094 --> 00:06:30,240
我们对这些经验抽样的方式

105
00:06:30,240 --> 00:06:33,920
必须与来源底层分布匹配

106
00:06:33,920 --> 00:06:39,195
当我们从回放缓冲区均匀地抽样经验元组时 能够满足这一前提条件

107
00:06:39,195 --> 00:06:42,240
但是当我们采用非均匀抽样（例如使用优先级）时

108
00:06:42,240 --> 00:06:46,170
则不满足该前提条件

109
00:06:46,170 --> 00:06:48,960
我们学习的 Q 值将因为这些优先级值出现偏差

110
00:06:48,960 --> 00:06:53,170
我们只希望使用这些值进行抽样

111
00:06:53,170 --> 00:06:55,350
为了更正这一偏差

112
00:06:55,350 --> 00:07:00,330
我们需要引入一个重要的抽样权重 等于 1/N

113
00:07:00,330 --> 00:07:03,330
N 是该回放缓冲区的大小

114
00:07:03,330 --> 00:07:06,790
乘以 1/p(i) p(i) 是抽样概率

115
00:07:06,790 --> 00:07:09,660
我们可以添加另一个超参数 b

116
00:07:09,660 --> 00:07:12,500
并将每个重要抽样权重提升为 b

117
00:07:12,500 --> 00:07:16,150
控制这些权重对学习的影响程度

118
00:07:16,150 --> 00:07:20,670
实际上 这些权重在学习结束时更加重要

119
00:07:20,670 --> 00:07:27,330
这时候 Q 值开始收敛 因此你可以逐渐将 b 从很低的值增加到 1

120
00:07:27,330 --> 00:07:30,900
这些细节部分可能一开始很难明白

121
00:07:30,900 --> 00:07:33,420
但是每个小小的改进

122
00:07:33,420 --> 00:07:37,005
都可以有效地达到更加稳定高效的学习算法

123
00:07:37,005 --> 00:07:43,310
因此确保仔细阅读优先经验回放论文

124
00:07:43,310 --> 00:07:46,350
我们将简单介绍最后一个 DQN 增强技巧

125
00:07:46,350 --> 00:07:50,340
叫做对抗网络 名称很好懂

126
00:07:50,340 --> 00:07:52,650
这是一个典型 DQN 架构

127
00:07:52,650 --> 00:07:55,290
有一系列卷积层

128
00:07:55,290 --> 00:07:59,355
然后是几个完全连接层 生成了 Q 值

129
00:07:59,355 --> 00:08:03,555
对抗网络的核心概念是使用两个信息流

130
00:08:03,555 --> 00:08:06,660
一个估算状态值函数

131
00:08:06,660 --> 00:08:10,855
一个估算每个动作的优势

132
00:08:10,855 --> 00:08:15,372
这两个信息流可能在一开始具有一些共同的层级 例如卷积层

133
00:08:15,372 --> 00:08:18,450
然后分支为各自的完全连接层

134
00:08:18,450 --> 00:08:21,060
最后通过结合状态和优势值

135
00:08:21,060 --> 00:08:24,785
获得期望的 Q 值

136
00:08:24,785 --> 00:08:27,600
这么做的原因是

137
00:08:27,600 --> 00:08:31,740
大部分状态在动作之间变化不大

138
00:08:31,740 --> 00:08:35,220
因此可以尝试直接估算它们

139
00:08:35,220 --> 00:08:39,740
但是我们依然需要捕获动作在每个状态中产生的区别

140
00:08:39,740 --> 00:08:42,630
这时候就要用到优势函数

141
00:08:42,630 --> 00:08:47,375
需要进行一些调整 以便将 Q 学习应用于该架构

142
00:08:47,375 --> 00:08:50,085
你可以在对抗网络论文中找到该调整方法

143
00:08:50,085 --> 00:08:53,305
和双 DQN 以及经验回放一样

144
00:08:53,305 --> 00:08:59,000
该技巧显著改善了 vanilla DQN

