1
00:00:03,229 --> 00:00:07,500
在 2015 年 DeepMind 取得了突破性的成果

2
00:00:07,500 --> 00:00:11,684
他们设计了一个可以学会玩视频游戏的智能体 并且比人类玩家的能力更强

3
00:00:11,685 --> 00:00:15,179
如果你知道电子游戏 “Pong” 的底层游戏状态

4
00:00:15,179 --> 00:00:18,660
或许可以轻松地编写一个玩这个游戏的程序

5
00:00:18,660 --> 00:00:21,135
例如球的位置 挡板等

6
00:00:21,135 --> 00:00:24,554
但是这个智能体只能获得原始的像素数据

7
00:00:24,554 --> 00:00:26,864
即人类玩家在屏幕上看到的结果

8
00:00:26,864 --> 00:00:31,859
它学会了从头开始玩各种不同的 Atari 游戏

9
00:00:31,859 --> 00:00:35,060
他们将这个智能体称之为深度 Q 网络

10
00:00:35,060 --> 00:00:38,125
我们深入了解下它的工作原理

11
00:00:38,125 --> 00:00:41,500
正如它的名字所表示的含义 该智能体的核心是

12
00:00:41,500 --> 00:00:45,609
一个充当函数逼近器的深度神经网络

13
00:00:45,609 --> 00:00:50,229
你一次传入一个你喜欢的视频游戏屏幕的图片

14
00:00:50,229 --> 00:00:52,914
它会生成一个动作值向量

15
00:00:52,914 --> 00:00:56,335
最大值表示采取的动作

16
00:00:56,335 --> 00:00:58,314
根据强化信号

17
00:00:58,314 --> 00:01:02,199
它会在每个时间步将游戏得分的变化往回馈送

18
00:01:02,200 --> 00:01:06,685
一开始 当神经网络初始化为随机的值时

19
00:01:06,685 --> 00:01:09,340
采取的动作很混乱

20
00:01:09,340 --> 00:01:11,665
正如你所预期的 效果很差

21
00:01:11,665 --> 00:01:15,820
但是随着时间的推移 它开始将游戏中的情景和顺序

22
00:01:15,819 --> 00:01:20,709
与相应的动作关联起来 并且学会很好地玩游戏

23
00:01:20,709 --> 00:01:23,619
输入空间非常复杂

24
00:01:23,620 --> 00:01:29,260
Atari 游戏的分辨率为 210 x 160 像素

25
00:01:29,260 --> 00:01:33,685
每个像素有 128 种可能的颜色

26
00:01:33,685 --> 00:01:40,105
这依然是一个离散状态空间 但是处理起来非常庞大

27
00:01:40,105 --> 00:01:42,145
为了降低复杂性

28
00:01:42,144 --> 00:01:45,774
DeepMind 团队决定稍加处理

29
00:01:45,775 --> 00:01:47,780
将帧转换为灰阶

30
00:01:47,780 --> 00:01:52,215
并缩小为 84 x 84 像素的正方形

31
00:01:52,215 --> 00:01:57,960
正方形图片使他们能够在 GPU 上使用更加优化的神经网络运算

32
00:01:57,959 --> 00:02:01,634
为了使智能体能够访问一系列的帧

33
00:02:01,635 --> 00:02:04,800
他们将四个此类帧堆叠起来

34
00:02:04,799 --> 00:02:09,155
形成 84 x 84 x 4 的最终状态空间

35
00:02:09,155 --> 00:02:11,560
也许还有其他处理序列数据的方法

36
00:02:11,560 --> 00:02:16,180
但是这个似乎是一个效果很好的简单方法

37
00:02:16,180 --> 00:02:17,635
在输出端

38
00:02:17,634 --> 00:02:20,454
与传统强化学习设置

39
00:02:20,455 --> 00:02:23,410
（一次仅生成一个 Q 值）不同

40
00:02:23,409 --> 00:02:25,930
深度 Q 网络会为一个前向传递中的

41
00:02:25,930 --> 00:02:30,444
所有可能的动作生成一个 Q 值

42
00:02:30,444 --> 00:02:34,974
不这么处理的话 你需要单独为每个动作运行该网络

43
00:02:34,974 --> 00:02:38,784
现在 你可以直接根据该向量采取动作

44
00:02:38,784 --> 00:02:45,034
要么是随机形式 要么选择值最大的动作 很简单 对吧

45
00:02:45,034 --> 00:02:49,620
这些创新型输入和输出转换

46
00:02:49,620 --> 00:02:53,534
支持在后台使用强大却简单的神经网络结构

47
00:02:53,534 --> 00:02:57,329
屏幕图片首先用卷积层处理

48
00:02:57,330 --> 00:03:01,094
使系统能够发现空间关系

49
00:03:01,094 --> 00:03:03,930
并探索空间规则空间

50
00:03:03,930 --> 00:03:08,175
此外 因为将四个帧堆叠起来作为输入

51
00:03:08,175 --> 00:03:13,800
这些卷积层还将从这些帧中提取时间属性

52
00:03:13,800 --> 00:03:16,485
原始 DQN 智能体

53
00:03:16,485 --> 00:03:21,700
使用了三个此类卷积层 并采用 ReLU 激活函数 即正则化线性单元

54
00:03:21,699 --> 00:03:26,224
然后是一层采用 ReLU 激活函数的完全连接隐藏层

55
00:03:26,224 --> 00:03:31,835
以及一层完全连接的线性输出层 用于生成动作值向量

56
00:03:31,835 --> 00:03:36,320
他们在所有接受测试的 Atari 游戏上应用了同一架构

57
00:03:36,319 --> 00:03:41,609
但是每个游戏都从头开始学习并使用重新初始化的网络

58
00:03:41,610 --> 00:03:45,115
训练此类网络需要大量数据

59
00:03:45,115 --> 00:03:49,435
但即使这样 也不能保证会收敛于最优值函数

60
00:03:49,435 --> 00:03:53,949
实际上 在某些情况下 网络权重会因为动作和状态

61
00:03:53,949 --> 00:03:57,369
之间的关系非常紧密而振荡或发散

62
00:03:57,370 --> 00:04:01,405
这样会导致非常不稳定并且效率很低的策略

63
00:04:01,405 --> 00:04:03,520
为了克服这些挑战

64
00:04:03,520 --> 00:04:06,040
研究人员想出了多个技巧

65
00:04:06,039 --> 00:04:08,949
稍微修改了基础 Q 学习算法

66
00:04:08,949 --> 00:04:11,919
我们将了解其中的两个技巧

67
00:04:11,919 --> 00:04:14,799
我认为这两个技巧是他们的成果中最重要的贡献

68
00:04:14,800 --> 00:04:18,329
即经验回放和固定 Q 目标

