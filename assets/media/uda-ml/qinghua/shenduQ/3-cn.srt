1
00:00:00,000 --> 00:00:03,750
我们的第一个策略是使用蒙特卡洛学习

2
00:00:03,750 --> 00:00:06,660
还记得在经典蒙特卡洛学习中

3
00:00:06,660 --> 00:00:10,230
用来更新值函数的递增步骤吗？

4
00:00:10,230 --> 00:00:14,670
这里 Gt 是回报 即在时间 t 之后收到的

5
00:00:14,669 --> 00:00:20,699
累积折扣奖励 这是一个适合获得的目标 对吧？

6
00:00:20,699 --> 00:00:24,179
我们采用神经网络更新规则

7
00:00:24,179 --> 00:00:28,679
并将未知真值函数替换为这个回报

8
00:00:28,679 --> 00:00:32,000
这就获得了由神经网络或其他函数逼近器

9
00:00:32,000 --> 00:00:37,829
表示的具体状态值函数更新规则

10
00:00:37,829 --> 00:00:39,929
正如你可能猜到的

11
00:00:39,929 --> 00:00:44,119
我们也可以对动作值函数执行相同的步骤

12
00:00:44,119 --> 00:00:47,844
我们已经具有了主要构建组件 即更新规则

13
00:00:47,844 --> 00:00:52,560
我们根据它构建一个完整的蒙特卡洛算法

14
00:00:52,560 --> 00:00:55,200
我们重点看看控制问题

15
00:00:55,200 --> 00:00:59,420
并调整我们的经典蒙特卡洛算法 以便使用函数逼近器

16
00:00:59,420 --> 00:01:04,320
它通常包含一个评估步骤

17
00:01:04,319 --> 00:01:09,434
我们在此步估算当前策略下的每个状态动作对的值

18
00:01:09,435 --> 00:01:12,150
为此 我们与环境互动

19
00:01:12,150 --> 00:01:14,880
并使用策略 π 生成一个阶段

20
00:01:14,879 --> 00:01:18,089
然后在该阶段中的每个时间步 t

21
00:01:18,090 --> 00:01:22,760
使用状态动作对 St At 和根据该阶段剩余时间步

22
00:01:22,760 --> 00:01:28,564
计算的回报 Gt 更新参数向量 w

23
00:01:28,564 --> 00:01:31,950
接着是一个完善步骤

24
00:01:31,950 --> 00:01:36,200
我们在该步骤根据这些 q 值提取 epsilon 贪婪策略

25
00:01:36,200 --> 00:01:40,265
一开始 我们需要初始化参数 w

26
00:01:40,265 --> 00:01:42,030
假设我们随机地初始化

27
00:01:42,030 --> 00:01:47,400
并以采用同一 epsilon 贪婪方式定义的策略 π 开始

28
00:01:47,400 --> 00:01:51,180
然后不断重复这两个步骤 直到权重收敛

29
00:01:51,180 --> 00:01:57,200
形成最优值函数和相应的策略

30
00:01:57,200 --> 00:02:01,079
注意 这是蒙特卡洛所有经历版本

31
00:02:01,079 --> 00:02:02,945
对于首次经历版本

32
00:02:02,945 --> 00:02:05,340
仅当您在某个阶段中首次见到

33
00:02:05,340 --> 00:02:09,000
该状态动作对时才更新权重

