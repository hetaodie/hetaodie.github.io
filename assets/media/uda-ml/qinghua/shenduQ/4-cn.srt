1
00:00:00,000 --> 00:00:02,745
第二个策略是使用函数逼近

2
00:00:02,745 --> 00:00:06,629
构建一个时间差分技巧

3
00:00:06,629 --> 00:00:10,964
比较蒙特卡洛学习的递增更新步骤

4
00:00:10,964 --> 00:00:14,669
（使用在阶段中获得的实际回报）

5
00:00:14,669 --> 00:00:19,500
与时间差分的更新步骤（使用估算回报）

6
00:00:19,500 --> 00:00:22,035
这叫做 TD 目标

7
00:00:22,035 --> 00:00:24,861
在最简单的形式 TD(0) 下

8
00:00:24,861 --> 00:00:28,769
我们使用下个奖励和下个状态的折扣值

9
00:00:28,769 --> 00:00:30,750
和之前的方法类似

10
00:00:30,750 --> 00:00:35,234
我们可以使用这个 TD 目标替换未知真值函数

11
00:00:35,234 --> 00:00:37,710
这样就可以使用具体的数据

12
00:00:37,710 --> 00:00:40,905
不再是想象的数据

13
00:00:40,905 --> 00:00:46,125
注意 我们需要调整值函数 才能使用函数逼近器 v^

14
00:00:46,125 --> 00:00:50,369
这整个差值称之为 TD 误差 表示为 δt

15
00:00:50,369 --> 00:00:55,379
我们也可以将同一概念应用到动作值函数

16
00:00:55,380 --> 00:01:00,304
我们已经准备好围绕这个更新规则构建一个算法

17
00:01:00,304 --> 00:01:05,650
我们将使用 TD(0) 目标并再次侧重于控制问题

18
00:01:05,650 --> 00:01:07,045
你可能还记得

19
00:01:07,045 --> 00:01:09,760
这本质上是 SARSA 算法

20
00:01:09,760 --> 00:01:13,450
我们首先随机地初始化参数 W

21
00:01:13,450 --> 00:01:16,750
策略 π 隐式地定义为

22
00:01:16,750 --> 00:01:21,144
针对估算动作值函数 Q^ 的 Epsilon 贪婪选择

23
00:01:21,144 --> 00:01:24,069
然后我们开始与环境互动

24
00:01:24,069 --> 00:01:29,789
对于每个阶段 我们以从环境获得的初始状态 S 开始

25
00:01:29,790 --> 00:01:33,470
我们继续互动 直到抵达最终状态

26
00:01:33,469 --> 00:01:35,164
在每个时间步

27
00:01:35,165 --> 00:01:41,420
我们选择执行一个动作 A 并获得奖励 R 和下个状态 S′

28
00:01:41,420 --> 00:01:44,540
现在根据 Epsilon 贪婪策略 π

29
00:01:44,540 --> 00:01:48,860
从状态 S′ 选择另一个动作 A′

30
00:01:48,859 --> 00:01:53,209
这样就获得了 SARSA 更新所需的一切

31
00:01:53,209 --> 00:01:56,269
即 S A R S′ A′

32
00:01:56,269 --> 00:02:01,909
将这些项代入梯度下降更新规则并相应地调整权重

33
00:02:01,909 --> 00:02:06,009
最后 直接将 S′ 替换为新的 S

34
00:02:06,010 --> 00:02:08,879
将 A′ 替换为新的 A 并重复刚刚的流程

35
00:02:08,879 --> 00:02:11,144
这个公式适用于阶段性任务

36
00:02:11,145 --> 00:02:16,409
在阶段性任务中 每个阶段都肯定会终止

37
00:02:16,409 --> 00:02:22,109
这个公式可以经过调整并应用于连续性任务

38
00:02:22,110 --> 00:02:28,155
方法是消除阶段之间的界限并将一系列的动作当做一个很长的无止境阶段

39
00:02:28,155 --> 00:02:31,080
SARSA 是一种异同策略算法

40
00:02:31,080 --> 00:02:32,820
表示我们的更新策略

41
00:02:32,819 --> 00:02:36,389
和执行动作时遵守的策略一样

42
00:02:36,389 --> 00:02:40,484
通常效果很好并且能快速收敛

43
00:02:40,485 --> 00:02:45,375
因为你会根据最新的策略执行动作

44
00:02:45,375 --> 00:02:47,899
但是也有一些缺点

45
00:02:47,899 --> 00:02:50,490
主要是学习的策略

46
00:02:50,490 --> 00:02:53,574
和遵守的策略关系太紧密

47
00:02:53,574 --> 00:02:56,550
如果我们想遵守一个策略 例如更加探索性的策略

48
00:02:56,550 --> 00:03:01,590
并学习更加优化的策略呢

49
00:03:01,590 --> 00:03:05,530
这时候就需要用到离线策略算法

