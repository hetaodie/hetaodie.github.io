1
00:00:00,000 --> 00:00:04,469
We've seen that the reinforcement learning framework gives us a way to study

2
00:00:04,469 --> 00:00:09,504
how an agent can learn to accomplish a goal from interacting with its environment.

3
00:00:09,505 --> 00:00:13,500
This framework works for many real world applications and simplifies

4
00:00:13,500 --> 00:00:18,780
the interaction into three signals that are passed between agent and environment.

5
00:00:18,780 --> 00:00:23,960
The state signal is the environment's way of presenting a situation to the agent.

6
00:00:23,960 --> 00:00:28,530
The agent then responds with an action which influences the environment.

7
00:00:28,530 --> 00:00:31,679
And the environment responds with the reward which gives

8
00:00:31,679 --> 00:00:37,454
some indication of whether the agent has responded appropriately to the environment.

9
00:00:37,454 --> 00:00:43,239
Also built into the framework is the agent's goal which is to maximize cumulative reward.

10
00:00:43,240 --> 00:00:47,645
But what exactly does this mean and how does the agent accomplish this?

11
00:00:47,645 --> 00:00:49,960
Towards its goal, what do you think?

12
00:00:49,960 --> 00:00:54,255
Could the agent just maximize the reward and each time step?

13
00:00:54,255 --> 00:00:56,969
The short answer to that question is, no.

14
00:00:56,969 --> 00:01:00,789
But I think a long answer would be a lot more satisfying.

15
00:01:00,789 --> 00:01:04,609
So let's try to understand this with the walking robot example.

16
00:01:04,609 --> 00:01:06,239
Remember that in this case,

17
00:01:06,239 --> 00:01:09,420
the goal of the robot was to stay walking forward for as

18
00:01:09,420 --> 00:01:14,820
long and as quickly as possible while also exerting minimal effort.

19
00:01:14,819 --> 00:01:16,439
In this case, if the robot tried to

20
00:01:16,439 --> 00:01:19,834
maximize the reward it received at a single time step,

21
00:01:19,834 --> 00:01:23,099
that would look like trying to move as quickly as possible with

22
00:01:23,099 --> 00:01:27,099
as little effort without falling immediately.

23
00:01:27,099 --> 00:01:31,199
That could work well in the short term but it's possible, for instance,

24
00:01:31,200 --> 00:01:35,844
that the agents movement gets it moving quickly without falling initially.

25
00:01:35,844 --> 00:01:38,370
But that first movement was de-stabilising

26
00:01:38,370 --> 00:01:41,969
enough that it doomed the agent to fall in a short time.

27
00:01:41,969 --> 00:01:45,674
In this way, if the agent focused on individual time steps,

28
00:01:45,674 --> 00:01:49,344
it could learn actions that maximize initial rewards.

29
00:01:49,344 --> 00:01:52,049
But then the episode terminates quite quickly.

30
00:01:52,049 --> 00:01:54,944
And so the cumulative reward is quite small.

31
00:01:54,944 --> 00:01:57,269
And still worse, in this case,

32
00:01:57,269 --> 00:02:00,354
the agent will not have learned to walk.

33
00:02:00,355 --> 00:02:01,545
In this example then,

34
00:02:01,545 --> 00:02:06,254
it's clear that the agent cannot focus on individual time steps and instead,

35
00:02:06,254 --> 00:02:08,775
needs to keep all time steps in mind.

36
00:02:08,775 --> 00:02:13,289
But this also holds true for reinforcement learning agents in general.

37
00:02:13,289 --> 00:02:18,030
Actions have short and long term consequences and the agent needs

38
00:02:18,030 --> 00:02:23,289
to gain some understanding of the complex effects its actions have on the environment.

39
00:02:23,289 --> 00:02:26,155
Along these lines in the walking robot example,

40
00:02:26,155 --> 00:02:30,280
the agent always has reward at all time steps in mind,

41
00:02:30,280 --> 00:02:34,430
it will learn to choose movement designed for long term stability.

42
00:02:34,430 --> 00:02:40,080
So in this way, the robot moves a bit slowly to sacrifice a little bit of reward

43
00:02:40,080 --> 00:02:42,990
but it will payoff because it will avoid falling for

44
00:02:42,990 --> 00:02:47,000
longer and collect higher cumulative reward.

45
00:02:47,000 --> 00:02:49,185
But now, what does all of this mean when the agent

46
00:02:49,185 --> 00:02:51,914
chooses an action at an arbitrary time step?

47
00:02:51,914 --> 00:02:55,734
How exactly does it keep all time steps in mind?

48
00:02:55,735 --> 00:02:59,315
Well, if we're looking at some time step, t,

49
00:02:59,314 --> 00:03:02,099
it's important to note that the rewards for

50
00:03:02,099 --> 00:03:06,884
all previous time steps have already been decided as they're in the past.

51
00:03:06,884 --> 00:03:11,144
Only future rewards are inside the agent's control.

52
00:03:11,145 --> 00:03:15,090
We refer to the sum of rewards from the next time step

53
00:03:15,090 --> 00:03:19,604
onward as the return and denote it with a capital G,

54
00:03:19,604 --> 00:03:21,794
and at an arbitrary time step,

55
00:03:21,794 --> 00:03:27,530
the agent will always choose an action towards the goal of maximizing the return.

56
00:03:27,530 --> 00:03:34,349
But it's actually more accurate to say that the agent seeks to maximize expected return.

57
00:03:34,349 --> 00:03:37,530
This is because it's generally the case that the agent can't

58
00:03:37,530 --> 00:03:42,219
predict with complete certainty what the future reward is likely to be.

59
00:03:42,219 --> 00:03:46,085
So it has to rely on a prediction or an estimate.

60
00:03:46,085 --> 00:03:49,375
We'll massage this a bit when we talk about discounted return.

61
00:03:49,375 --> 00:03:51,000
But this is the main idea.

