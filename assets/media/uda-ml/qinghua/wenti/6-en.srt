1
00:00:00,000 --> 00:00:03,145
So far, we've been trying to frame the idea of a humanoid

2
00:00:03,145 --> 00:00:06,640
learning to walk in the context of reinforcement learning.

3
00:00:06,639 --> 00:00:08,429
We've detailed the states in actions,

4
00:00:08,429 --> 00:00:11,099
and we still need to specify the rewards.

5
00:00:11,099 --> 00:00:15,699
And the reward structure from the DeepMind paper is surprisingly intuitive.

6
00:00:15,699 --> 00:00:19,230
This line is pulled from the appendix of the DeepMind paper,

7
00:00:19,230 --> 00:00:22,835
and describes how the reward is decided at every time step?

8
00:00:22,835 --> 00:00:27,785
Each term communicates to the agent some part of what we'd like it to accomplish.

9
00:00:27,785 --> 00:00:30,365
So let's look at each term individually.

10
00:00:30,364 --> 00:00:32,969
To begin, at every time step,

11
00:00:32,969 --> 00:00:36,759
the agent receives a reward proportional to its forward velocity.

12
00:00:36,759 --> 00:00:38,344
So if moves faster,

13
00:00:38,344 --> 00:00:40,699
it gets more reward, but up to a limit.

14
00:00:40,700 --> 00:00:43,880
Here denoted Vmax, but it's penalized by

15
00:00:43,880 --> 00:00:47,740
an amount proportional to the force applied to each joint.

16
00:00:47,740 --> 00:00:50,429
So if the agent applies more force to the joints,

17
00:00:50,429 --> 00:00:53,225
then more reward is taken away as punishment.

18
00:00:53,225 --> 00:00:57,632
Since the researchers also wanted the humanoid to focus on moving forward,

19
00:00:57,631 --> 00:01:02,640
the agent is also penalized for moving left, right, or vertically.

20
00:01:02,640 --> 00:01:07,875
It was also penalized if the humanoid moved its body away from the center of the track.

21
00:01:07,875 --> 00:01:12,549
So the agent will try to keep the humanoid as close to the center as possible.

22
00:01:12,549 --> 00:01:14,039
At every time step,

23
00:01:14,040 --> 00:01:19,885
the agent also receives some positive reward if the humanoid has not yet fallen.

24
00:01:19,885 --> 00:01:23,984
They frame the problem as an episodic task where if the human falls,

25
00:01:23,984 --> 00:01:26,025
then the episode is terminated.

26
00:01:26,025 --> 00:01:29,190
At this point, whatever cumulative reward the agent

27
00:01:29,189 --> 00:01:32,564
had at that time point is all it's ever going to get.

28
00:01:32,564 --> 00:01:35,084
In this way, the reward signal is designed,

29
00:01:35,084 --> 00:01:39,149
so if the robot focused entirely on maximizing this reward,

30
00:01:39,150 --> 00:01:42,510
it would also coincidentally learn to walk.

31
00:01:42,510 --> 00:01:47,400
To see this, first note that if the robot falls, the episode terminates.

32
00:01:47,400 --> 00:01:51,835
And that's a missed opportunity to collect more of this positive reward.

33
00:01:51,834 --> 00:01:55,494
And in general, if the robot walks for ten time steps,

34
00:01:55,495 --> 00:01:57,915
that's only 10 opportunities to get reward.

35
00:01:57,915 --> 00:02:00,450
And if it stays walking for 100,

36
00:02:00,450 --> 00:02:03,375
that's a lot more time to collect more reward.

37
00:02:03,375 --> 00:02:05,834
So if we get the reward in this way,

38
00:02:05,834 --> 00:02:09,715
the agent will try to keep from falling for as long as possible.

39
00:02:09,715 --> 00:02:14,474
Next, since the reward is proportional to the forward velocity,

40
00:02:14,474 --> 00:02:19,109
this will ensure the robot also feels pressured to walk as quickly as possible,

41
00:02:19,110 --> 00:02:21,375
in the direction of the walking track,

42
00:02:21,375 --> 00:02:26,735
but it also makes sense to penalize the agent for applying too much force to the joints.

43
00:02:26,735 --> 00:02:28,740
This is because otherwise,

44
00:02:28,740 --> 00:02:32,805
we could end up with a situation where the humanoid walks to erratically.

45
00:02:32,805 --> 00:02:34,939
By penalizing large forces,

46
00:02:34,939 --> 00:02:38,155
we can try to keep the movements more smooth and elegant.

47
00:02:38,155 --> 00:02:42,685
Likewise, we want to keep the agent on the track and moving forward.

48
00:02:42,685 --> 00:02:46,500
Otherwise, who knows where it could end up walking off to.

49
00:02:46,500 --> 00:02:51,094
Of course, the robot can't focus just on walking fast,

50
00:02:51,094 --> 00:02:53,509
or just on moving forward,

51
00:02:53,509 --> 00:02:56,284
or only on walking smoothly,

52
00:02:56,284 --> 00:03:00,310
or just on walking for as long as possible.

53
00:03:00,310 --> 00:03:04,625
These are four somewhat competing requirements that the agent has to balance for

54
00:03:04,625 --> 00:03:09,865
all time steps towards its goal of maximizing expected cumulative reward.

55
00:03:09,865 --> 00:03:14,655
And Google DeepMind demonstrated that from this very simple reward function,

56
00:03:14,655 --> 00:03:19,340
the agent is able to learn how to walk in a very human like fashion.

57
00:03:19,340 --> 00:03:22,009
In fact, this rewards function is so simple

58
00:03:22,009 --> 00:03:24,979
that it may seem that deciding reward is quite straightforward,

59
00:03:24,979 --> 00:03:28,155
but in general, this is not the case.

60
00:03:28,155 --> 00:03:31,844
Of course, there are some counterexamples to this.

61
00:03:31,844 --> 00:03:35,115
For instance, if you're teaching an agent to play a video game,

62
00:03:35,115 --> 00:03:37,715
the reward is just the score on the screen.

63
00:03:37,715 --> 00:03:41,200
And if you're teaching an agent to play Backgammon, well,

64
00:03:41,199 --> 00:03:43,854
the reward is delivered only at the end of the game,

65
00:03:43,854 --> 00:03:46,884
and you could construct it to be positive if the agent wins,

66
00:03:46,884 --> 00:03:48,834
and negative, if it loses.

67
00:03:48,835 --> 00:03:51,400
The fact, that the reward is so simple is

68
00:03:51,400 --> 00:03:55,000
precisely what makes this research from DeepMind so fascinating.

