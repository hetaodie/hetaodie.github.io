1
00:00:00,000 --> 00:00:02,100
我们将回收机器人作为讲解示例

2
00:00:02,100 --> 00:00:06,004
并详细介绍了状态和动作

3
00:00:06,004 --> 00:00:08,640
在此示例中 状态对应的是

4
00:00:08,640 --> 00:00:11,865
机器人的电池电量

5
00:00:11,865 --> 00:00:15,900
有两个潜在状态 很高和很低的电量

6
00:00:15,900 --> 00:00:20,329
首先 考虑电池电量很高的状态

7
00:00:20,329 --> 00:00:24,464
机器人可以选择搜索 等待或重新充电

8
00:00:24,464 --> 00:00:30,554
但如果电量很高 则没必要充电

9
00:00:30,554 --> 00:00:34,302
因此唯一的选择是搜索或等待

10
00:00:34,302 --> 00:00:38,159
如果智能体选择搜索

11
00:00:38,159 --> 00:00:39,919
在下个时间步

12
00:00:39,920 --> 00:00:42,725
状态可能是很高或很低的电量

13
00:00:42,725 --> 00:00:47,250
假设有 70% 的可能性保持高电量

14
00:00:47,250 --> 00:00:50,850
因此有 30% 的可能性保持很低的电量

15
00:00:50,850 --> 00:00:54,480
在两种情形下 假设决定搜索

16
00:00:54,479 --> 00:00:58,019
会导致机器人刚好收集 4 个易拉罐

17
00:00:58,020 --> 00:00:59,565
因此

18
00:00:59,564 --> 00:01:04,140
环境向智能体提供 4 个奖励

19
00:01:04,141 --> 00:01:06,765
另一个选项是等待

20
00:01:06,765 --> 00:01:11,040
如果机器人的电量很高并决定等待

21
00:01:11,040 --> 00:01:14,955
等待不消耗任何电量

22
00:01:14,954 --> 00:01:19,719
可以保证在下个时间步电量还是很高

23
00:01:19,719 --> 00:01:24,864
在这种情形下 我们假设因为机器人没有去积极地搜索易拉罐

24
00:01:24,864 --> 00:01:30,629
它能够收集的易拉罐数量更少 假设只获得了一个易拉罐

25
00:01:30,629 --> 00:01:33,149
因此

26
00:01:33,150 --> 00:01:37,109
环境向智能体提供 1 个奖励

27
00:01:37,109 --> 00:01:40,454
对于电量很低的状态

28
00:01:40,454 --> 00:01:44,224
机器人同样有三个选择

29
00:01:44,224 --> 00:01:48,495
如果电量很低 它选择等待有人扔进易拉罐

30
00:01:48,495 --> 00:01:54,180
不消耗任何电量 下个时间步的状态将为低电量

31
00:01:54,180 --> 00:01:58,095
和电量很高时机器人决定等待一样

32
00:01:58,094 --> 00:02:01,620
智能体将获得一个奖励

33
00:02:01,620 --> 00:02:04,011
如果机器人重新充电

34
00:02:04,010 --> 00:02:06,509
则回到充电基站

35
00:02:06,510 --> 00:02:10,289
下个时间步的状态肯定是电量很高

36
00:02:10,289 --> 00:02:15,525
假设回去的路上没有收集到易拉罐 获得的奖励是 0

37
00:02:15,525 --> 00:02:19,200
如果搜索易拉罐 则风险很高

38
00:02:19,199 --> 00:02:23,289
有可能这次没有风险 在下个时间步

39
00:02:23,289 --> 00:02:27,724
电量依然很低 但是没有完全耗尽

40
00:02:27,724 --> 00:02:31,829
只是更有可能会耗尽电量

41
00:02:31,830 --> 00:02:37,195
必须需要有人将它放到充电基站上充电

42
00:02:37,194 --> 00:02:41,219
因此下个时间步的电量很高

43
00:02:41,219 --> 00:02:45,120
假设机器人耗尽电量的概率是 80%

44
00:02:45,120 --> 00:02:50,034
否则有 20% 的概率躲过这次风险

45
00:02:50,034 --> 00:02:51,314
对于奖励

46
00:02:51,314 --> 00:02:53,444
如果机器人需要我们营救它

47
00:02:53,444 --> 00:02:57,060
则确保对这种情形进行惩罚

48
00:02:57,060 --> 00:03:00,659
假设我们不管它能够收集的易拉罐总数是多少

49
00:03:00,659 --> 00:03:05,865
直接将奖励设为 -3

50
00:03:05,865 --> 00:03:08,125
但是如果机器人没有耗尽电量

51
00:03:08,125 --> 00:03:12,539
它收集了 4 个易拉罐并获得 4 个奖励

52
00:03:12,539 --> 00:03:17,425
这个图表完全描述了

53
00:03:17,425 --> 00:03:22,679
环境决定在任何时间点的下个状态可以采用的方法

54
00:03:22,680 --> 00:03:26,849
为此 我们来看一个具体示例

55
00:03:26,849 --> 00:03:31,844
例如 上个状态是电量很高 智能体决定搜索

56
00:03:31,844 --> 00:03:34,469
然后环境将抛掷一个虚拟硬币

57
00:03:34,469 --> 00:03:39,240
正面朝上的概率是 70%

58
00:03:39,240 --> 00:03:40,770
如果硬币正面朝上

59
00:03:40,770 --> 00:03:43,020
环境会判断下个状态是很高的电量

60
00:03:43,020 --> 00:03:46,575
智能体获得 4 个奖励

61
00:03:46,574 --> 00:03:49,289
否则 如果背面朝上

62
00:03:49,289 --> 00:03:53,759
下一个状态将是很低的电量 奖励将为 4

63
00:03:53,759 --> 00:04:00,780
再举个例子 如果上个状态是电量很低 智能体决定去搜索易拉罐

64
00:04:00,780 --> 00:04:03,569
环境再次抛掷一个虚拟硬币

65
00:04:03,569 --> 00:04:08,159
正面朝上的概率是 80%

66
00:04:08,159 --> 00:04:09,704
如果正面朝上

67
00:04:09,705 --> 00:04:12,120
环境判断下个状态是电量很高

68
00:04:12,120 --> 00:04:16,560
智能体获得的奖励是 -3

69
00:04:16,560 --> 00:04:18,480
否则 如果背面朝上

70
00:04:18,480 --> 00:04:23,075
下个状态是电量很低 奖励是 4

71
00:04:23,074 --> 00:04:25,469
但是要强调的是

72
00:04:25,470 --> 00:04:29,780
环境做出决策所需的信息很少

73
00:04:29,779 --> 00:04:32,924
它不关心在 10 个或 100 个甚至 2 个时间步之前

74
00:04:32,925 --> 00:04:36,845
提供给智能体的情形

75
00:04:36,845 --> 00:04:41,610
并且不会查看在上个时间步之前智能体采取的动作

76
00:04:41,610 --> 00:04:45,629
智能体的表现或收集的奖励数量

77
00:04:45,629 --> 00:04:51,115
不影响环境如何选择对智能体做出响应

78
00:04:51,115 --> 00:04:54,300
当然 可以设计与智能体的互动流程

79
00:04:54,300 --> 00:04:58,129
更加复杂的环境

80
00:04:58,129 --> 00:05:00,634
但是强化学习就是这么实现的

81
00:05:00,634 --> 00:05:03,409
当你自己去实现强化学习问题时

82
00:05:03,410 --> 00:05:08,000
就会发现这个框架非常强大

