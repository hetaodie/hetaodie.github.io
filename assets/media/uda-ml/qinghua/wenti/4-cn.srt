1
00:00:00,000 --> 00:00:04,349
我们讨论了强化学习的各种应用

2
00:00:04,349 --> 00:00:07,195
每个应用都有一个智能体和环境

3
00:00:07,195 --> 00:00:09,120
每个智能体都有一个目标

4
00:00:09,119 --> 00:00:14,910
从学习无人驾驶的汽车到学习玩 Atari 游戏的智能体

5
00:00:14,910 --> 00:00:16,740
很令人震惊的是

6
00:00:16,739 --> 00:00:21,954
所有这些完全不同的目标可以通过相同的理论框架来实现

7
00:00:21,954 --> 00:00:24,869
到目前为止 我们通过小狗与主人互动

8
00:00:24,870 --> 00:00:28,710
了解了奖励这一概念

9
00:00:28,710 --> 00:00:31,170
在此情形中 任何时间步的状态是

10
00:00:31,170 --> 00:00:34,380
主人向小狗发出的指令

11
00:00:34,380 --> 00:00:36,480
动作是小狗的响应

12
00:00:36,479 --> 00:00:39,309
奖励是骨头奖赏次数

13
00:00:39,310 --> 00:00:42,405
和良好的强化学习智能体一样

14
00:00:42,405 --> 00:00:45,105
小狗的目标是最大化这种奖赏

15
00:00:45,104 --> 00:00:48,464
在这种情形下 奖励这一概念很自然

16
00:00:48,465 --> 00:00:52,075
与我们训练小狗的思维很一致

17
00:00:52,075 --> 00:00:56,295
但实际上 强化学习框架是指让所有智能体

18
00:00:56,295 --> 00:01:02,125
制定最大化期望累积奖励这一目标

19
00:01:02,125 --> 00:01:07,305
对于学习如何行走的机器人来说 奖励是什么意思呢？

20
00:01:07,305 --> 00:01:10,470
我们可以将这种环境看做训练员观察机器人的动作

21
00:01:10,469 --> 00:01:14,444
并在行走姿势正确时奖励机器人

22
00:01:14,444 --> 00:01:17,549
但是这种奖励可能会非常具有主观性

23
00:01:17,549 --> 00:01:21,375
根本没有科学定义

24
00:01:21,375 --> 00:01:22,995
什么样的行走姿势是好的姿势？

25
00:01:22,995 --> 00:01:24,450
什么样的又是坏的姿势？

26
00:01:24,450 --> 00:01:26,549
如何处理此问题？

27
00:01:26,549 --> 00:01:29,909
通常 如何指定奖励

28
00:01:29,909 --> 00:01:34,109
以便描述智能体可能会具有的任何潜在目标数量？

29
00:01:34,109 --> 00:01:36,355
在回答此问题之前

30
00:01:36,355 --> 00:01:38,180
我们先来了解另一个知识

31
00:01:38,180 --> 00:01:41,520
要注意的是 “强化”和“强化学习”

32
00:01:41,519 --> 00:01:46,164
这两个术语本身来自行为科学

33
00:01:46,165 --> 00:01:49,470
是指在行为之后立即发生的刺激

34
00:01:49,469 --> 00:01:53,605
使该行为在未来更有可能发生

35
00:01:53,605 --> 00:01:57,040
采用这一名称并非偶然

36
00:01:57,040 --> 00:02:00,359
实际上 在强化学习中 有一个重要的定义假设

37
00:02:00,359 --> 00:02:04,560
即智能体的目标始终可以描述为

38
00:02:04,560 --> 00:02:09,170
最大化期望累积奖励

39
00:02:09,169 --> 00:02:13,109
我们将这种假设称之为奖励假设

40
00:02:13,110 --> 00:02:17,055
如果你依然不太理解 不用担心 并不是只有你一个人有这种想法

41
00:02:17,055 --> 00:02:20,000
我将在下个视频中详细讲解

