1
00:00:00,000 --> 00:00:02,100
So we're working with an example of

2
00:00:02,100 --> 00:00:06,004
a recycling robot and we've already detailed the states and actions.

3
00:00:06,004 --> 00:00:08,640
In this example, remember that the state

4
00:00:08,640 --> 00:00:11,865
corresponds to the charge left on the robot's battery.

5
00:00:11,865 --> 00:00:15,900
And there are two potential states, high and low.

6
00:00:15,900 --> 00:00:20,329
As a first step, consider the case of the charge on the battery is high.

7
00:00:20,329 --> 00:00:24,464
Then, the robot could choose to search, wait, or recharge.

8
00:00:24,464 --> 00:00:30,554
But actually, recharging doesn't make much sense if the battery is already high,

9
00:00:30,554 --> 00:00:34,302
so we'll say that the only options are to search or wait.

10
00:00:34,302 --> 00:00:38,159
All right, so if the agent chooses to search,

11
00:00:38,159 --> 00:00:39,919
then at the next time step,

12
00:00:39,920 --> 00:00:42,725
the state could be high or low.

13
00:00:42,725 --> 00:00:47,250
Let's say that with 70 percent probability, it stays high.

14
00:00:47,250 --> 00:00:50,850
So there's a 30 percent chance the battery switches to low.

15
00:00:50,850 --> 00:00:54,480
In both cases, we'll say that this decision to search led

16
00:00:54,479 --> 00:00:58,019
to the robot collecting exactly four cans.

17
00:00:58,020 --> 00:00:59,565
And in line with this,

18
00:00:59,564 --> 00:01:04,140
the environment gives the agent a reward of four.

19
00:01:04,141 --> 00:01:06,765
The other option is to wait.

20
00:01:06,765 --> 00:01:11,040
If the robot has a high battery and then decides to wait, well,

21
00:01:11,040 --> 00:01:14,955
waiting doesn't use any battery at all and we'll say that then,

22
00:01:14,954 --> 00:01:19,719
it's guaranteed that the battery will again be high at the next time step.

23
00:01:19,719 --> 00:01:24,864
In this case, we'll suppose that since the robot wasn't out actively searching,

24
00:01:24,864 --> 00:01:30,629
it's able to collect fewer cans and say it's delivered just one can.

25
00:01:30,629 --> 00:01:33,149
And again in line with this,

26
00:01:33,150 --> 00:01:37,109
the environment gives the agent a reward of one.

27
00:01:37,109 --> 00:01:40,454
Onto the case where the battery is low.

28
00:01:40,454 --> 00:01:44,224
Again, the robot has three options.

29
00:01:44,224 --> 00:01:48,495
If the battery is low and it chooses to wait for people to bring cans,

30
00:01:48,495 --> 00:01:54,180
that doesn't use any battery until the state at the next time step is going to be low.

31
00:01:54,180 --> 00:01:58,095
And just like when the robot decided to wait when the battery was high,

32
00:01:58,094 --> 00:02:01,620
the agent gets a reward of one.

33
00:02:01,620 --> 00:02:04,011
If the robot recharges,

34
00:02:04,010 --> 00:02:06,509
then it goes back to the docking station and

35
00:02:06,510 --> 00:02:10,289
the state of that the next time step is guaranteed to be high.

36
00:02:10,289 --> 00:02:15,525
Say it collects no cans along the way and gets a reward of zero.

37
00:02:15,525 --> 00:02:19,200
And if it searches, well, that's risky.

38
00:02:19,199 --> 00:02:23,289
It's possible that it gets away with this and then at the next time step,

39
00:02:23,289 --> 00:02:27,724
the battery is still low but not entirely depleted.

40
00:02:27,724 --> 00:02:31,829
But it's probably more likely that the robot depletes its battery,

41
00:02:31,830 --> 00:02:37,195
has to be rescued and is carried to a docking station to be charged.

42
00:02:37,194 --> 00:02:41,219
So the charge on its battery at the next time step is high.

43
00:02:41,219 --> 00:02:45,120
So the robot depletes its battery with 80 percent probability

44
00:02:45,120 --> 00:02:50,034
and otherwise gets away with that risky action with 20 percent probability.

45
00:02:50,034 --> 00:02:51,314
As for the reward,

46
00:02:51,314 --> 00:02:53,444
if the robot needs to be rescued,

47
00:02:53,444 --> 00:02:57,060
we want to make sure we're punishing the robot in this case,

48
00:02:57,060 --> 00:03:00,659
so say we don't look at all at the number of cans it was able to

49
00:03:00,659 --> 00:03:05,865
collect and we just give the robot a reward of negative three for that.

50
00:03:05,865 --> 00:03:08,125
But if the robot gets away with it,

51
00:03:08,125 --> 00:03:12,539
he collects four cans and get the reward of four.

52
00:03:12,539 --> 00:03:17,425
This picture completely characterizes one method that

53
00:03:17,425 --> 00:03:22,679
the environment could use to decide the next state in reward at any point in time.

54
00:03:22,680 --> 00:03:26,849
To see this, let's look at a concrete example.

55
00:03:26,849 --> 00:03:31,844
Say for instance, the last state was high and the agent decided to search.

56
00:03:31,844 --> 00:03:34,469
Then, the environment would flip

57
00:03:34,469 --> 00:03:39,240
a theoretical coin with 70 percent probability of landing heads,

58
00:03:39,240 --> 00:03:40,770
and if that coin landed heads,

59
00:03:40,770 --> 00:03:43,020
the environment would decide that the next state was

60
00:03:43,020 --> 00:03:46,575
high and the agent would get a reward four.

61
00:03:46,574 --> 00:03:49,289
Otherwise, if it landed tails,

62
00:03:49,289 --> 00:03:53,759
the next state would be low and the reward would be four.

63
00:03:53,759 --> 00:04:00,780
As another example, if the last state was low and the agent decided to search,

64
00:04:00,780 --> 00:04:03,569
the environment would again flip

65
00:04:03,569 --> 00:04:08,159
a theoretical coin now with 80 percent probability of landing heads.

66
00:04:08,159 --> 00:04:09,704
If it landed heads,

67
00:04:09,705 --> 00:04:12,120
the environment would decide the next state was

68
00:04:12,120 --> 00:04:16,560
high and the agent would get a reward of negative three.

69
00:04:16,560 --> 00:04:18,480
Otherwise, if it landed tails,

70
00:04:18,480 --> 00:04:23,075
the next state would be low and the reward would be four.

71
00:04:23,074 --> 00:04:25,469
But what's important to emphasize here is

72
00:04:25,470 --> 00:04:29,780
how little information the environment uses to make decisions.

73
00:04:29,779 --> 00:04:32,924
It doesn't care what situation was presented to the agent

74
00:04:32,925 --> 00:04:36,845
10 or 100 or even two steps prior.

75
00:04:36,845 --> 00:04:41,610
And it doesn't look at the actions that the agent took prior to the last one.

76
00:04:41,610 --> 00:04:45,629
And how well the agent is doing or how much reward it's collected

77
00:04:45,629 --> 00:04:51,115
has no effect on how the environment chooses to respond to the agent.

78
00:04:51,115 --> 00:04:54,300
Of course, it's possible to design environments that

79
00:04:54,300 --> 00:04:58,129
have much more complex procedures for interacting with the agent,

80
00:04:58,129 --> 00:05:00,634
but this is how it's done in reinforcement learning,

81
00:05:00,634 --> 00:05:03,409
and you'll see soon for yourself in

82
00:05:03,410 --> 00:05:08,000
your implementations just how powerful this framework is.

