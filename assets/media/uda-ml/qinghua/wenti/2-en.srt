1
00:00:00,000 --> 00:00:02,640
Remember the cute puppy from the previous lesson?

2
00:00:02,640 --> 00:00:06,269
He set the stage as an agent who learns from trial

3
00:00:06,269 --> 00:00:10,644
and error how to behave in an environment to maximize reward.

4
00:00:10,644 --> 00:00:15,114
But what do we mean when we talk about reinforcement learning in general?

5
00:00:15,115 --> 00:00:16,905
Well, you might be surprised to hear

6
00:00:16,905 --> 00:00:20,980
that not much changes when we trade this puppy for a self-driving car,

7
00:00:20,980 --> 00:00:26,414
a robot, or a more in general, reinforcement learning agent.

8
00:00:26,414 --> 00:00:29,489
In particular, the RL framework is characterized

9
00:00:29,489 --> 00:00:33,659
by an agent learned to interact with its environment.

10
00:00:33,659 --> 00:00:37,199
We assume that time evolves and discrete timesteps.

11
00:00:37,200 --> 00:00:38,910
At the initial timestep,

12
00:00:38,909 --> 00:00:42,359
the agent observes the environment.

13
00:00:42,359 --> 00:00:43,950
You can think of this observation as

14
00:00:43,950 --> 00:00:46,995
a situation that the environment presents to the agent.

15
00:00:46,994 --> 00:00:52,199
Then, it must select an appropriate action in response.

16
00:00:52,200 --> 00:00:55,425
Then at the next timestep in response to the agents action,

17
00:00:55,424 --> 00:00:59,149
the environment presents a new situation to the agent.

18
00:00:59,149 --> 00:01:03,869
At the same time the environment gives the agent a reward which provides

19
00:01:03,869 --> 00:01:09,759
some indication of whether the agent has responded appropriately to the environment.

20
00:01:09,760 --> 00:01:12,480
Then the process continues where at each timestep

21
00:01:12,480 --> 00:01:16,290
the environment sends the agent an observation and reward.

22
00:01:16,290 --> 00:01:21,800
And in response, the agent must choose an action.

23
00:01:21,799 --> 00:01:24,810
In general, we don't need to assume that the environment

24
00:01:24,810 --> 00:01:28,875
shows the agent everything he needs to make well-informed decisions.

25
00:01:28,875 --> 00:01:33,165
But it greatly simplifies the underlying mathematics if we do.

26
00:01:33,165 --> 00:01:34,725
So in this course,

27
00:01:34,724 --> 00:01:37,199
we'll make the assumption that the agent is able to

28
00:01:37,200 --> 00:01:40,605
fully observe what ever state the environment is in.

29
00:01:40,605 --> 00:01:45,418
And instead of referring to the agent as receiving an observation,

30
00:01:45,418 --> 00:01:49,820
well, Huntsworth say that it receives the environment state.

31
00:01:49,819 --> 00:01:52,829
But let's make this description a bit clearer with

32
00:01:52,829 --> 00:01:59,134
some added notation where we again start from the very beginning at timestep zero.

33
00:01:59,135 --> 00:02:03,480
The agent first receives the environment state which we denote by S0,

34
00:02:03,480 --> 00:02:07,635
where zero stands for a timestep zero of course.

35
00:02:07,635 --> 00:02:11,745
Then, based on that observation the agent chooses an action,

36
00:02:11,745 --> 00:02:15,120
A0, at the next timestep,

37
00:02:15,120 --> 00:02:17,640
in this case, it timestep one and that's

38
00:02:17,639 --> 00:02:22,289
a direct consequence of the agent's choice of action, A0.

39
00:02:22,289 --> 00:02:24,810
And the environments previous state, S0,

40
00:02:24,810 --> 00:02:28,099
the environment transitions to a new state, S1,

41
00:02:28,099 --> 00:02:30,284
and gives some reward,

42
00:02:30,284 --> 00:02:32,530
R1, to the agent.

43
00:02:32,530 --> 00:02:36,080
The agent then chooses an action, A1.

44
00:02:36,080 --> 00:02:42,680
At timestep two, the process continues where the environment passes the reward in state.

45
00:02:42,680 --> 00:02:47,659
Then the agent responds with an action and so on.

46
00:02:47,659 --> 00:02:50,340
Whereas the agent interacts with the environment,

47
00:02:50,340 --> 00:02:56,414
this interaction is manifest as a sequence of states, actions, and rewards.

48
00:02:56,413 --> 00:03:01,349
That said, the reward will always be the most relevant quantity to the agent.

49
00:03:01,349 --> 00:03:05,370
To be specific, any agent has the goal to maximize

50
00:03:05,370 --> 00:03:11,270
expected cumulative reward or the some of the rewards attained over all timesteps.

51
00:03:11,270 --> 00:03:14,310
In other words, it seeks to find the strategy for choosing

52
00:03:14,310 --> 00:03:18,490
actions with the cumulative reward is likely to be quite high.

53
00:03:18,490 --> 00:03:23,879
And the agent can only accomplish this by interacting with the environment.

54
00:03:23,879 --> 00:03:26,025
This is because at every timestep,

55
00:03:26,025 --> 00:03:30,414
the environment decides how much reward the agent receives.

56
00:03:30,414 --> 00:03:34,139
In other words, the agent must play by the rules of the environment.

57
00:03:34,139 --> 00:03:36,779
But through interaction, the agent can learn

58
00:03:36,780 --> 00:03:40,763
those rules and choose appropriate actions to accomplish its goal.

59
00:03:40,763 --> 00:03:44,509
And this is essentially what we'll try to accomplish in this course.

60
00:03:44,509 --> 00:03:47,399
But it's important to emphasize that all of this is

61
00:03:47,400 --> 00:03:50,605
just a mathematical model for a real world problem.

62
00:03:50,604 --> 00:03:52,919
So if you have a problem in mind that you

63
00:03:52,919 --> 00:03:55,559
think can be solved with reinforcement learning,

64
00:03:55,560 --> 00:03:58,155
you will have to specify the states, actions,

65
00:03:58,155 --> 00:04:03,270
and rewards, and you'll have to decide the rules of the environment in this course.

66
00:04:03,270 --> 00:04:07,000
You'll see a lot of examples for how to accomplish this.

