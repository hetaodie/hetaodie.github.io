1
00:00:00,000 --> 00:00:02,745
我们来看看我们面临的问题

2
00:00:02,745 --> 00:00:05,259
我们有一个智能体和一个环境

3
00:00:05,259 --> 00:00:08,009
时间被拆分为离散时间步

4
00:00:08,009 --> 00:00:09,285
在每个时间步

5
00:00:09,285 --> 00:00:12,269
智能体都从环境中获得一个奖励和状态

6
00:00:12,269 --> 00:00:16,835
并选择一个响应动作

7
00:00:16,835 --> 00:00:22,605
这样的话 互动变成一系列的状态 动作和奖励

8
00:00:22,605 --> 00:00:27,300
在这节课 我们将重点讨论阶段性任务

9
00:00:27,300 --> 00:00:32,535
当智能体在时间 T 遇到最终状态时 互动结束

10
00:00:32,534 --> 00:00:35,324
我们将这一过程称为一个阶段

11
00:00:35,325 --> 00:00:38,445
对于任何阶段 智能体的目标都是发现最优策略

12
00:00:38,445 --> 00:00:44,219
以便最大化预期累积奖励

13
00:00:44,219 --> 00:00:48,450
为此 我们将从预测问题开始

14
00:00:48,450 --> 00:00:54,150
给定策略后 智能体如何估算该策略的值函数？

15
00:00:54,149 --> 00:00:58,452
注意 智能体不了解该环境的动态特性

16
00:00:58,453 --> 00:01:03,638
因此它需要通过与环境互动估算值函数

17
00:01:03,637 --> 00:01:05,828
为了与环境互动

18
00:01:05,828 --> 00:01:09,250
智能体需要遵守一个策略

19
00:01:09,250 --> 00:01:12,329
我们可以用一个策略来进行评估

20
00:01:12,329 --> 00:01:16,875
并用另一个策略来与环境互动

21
00:01:16,875 --> 00:01:21,352
称之为针对预测问题的离线策略方法

22
00:01:21,352 --> 00:01:26,129
离线策略方法具有一定的复杂性 稍后我们将详细讲解

23
00:01:26,129 --> 00:01:29,564
我们先讲解异同策略方法

24
00:01:29,564 --> 00:01:33,375
即智能体通过遵循某个策略与环境互动

25
00:01:33,375 --> 00:01:36,531
并计算该策略的值函数

26
00:01:36,531 --> 00:01:39,025
在详细讲解该算法之前

27
00:01:39,025 --> 00:01:41,484
我们来看一个示例

28
00:01:41,484 --> 00:01:46,004
假设我们要处理一个阶段性任务 该 MDP 具有三个状态

29
00:01:46,004 --> 00:01:50,530
X Y 和 Z 其中 Z 是最终状态

30
00:01:50,530 --> 00:01:54,254
为了更直观地理解该 MDP

31
00:01:54,254 --> 00:01:56,234
你可以将问题看做一个

32
00:01:56,234 --> 00:01:58,314
非常非常小的冰雪环境

33
00:01:58,314 --> 00:02:04,679
假设状态 X 和 Y 对应于一座白雪皑皑的森林中的不同地点

34
00:02:04,680 --> 00:02:11,465
森林中有令人愉悦和害怕的景象 对应于正面或负面奖励

35
00:02:11,465 --> 00:02:17,159
状态 Z 是最终状态 对应于森林中的一个小屋

36
00:02:17,159 --> 00:02:19,365
如果智能体抵达该状态

37
00:02:19,365 --> 00:02:22,159
则进入小屋并结束这一阶段

38
00:02:22,159 --> 00:02:24,525
假设有两个潜在动作

39
00:02:24,525 --> 00:02:28,913
向上和向下 和冰冻湖泊环境相似

40
00:02:28,913 --> 00:02:30,855
该环境非常光滑

41
00:02:30,854 --> 00:02:33,659
如果智能体选择向下动作

42
00:02:33,659 --> 00:02:35,384
在下个时间步

43
00:02:35,384 --> 00:02:41,084
有一定的概率表明它会向上移动或保持不动

44
00:02:41,085 --> 00:02:46,680
同样 如果它决定向上移动 当它尝试朝着该方向移动时

45
00:02:46,680 --> 00:02:51,000
可能会向下移动或保持不动

46
00:02:51,000 --> 00:02:55,530
假设我们要评估以下策略

47
00:02:55,530 --> 00:03:00,328
智能体在状态 X 向上移动 在状态 Y 向下移动

48
00:03:00,328 --> 00:03:02,175
智能体可以通过遵循该策略

49
00:03:02,175 --> 00:03:05,535
与环境互动

50
00:03:05,534 --> 00:03:07,935
假设在一开始互动时

51
00:03:07,935 --> 00:03:10,545
智能体观察状态 X

52
00:03:10,544 --> 00:03:12,524
然后根据策略

53
00:03:12,525 --> 00:03:15,390
选择向上动作

54
00:03:15,389 --> 00:03:19,500
结果获得奖励 -2 并进入下个状态 Y

55
00:03:19,500 --> 00:03:21,900
接着根据该策略

56
00:03:21,900 --> 00:03:23,520
选择向下动作

57
00:03:23,520 --> 00:03:24,774
结果

58
00:03:24,774 --> 00:03:28,689
获得奖励 0 并进入下个状态 Y

59
00:03:28,689 --> 00:03:32,699
此刻 根据该策略选择向下动作

60
00:03:32,699 --> 00:03:36,889
结果获得奖励 3 并抵达最终状态 Z

61
00:03:36,889 --> 00:03:39,059
互动结束

62
00:03:39,060 --> 00:03:44,090
假设智能体在另外两个阶段与环境互动

63
00:03:44,090 --> 00:03:48,485
我们可以使用这些阶段估算值函数

64
00:03:48,485 --> 00:03:51,420
当然 这三个简短的互动阶段

65
00:03:51,419 --> 00:03:56,024
并不足以让智能体充分了解环境

66
00:03:56,025 --> 00:03:58,890
暂时先假设已足够

67
00:03:58,889 --> 00:04:03,309
我们将从一个状态开始 例如状态 X

68
00:04:03,310 --> 00:04:08,599
然后查看在所有阶段的状态 X 的所有状况

69
00:04:08,599 --> 00:04:13,269
接着计算在出现该状态之后的折扣回报

70
00:04:13,270 --> 00:04:17,699
假设折扣率是 1 即不折扣

71
00:04:17,699 --> 00:04:20,235
对于第一个阶段

72
00:04:20,235 --> 00:04:22,206
回报是 -2

73
00:04:22,206 --> 00:04:24,790
加上 0 加上 3 结果为 1

74
00:04:24,790 --> 00:04:26,955
在第三个阶段

75
00:04:26,954 --> 00:04:31,349
回报是 -3 加上 3 结果为 0

76
00:04:31,350 --> 00:04:34,770
蒙特卡罗预测算法会对这些值取平均值

77
00:04:34,769 --> 00:04:40,544
并代入状态 X 的值估算方程中

78
00:04:40,545 --> 00:04:45,824
这里 状态 X 的估值为 1/2

79
00:04:45,824 --> 00:04:48,629
该算法比较直观

80
00:04:48,629 --> 00:04:51,709
状态的值定义为

81
00:04:51,709 --> 00:04:54,959
该状态之后的预期回报

82
00:04:54,959 --> 00:05:01,114
因此智能体体验的平均回报是个很好的估值

83
00:05:01,115 --> 00:05:04,930
我们将按照相同的流程评估状态 Y

84
00:05:04,930 --> 00:05:08,025
你将发现状态 Y 在每个阶段出现了多次

85
00:05:08,024 --> 00:05:12,529
这种情况不太清楚该如何处理

86
00:05:12,529 --> 00:05:16,274
为了解决这一问题 我们需要介绍几个其他术语

87
00:05:16,274 --> 00:05:22,185
我们将状态在某个阶段中的每次访问定义为该状态的访问

88
00:05:22,185 --> 00:05:24,720
如果状态在某个阶段中访问了多次

89
00:05:24,720 --> 00:05:28,245
我们有两种处理方式

90
00:05:28,245 --> 00:05:30,030
第一种方式是

91
00:05:30,029 --> 00:05:33,344
在每个阶段 我们只考虑该状态的首次访问

92
00:05:33,345 --> 00:05:38,070
并对这些回报取平均值

93
00:05:38,069 --> 00:05:42,995
在这种情况下 状态 Y 的值估算为

94
00:05:42,995 --> 00:05:47,069
3 加 3 再加 1 的平均值 即 3/7

95
00:05:47,069 --> 00:05:48,490
如果采用这种方式

96
00:05:48,490 --> 00:05:52,875
则表示我们使用的是首次经历 MC 方法

97
00:05:52,875 --> 00:05:55,524
另一种方式是对所有阶段中

98
00:05:55,524 --> 00:06:00,154
状态 Y 的所有访问之后的回报取平均值

99
00:06:00,154 --> 00:06:03,299
在这种情况下 状态 Y 的值估算为

100
00:06:03,300 --> 00:06:06,884
所有这些数字的平均值

101
00:06:06,884 --> 00:06:11,295
结果为 14/7 即 2

102
00:06:11,295 --> 00:06:16,035
很快你将有机会自己实现该算法

103
00:06:16,035 --> 00:06:20,939
你可以选择首次经历或所有经历 MC 预测方法

104
00:06:20,939 --> 00:06:23,060
也可以同时实现这两种方式

