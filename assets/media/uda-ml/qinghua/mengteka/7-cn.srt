1
00:00:00,000 --> 00:00:03,120
我们已经拥有了完善的策略评估方法

2
00:00:03,120 --> 00:00:06,095
现在将重点了解如何执行策略完善步骤

3
00:00:06,094 --> 00:00:08,355
给定一个动作值函数

4
00:00:08,355 --> 00:00:11,015
如何使用它来提出策略？

5
00:00:11,015 --> 00:00:14,935
可以直接复制动态规划设置中的算法吗？

6
00:00:14,935 --> 00:00:17,190
或许可以用动作值函数

7
00:00:17,190 --> 00:00:19,825
构建相应的贪婪策略

8
00:00:19,824 --> 00:00:24,054
换句话说 在每个状态 我们直接选择值最高的动作

9
00:00:24,054 --> 00:00:26,939
你认为现在该方法有效吗？

10
00:00:26,940 --> 00:00:28,855
答案是也算有效

11
00:00:28,855 --> 00:00:30,780
如果你打算将其与

12
00:00:30,780 --> 00:00:34,498
刚刚学会的策略评估算法结合使用

13
00:00:34,497 --> 00:00:36,794
则需要稍加修改

14
00:00:36,795 --> 00:00:39,450
为此 我们来看一个示例

15
00:00:39,450 --> 00:00:42,815
假设你是一个智能体 你面前有两扇门

16
00:00:42,814 --> 00:00:45,314
你需要判断哪扇门后面的值更高

17
00:00:45,314 --> 00:00:49,924
我们来看看如何使用蒙特卡洛方法解决这一问题

18
00:00:49,924 --> 00:00:53,969
一开始 你没有任何理由偏爱某扇门

19
00:00:53,969 --> 00:00:58,359
因此假设将每扇门的估算值初始化为 0

20
00:00:58,359 --> 00:01:00,884
为了判断应该打开哪扇门

21
00:01:00,884 --> 00:01:02,114
假设你抛掷了一枚硬币

22
00:01:02,115 --> 00:01:04,500
背面朝上 因此你打开门 B

23
00:01:04,500 --> 00:01:07,459
这么做时 收到奖励 0

24
00:01:07,459 --> 00:01:09,455
为简单起见

25
00:01:09,456 --> 00:01:12,435
当打开一扇门时 一个阶段结束

26
00:01:12,435 --> 00:01:13,875
换句话说

27
00:01:13,875 --> 00:01:15,420
打开门 B 后

28
00:01:15,420 --> 00:01:17,745
你收到回报 0

29
00:01:17,745 --> 00:01:20,160
这不会更改值函数的估值

30
00:01:20,159 --> 00:01:24,429
因此可以再次随机选择一扇门

31
00:01:24,430 --> 00:01:25,750
你抛掷了一枚硬币

32
00:01:25,750 --> 00:01:27,105
这次正面朝上

33
00:01:27,105 --> 00:01:28,576
打开门 A

34
00:01:28,576 --> 00:01:29,810
这么做时

35
00:01:29,810 --> 00:01:31,799
获得奖励 1

36
00:01:31,799 --> 00:01:36,250
这样就会将门 A 的估值更新为 1

37
00:01:36,250 --> 00:01:39,915
现在如果对值函数进行贪婪的操作

38
00:01:39,915 --> 00:01:41,925
再次打开门 A

39
00:01:41,924 --> 00:01:44,799
假设这次获得奖励 3

40
00:01:44,799 --> 00:01:47,709
使门 A 的值更新为 2

41
00:01:47,709 --> 00:01:49,589
在下个时间点

42
00:01:49,590 --> 00:01:52,409
贪婪策略表示再次选择门 A

43
00:01:52,409 --> 00:01:55,019
每次这么做时

44
00:01:55,019 --> 00:01:59,265
我们都获得正面奖励 始终是 1 或 3

45
00:01:59,265 --> 00:02:02,555
因此我们一直都打开相同的门

46
00:02:02,555 --> 00:02:04,088
这样就存在一个很大的问题

47
00:02:04,087 --> 00:02:08,530
因为我们始终没有机会看看第二扇门后面的值

48
00:02:08,530 --> 00:02:13,770
例如 假设门 A 后面的机制是你期望的结果

49
00:02:13,770 --> 00:02:16,515
它产生奖励 1 或 3

50
00:02:16,514 --> 00:02:23,484
二者的概率一样 但是门 B 后面的机制产生奖励 0 或 100

51
00:02:23,485 --> 00:02:26,190
这是你希望发现的信息

52
00:02:26,189 --> 00:02:29,849
但是贪婪策略却阻止你这么做

53
00:02:29,849 --> 00:02:33,030
问题是 当我们很早就遇到

54
00:02:33,030 --> 00:02:37,800
门 A 似乎比门 B 更有利的情形时

55
00:02:37,800 --> 00:02:40,945
我们非常有必要花费更多的时间确认这一点

56
00:02:40,944 --> 00:02:43,814
因为一开始的猜测不正确

57
00:02:43,814 --> 00:02:46,960
更好的策略是采用随机性策略

58
00:02:46,960 --> 00:02:51,210
而不是构建刚才的贪婪策略

59
00:02:51,210 --> 00:02:55,890
例如选择门 A 的概率是 95% 选择门 B 的概率是 5%

60
00:02:55,889 --> 00:02:58,804
依然很接近贪婪策略

61
00:02:58,805 --> 00:03:00,990
我们的方法依然很适宜

62
00:03:00,990 --> 00:03:03,300
但是如果我们继续以很小的概率选择门 B

63
00:03:03,300 --> 00:03:06,390
就会获得额外的价值

64
00:03:06,389 --> 00:03:10,349
在某个时刻 我们将看到回报 100

65
00:03:10,349 --> 00:03:15,693
这个示例解释了如何定义蒙特卡洛策略完善算法

66
00:03:15,693 --> 00:03:18,915
我们不再始终构建贪婪策略

67
00:03:18,915 --> 00:03:20,900
而是构建一个随机策略

68
00:03:20,900 --> 00:03:25,075
以非常高的概率选择贪婪动作

69
00:03:25,074 --> 00:03:30,774
但是以很小的非零概率选择某个非贪婪动作

70
00:03:30,775 --> 00:03:35,955
在这种情况下 你将设置一个很小的正数 ϵ

71
00:03:35,955 --> 00:03:39,504
设置得越大 你就越有可能选择非贪婪动作

72
00:03:39,503 --> 00:03:42,679
称之为 epsilon 贪婪算法

73
00:03:42,680 --> 00:03:46,155
ϵ 必须是 0 到 1 之间的数字

74
00:03:46,155 --> 00:03:49,155
智能体以 1-ϵ 的概率

75
00:03:49,155 --> 00:03:51,719
选择贪婪动作

76
00:03:51,719 --> 00:03:53,935
并以概率 ϵ

77
00:03:53,935 --> 00:03:56,145
随机选择任何动作

78
00:03:56,145 --> 00:03:59,460
只要将 ϵ 设为一个很小的数字

79
00:03:59,460 --> 00:04:04,920
我就能够构建一个非常接近贪婪策略的策略

80
00:04:04,919 --> 00:04:07,139
并且不会阻止智能体

81
00:04:07,139 --> 00:04:11,099
继续探索其他可能性

82
00:04:11,099 --> 00:04:16,139
总之 我们的蒙特卡洛控制算法将如下所示

83
00:04:16,139 --> 00:04:18,060
我代入了优化步骤将获得

84
00:04:18,060 --> 00:04:22,439
Epsilon 贪婪策略这个结果

