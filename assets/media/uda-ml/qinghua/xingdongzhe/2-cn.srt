1
00:00:00,000 --> 00:00:04,950
回忆下典型策略梯度更新规则中使用的得分函数

2
00:00:04,950 --> 00:00:09,839
对于阶段性任务 每个实例有一个清晰的开始和结束点

3
00:00:09,839 --> 00:00:14,609
你可以使用阶段回报 Gt 作为得分函数的值

4
00:00:14,609 --> 00:00:16,829
它等同于在该阶段后续时间内

5
00:00:16,829 --> 00:00:20,109
获得的所有奖励的折扣和

6
00:00:20,109 --> 00:00:22,980
这就是蒙特卡洛方法的基础

7
00:00:22,980 --> 00:00:25,125
例如在每个阶段结束时

8
00:00:25,125 --> 00:00:31,146
进行更新的强化算法

9
00:00:31,146 --> 00:00:35,563
但是如果任务不是阶段性的 该怎么办？

10
00:00:35,563 --> 00:00:38,065
如果没有清晰的结束点

11
00:00:38,064 --> 00:00:40,780
则无法计算折扣回报

12
00:00:40,780 --> 00:00:44,469
更糟糕的是 何时进行策略更新？

13
00:00:44,469 --> 00:00:47,799
很明显我们需要更好的得分函数

14
00:00:47,799 --> 00:00:52,524
在与环境互动时能够在线计算的函数

15
00:00:52,524 --> 00:00:56,777
并且不依赖于正在运行的整个阶段

16
00:00:56,777 --> 00:00:59,050
我们将更新规则中的阶段

17
00:00:59,049 --> 00:01:03,324
替换为当前状态动作对的动作值

18
00:01:03,325 --> 00:01:08,859
没错 和我们在基于值的方法中尝试估算的动作值相同

19
00:01:08,859 --> 00:01:11,484
绕了一个圈 对吧？

20
00:01:11,484 --> 00:01:14,635
怎么找到这些动作值？

21
00:01:14,635 --> 00:01:18,835
我们需要自己弄清楚这些值

22
00:01:18,834 --> 00:01:20,640
除非有个预言者可以明确告诉这些值应该是多少

23
00:01:20,640 --> 00:01:22,265
例如 我们可以使用时间差分机制

24
00:01:22,265 --> 00:01:27,170
迭代地更新这些动作值

25
00:01:27,170 --> 00:01:30,019
注意 该流程可以与策略更新同步运行

26
00:01:30,019 --> 00:01:34,194
不需要在整个阶段运行完毕后再执行

27
00:01:34,194 --> 00:01:39,215
因此 可以用于非阶段性或连续性任务

28
00:01:39,215 --> 00:01:43,159
实际上 你可以选择任何合适的表示法来存储这些 Q 值

29
00:01:43,159 --> 00:01:47,239
然后使用合适的算法更新它们

30
00:01:47,239 --> 00:01:51,634
注意 这里的 β 是另一个学习速率或步长参数

31
00:01:51,635 --> 00:01:55,180
和 α 一样 但是针对的是值更新

