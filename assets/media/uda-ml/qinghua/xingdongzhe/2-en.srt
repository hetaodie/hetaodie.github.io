1
00:00:00,000 --> 00:00:04,950
Recall the score function used in a typical policy gradients update rule.

2
00:00:04,950 --> 00:00:09,839
For episodic tasks, where each instance has a clear beginning and end,

3
00:00:09,839 --> 00:00:14,609
you can use the episode returns g t as the value of the score function.

4
00:00:14,609 --> 00:00:16,829
It is equal to the discounted sum of

5
00:00:16,829 --> 00:00:20,109
all rewards obtained in the remainder of the episode.

6
00:00:20,109 --> 00:00:22,980
This forms the basis for Monte Carlo approaches,

7
00:00:22,980 --> 00:00:25,125
such as the reinforce algorithm,

8
00:00:25,125 --> 00:00:31,146
that performs updates at the end of each episode.

9
00:00:31,146 --> 00:00:35,563
But what do you do if your task is not episodic?

10
00:00:35,563 --> 00:00:38,065
If you do not have a clear termination point,

11
00:00:38,064 --> 00:00:40,780
you won't be able to compute discounted returns.

12
00:00:40,780 --> 00:00:44,469
Worse, when would you perform policy updates?

13
00:00:44,469 --> 00:00:47,799
Clearly, we need a better score function.

14
00:00:47,799 --> 00:00:52,524
Something that can be computed online as we interact with the environment.

15
00:00:52,524 --> 00:00:56,777
Something that is not dependent on an entire episode being played out.

16
00:00:56,777 --> 00:00:59,050
Let's replace the episode returns in

17
00:00:59,049 --> 00:01:03,324
the update rule with the action value of the current state action pair.

18
00:01:03,325 --> 00:01:08,859
Yes, the same action values we were trying to estimate in value-based methods.

19
00:01:08,859 --> 00:01:11,484
We've come a full circle, haven't we?

20
00:01:11,484 --> 00:01:14,635
Now where can we find these action values?

21
00:01:14,635 --> 00:01:18,835
Unless we have an oracle to tell us exactly what these values should be,

22
00:01:18,834 --> 00:01:20,640
we need to figure them out.

23
00:01:20,640 --> 00:01:22,265
For instance, we can use

24
00:01:22,265 --> 00:01:27,170
a temporal difference mechanism to iteratively update these action values.

25
00:01:27,170 --> 00:01:30,019
Note that this process can run in parallel with

26
00:01:30,019 --> 00:01:34,194
our policy updates and doesn't need an entire episode to be played out.

27
00:01:34,194 --> 00:01:39,215
For this reason, it can be used with non episodic or continuing tasks.

28
00:01:39,215 --> 00:01:43,159
In fact, you can pick any suitable representation to store

29
00:01:43,159 --> 00:01:47,239
these Q values and an appropriate algorithm to update them.

30
00:01:47,239 --> 00:01:51,634
Note that beta here is another learning rate or step size parameter,

31
00:01:51,635 --> 00:01:55,180
just like alpha, but for value updates.

