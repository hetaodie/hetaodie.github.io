1
00:00:00,000 --> 00:00:03,029
行动者-评论者方法是一种基于策略的方法变体

2
00:00:03,029 --> 00:00:07,022
这类方法重新利用了值函数这一概念

3
00:00:07,022 --> 00:00:11,789
注意 行动者表示策略 评论者表示值函数

4
00:00:11,789 --> 00:00:14,504
告诉行动者它的策略有多好

5
00:00:14,505 --> 00:00:18,570
因为我们可以单独选择行动者和评论者表示法

6
00:00:18,570 --> 00:00:23,385
我们忽然在设计强化学习智能体时有了更多的灵活性

7
00:00:23,385 --> 00:00:27,630
这样就开辟了一个广阔的研究领域

8
00:00:27,629 --> 00:00:32,119
研究者可以利用强化学习解决以前根本无法想象能够解决的问题

9
00:00:32,119 --> 00:00:35,844
DeepMind 推出了能击败人类职业围棋手的智能程序 并让世界为之震撼

10
00:00:35,844 --> 00:00:38,740
他们还通过强化学习使 Google 数据中心

11
00:00:38,740 --> 00:00:43,780
缩减了 40% 的冷却费用

12
00:00:43,780 --> 00:00:46,855
OpenAI 或许是 Gym 最有名的工具

13
00:00:46,854 --> 00:00:51,099
它是一款算法平台 研究人员可以用来发布学习算法

14
00:00:51,100 --> 00:00:56,328
并比较这些算法在不同的视频游戏和模拟物理问题上的效果

15
00:00:56,328 --> 00:01:01,210
他们还通过强化学习指导实际的机器人学习复杂的任务

16
00:01:01,210 --> 00:01:06,655
例如拾取和放置木块以及模仿人类

17
00:01:06,655 --> 00:01:10,349
无数其他团队都在不断突破

18
00:01:10,349 --> 00:01:14,233
强化学习的潜在应用领域

19
00:01:14,233 --> 00:01:16,919
你也可以加入这个社区

20
00:01:16,920 --> 00:01:20,250
但是注意 要解决下个大型挑战

21
00:01:20,250 --> 00:01:24,750
你最需要的是创新思维

22
00:01:24,750 --> 00:01:28,155
时刻关注该领域的最新进展

23
00:01:28,155 --> 00:01:29,966
阅读各种研究论文

24
00:01:29,966 --> 00:01:31,530
文章和博客

25
00:01:31,530 --> 00:01:35,325
尝试实现新的算法并进行试验

26
00:01:35,325 --> 00:01:41,290
我非常期待看到你能用强化学习解决各种问题

