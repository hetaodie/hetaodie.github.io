1
00:00:00,000 --> 00:00:05,669
我们尝试开发一个完整的架构来训练行动者和评论者组件

2
00:00:05,669 --> 00:00:08,314
思考下应该采用什么样的流程

3
00:00:08,314 --> 00:00:12,910
一开始 行动者遵守的是某个随机的策略并且行为很糟糕

4
00:00:12,910 --> 00:00:15,509
评论者观察该行为并提供反馈

5
00:00:15,509 --> 00:00:19,285
告诉行动者行为有多糟糕

6
00:00:19,285 --> 00:00:23,679
行动者根据该反馈更新策略 π 并再次行动

7
00:00:23,679 --> 00:00:27,175
评论者继续提供更多反馈

8
00:00:27,175 --> 00:00:29,695
评论者也会更新自己的备注或值函数

9
00:00:29,695 --> 00:00:33,869
以便提供更好的反馈

10
00:00:33,869 --> 00:00:36,209
这一流程一直持续下去

11
00:00:36,210 --> 00:00:38,774
直到行动者达到提前设定的性能阈值

12
00:00:38,774 --> 00:00:41,469
或者没有什么改进

13
00:00:41,469 --> 00:00:44,670
注意 与此同时 评论者也会一直通过观察

14
00:00:44,670 --> 00:00:47,039
从环境中获得的状态和奖励

15
00:00:47,039 --> 00:00:50,670
学习提供越来越好的反馈

16
00:00:50,670 --> 00:00:54,789
现在我们已经完整地了解了不同组件之间的交互关系

17
00:00:54,789 --> 00:00:58,424
我们重新介绍下每个组件背后的数学原理

18
00:00:58,424 --> 00:01:02,729
策略逼近器 π 由 θ 参数化

19
00:01:02,729 --> 00:01:08,664
值函数逼近器 q^ 由 w 参数化

20
00:01:08,665 --> 00:01:13,658
在每个时间步 t 我们都从环境中抽样当前状态并表示为 St

21
00:01:13,658 --> 00:01:16,905
策略用它作为输入 生成动作 At

22
00:01:16,905 --> 00:01:20,129
行动者在环境中采取此动作

23
00:01:20,129 --> 00:01:25,739
生成下个状态 St+1 以及奖励 Rt+1

24
00:01:25,739 --> 00:01:28,716
现在评论者使用值函数 q^

25
00:01:28,716 --> 00:01:33,424
计算在状态 St 采取动作 A 的值

26
00:01:33,424 --> 00:01:37,369
行动者使用这个 q 值更新策略参数 θ

27
00:01:37,370 --> 00:01:45,155
根据这些更新的参数 行动者生成下个动作 At+1

28
00:01:45,155 --> 00:01:49,400
最后 评论者更新自己的值函数

29
00:01:49,400 --> 00:01:52,528
在每个时间步都重复这一流程

30
00:01:52,528 --> 00:01:54,620
而不仅仅是在阶段结束时

31
00:01:54,620 --> 00:01:57,050
因此行动者和评论者都可以

32
00:01:57,049 --> 00:02:01,000
更有效地利用与环境的互动结果

