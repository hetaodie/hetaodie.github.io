1
00:00:00,000 --> 00:00:01,995
For most complex problems,

2
00:00:01,995 --> 00:00:06,120
you will need to deal with continuous state and action spaces.

3
00:00:06,120 --> 00:00:12,175
So, let's apply Q learning with function approximation to estimate the action values.

4
00:00:12,175 --> 00:00:15,200
And here's a corresponding update rule for

5
00:00:15,199 --> 00:00:18,849
the action value function expressed as a change in weights,

6
00:00:18,850 --> 00:00:23,960
delta w. Note that theta and w are different parameter vectors,

7
00:00:23,960 --> 00:00:28,019
but they are similar in the sense that they each approximate a function.

8
00:00:28,019 --> 00:00:30,289
Theta characterizes the policy pi,

9
00:00:30,289 --> 00:00:32,899
the probability of taking an action from a given state,

10
00:00:32,899 --> 00:00:38,814
and w encodes the value Q hat of taking that action from that state.

11
00:00:38,814 --> 00:00:43,004
Let's forget about the update rules and focus on the two functions.

12
00:00:43,005 --> 00:00:48,730
Pi controls how your reinforcement learning agent behaves or acts.

13
00:00:48,729 --> 00:00:52,894
Think of pi as a puppeteer controlling a puppet performing on stage.

14
00:00:52,895 --> 00:00:56,120
And Q hat measures how good those actions are,

15
00:00:56,119 --> 00:00:59,059
that is it critiques those actions.

16
00:00:59,060 --> 00:01:01,535
These are your two function approximators.

17
00:01:01,534 --> 00:01:05,239
Policy or actor, and value or critic.

18
00:01:05,239 --> 00:01:06,784
You can design them separately,

19
00:01:06,784 --> 00:01:08,704
perhaps using two neural nets,

20
00:01:08,704 --> 00:01:12,000
and you can train them using separate processes as well.

