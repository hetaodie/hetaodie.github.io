1
00:00:00,000 --> 00:00:05,799
我们仔细研究下我们的更新机制 看看是否有任何改进之处

2
00:00:05,799 --> 00:00:09,000
还记得这个策略更新规则来自何处吗？

3
00:00:09,000 --> 00:00:11,629
它基于策略梯度方法

4
00:00:11,630 --> 00:00:14,625
我们以很小的分数 α

5
00:00:14,625 --> 00:00:19,015
目标函数的梯度 jθ 更改策略参数

6
00:00:19,015 --> 00:00:23,010
这个梯度可以表示为策略乘以某个得分函数 R

7
00:00:23,010 --> 00:00:28,676
生成的对数概率的导数预期值

8
00:00:28,676 --> 00:00:30,449
在此处

9
00:00:30,449 --> 00:00:35,729
我们将动作值函数 q^ 作为得分函数

10
00:00:35,729 --> 00:00:39,769
这个方法可行的原因是我们可以采取很小的步长

11
00:00:39,770 --> 00:00:45,200
（由学习速率 α 定义）迭代地计算该预期值

12
00:00:45,200 --> 00:00:48,970
务必确保始终采取很小的步长

13
00:00:48,969 --> 00:00:52,304
我们依然希望沿着梯度所指的方向移动

14
00:00:52,304 --> 00:00:56,524
但是需要避免很大的步长 因为最终是抽样随机流程

15
00:00:56,524 --> 00:01:01,350
单个样本可能会变化很大

16
00:01:01,350 --> 00:01:03,984
只要是预期值

17
00:01:03,984 --> 00:01:06,084
就存在相关的方差

18
00:01:06,084 --> 00:01:09,024
如果我们尝试估算这个预期值

19
00:01:09,025 --> 00:01:12,430
则最好样本之间的方差很小

20
00:01:12,430 --> 00:01:15,885
这样使得该流程更加稳定

21
00:01:15,885 --> 00:01:20,770
该方程的大小主要受到得分函数 q^ 的影响

22
00:01:20,769 --> 00:01:22,450
在每个时间步

23
00:01:22,450 --> 00:01:28,275
q^ 的值可能变化很大 因为它基于单个奖励

24
00:01:28,275 --> 00:01:31,330
这样可能会导致策略更新步长不一

25
00:01:31,329 --> 00:01:32,774
有时候是很小的步长

26
00:01:32,775 --> 00:01:34,295
有时候是很大的步长

27
00:01:34,295 --> 00:01:38,790
如何减小这一方差？

28
00:01:38,790 --> 00:01:43,310
假设 q 值抽样自某个分布

29
00:01:43,310 --> 00:01:48,200
假设这个分布是正态分布或高斯分布

30
00:01:48,200 --> 00:01:51,394
能算出该分布的均值是什么吗？

31
00:01:51,394 --> 00:01:55,414
均值是 q^ 的预期值

32
00:01:55,415 --> 00:01:57,332
对于特定的状态 s

33
00:01:57,331 --> 00:02:00,929
这个分布针对的是动作空间

34
00:02:00,930 --> 00:02:05,615
因此预期值本质上等于状态值

35
00:02:05,614 --> 00:02:11,210
如果用每个 Q 值减去这个均值并用作新的得分函数

36
00:02:11,210 --> 00:02:13,730
这样会使得均值得分值降低为 0

37
00:02:13,729 --> 00:02:18,204
有助于减小更新步长的偏差

38
00:02:18,205 --> 00:02:22,070
这个新得分函数称之为优势函数

39
00:02:22,069 --> 00:02:24,294
可以直观地看出它很合理

40
00:02:24,294 --> 00:02:27,139
Q 值告诉我们在状态 s 采取动作 a

41
00:02:27,139 --> 00:02:30,619
预期可以获得的奖励

42
00:02:30,620 --> 00:02:33,590
优势函数告诉我们在状态的预期值之外

43
00:02:33,590 --> 00:02:37,370
我们预期会获得多少额外的奖励

44
00:02:37,370 --> 00:02:43,384
即我们采取该动作（而不是任何随机的动作）会获得什么

45
00:02:43,384 --> 00:02:47,554
优势函数不仅会稳定学习过程

46
00:02:47,555 --> 00:02:51,230
而且可以更好地区分动作

