1
00:00:02,240 --> 00:00:08,504
我们可以通过两大类别的方法解决强化学习问题

2
00:00:08,505 --> 00:00:12,540
对于蒙特卡罗学习或 Q 学习等基于值的方法

3
00:00:12,539 --> 00:00:16,799
我们会尝试表示每个状态或状态动作对的值

4
00:00:16,800 --> 00:00:21,359
然后 根据任何状态 我们可以选择具有最佳值的动作

5
00:00:21,359 --> 00:00:24,809
如果你有数量有限的动作 这种方法很合适

6
00:00:24,809 --> 00:00:27,195
另一方面 基于策略的方法

7
00:00:27,195 --> 00:00:31,170
会对从状态到动作的映射进行编码

8
00:00:31,170 --> 00:00:35,910
不用担心值表示法 然后尝试直接优化策略

9
00:00:35,909 --> 00:00:40,169
当动作空间是连续性或者需要随机策略时

10
00:00:40,170 --> 00:00:42,855
这种方法非常有用

11
00:00:42,854 --> 00:00:46,019
基于策略的方法存在的最大挑战是

12
00:00:46,020 --> 00:00:49,545
很难计算策略到底有多好

13
00:00:49,545 --> 00:00:54,405
这时候我们就要用到值函数这一概念

14
00:00:54,405 --> 00:00:59,054
如果我们跟踪状态或状态动作值

15
00:00:59,054 --> 00:01:00,509
并使用这些数据计算目标

16
00:01:00,509 --> 00:01:05,295
而不是根据奖励或回报计算策略目标 会怎样

17
00:01:05,295 --> 00:01:09,700
这正是行动者-评论者方法的作用

