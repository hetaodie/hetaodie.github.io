1
00:00:00,000 --> 00:00:01,995
对于大多数复杂的问题

2
00:00:01,995 --> 00:00:06,120
你都需要处理连续状态和动作空间

3
00:00:06,120 --> 00:00:12,175
我们通过 Q 学习和函数逼近来估算动作值

4
00:00:12,175 --> 00:00:15,200
这是一个动作值函数的相应更新规则

5
00:00:15,199 --> 00:00:18,849
表示为权重变化 Δw

6
00:00:18,850 --> 00:00:23,960
注意 θ 和 w 是不同的参数向量

7
00:00:23,960 --> 00:00:28,019
但是它们很相似 分别表示一种函数逼近

8
00:00:28,019 --> 00:00:30,289
θ 表示策略 π

9
00:00:30,289 --> 00:00:32,899
在给定状态采取某个动作的概率

10
00:00:32,899 --> 00:00:38,814
w 表示从该状态采取该动作的值 q^

11
00:00:38,814 --> 00:00:43,004
我们先不管更新规则并重点看看这两个函数

12
00:00:43,005 --> 00:00:48,730
π 控制强化学习智能体的行为或动作方式

13
00:00:48,729 --> 00:00:52,894
可以将 π 看做在舞台上控制木偶表演的木偶师

14
00:00:52,895 --> 00:00:56,120
q^ 衡量的是这些动作的效果

15
00:00:56,119 --> 00:00:59,059
即对这些动作进行评价

16
00:00:59,060 --> 00:01:01,535
这是两个函数逼近器

17
00:01:01,534 --> 00:01:05,239
策略或行动者以及值或评论者

18
00:01:05,239 --> 00:01:06,784
你可以单独设计它们

19
00:01:06,784 --> 00:01:08,704
或许可以使用两个神经网络

20
00:01:08,704 --> 00:01:12,000
并且可以使用单独的流程训练它们

