1
00:00:00,000 --> 00:00:02,069
虽然我们学习了绝对值技巧

2
00:00:02,069 --> 00:00:04,544
和平方技巧 以及如何在线性回归中使用

3
00:00:04,544 --> 00:00:08,789
我们还希望了解这些如何计算出来的

4
00:00:08,789 --> 00:00:10,799
这些技巧看似非常神秘

5
00:00:10,800 --> 00:00:12,165
所以我们希望寻根溯源

6
00:00:12,164 --> 00:00:14,739
那么我们要以更加正式的方式进行

7
00:00:14,740 --> 00:00:16,170
例如我们有几个点

8
00:00:16,170 --> 00:00:18,330
想要开发个算法

9
00:00:18,329 --> 00:00:21,309
能够找到拟合这些点的直线

10
00:00:21,309 --> 00:00:23,414
这个算法具体如下

11
00:00:23,414 --> 00:00:27,280
首先绘制一条随机直线 可以计算误差

12
00:00:27,280 --> 00:00:32,579
这个误差为点到直线的距离值

13
00:00:32,579 --> 00:00:35,579
在这个图中 误差为这些距离的总和

14
00:00:35,579 --> 00:00:39,329
但是它也可以是一种测度 告诉我们离这些点有多远

15
00:00:39,329 --> 00:00:43,225
我们将要移动这个直线 观察一下是否可以降低误差

16
00:00:43,225 --> 00:00:47,484
我们向这个方向移动 看到误差增加

17
00:00:47,484 --> 00:00:49,344
所以不应该这样移动

18
00:00:49,344 --> 00:00:53,689
我们向另一个方向移动 看到误差减少

19
00:00:53,689 --> 00:00:56,189
所以应该使用这个 并保持不动

20
00:00:56,189 --> 00:00:59,588
现在我们不断重复这个过程

21
00:00:59,588 --> 00:01:01,579
每次都将误差减少一点

22
00:01:01,579 --> 00:01:03,429
直到得到最合适的直线

23
00:01:03,429 --> 00:01:04,829
为了减少误差

24
00:01:04,829 --> 00:01:07,254
我们将使用梯度下降法

25
00:01:07,254 --> 00:01:09,069
所以我们来了解一下梯度下降法

26
00:01:09,069 --> 00:01:12,719
这种方法相当于我们站在山顶 这叫做误差山 (Mount Rainierror)

27
00:01:12,719 --> 00:01:15,704
可以测算误差的大小

28
00:01:15,704 --> 00:01:18,149
我们想从山顶下来

29
00:01:18,150 --> 00:01:19,926
为了从山顶下来

30
00:01:19,926 --> 00:01:22,040
我们需要将高度减少至最小

31
00:01:22,040 --> 00:01:25,950
在左边 我们面临直线拟合数据的问题

32
00:01:25,950 --> 00:01:31,469
通过缩小误差 或者缩小点到直线的距离

33
00:01:31,469 --> 00:01:35,734
所以下山相当于让直线更靠近这些点

34
00:01:35,734 --> 00:01:37,560
现在如果你想下山

35
00:01:37,560 --> 00:01:39,570
我们要观察往哪个方向走

36
00:01:39,569 --> 00:01:42,599
然后找到让我们下降幅度最大的方向

37
00:01:42,599 --> 00:01:44,704
例如这个方向

38
00:01:44,704 --> 00:01:47,250
所以我们沿着这个方向下山

39
00:01:47,250 --> 00:01:50,900
这相当于让直线一点点逼近这些点

40
00:01:50,900 --> 00:01:53,250
我们的高度越小 因为我们越靠近

41
00:01:53,250 --> 00:01:56,474
这些点 到这些点的距离就越小

42
00:01:56,474 --> 00:02:00,069
不断重复 我们观察如何让它下降幅度最大

43
00:02:00,069 --> 00:02:01,619
可以说我们到这儿

44
00:02:01,620 --> 00:02:04,439
现在我们到达下山的地点

45
00:02:04,439 --> 00:02:08,734
在右边 我们看到直线距离这些点非常接近

46
00:02:08,735 --> 00:02:10,480
这样我们解决了问题

47
00:02:10,479 --> 00:02:12,104
这就是梯度下降法

48
00:02:12,104 --> 00:02:13,669
从数学角度来说

49
00:02:13,669 --> 00:02:14,909
具体过程如下

50
00:02:14,909 --> 00:02:17,479
我们有个绘图 这个图有两个维度

51
00:02:17,479 --> 00:02:20,564
虽然在实际过程中 绘图可能会包含多个维度

52
00:02:20,564 --> 00:02:22,027
权重位于 x 轴

53
00:02:22,027 --> 00:02:23,969
误差位于 y 轴

54
00:02:23,969 --> 00:02:27,544
并且还有一个这样的误差函数

55
00:02:27,544 --> 00:02:30,509
我们站在此处 下降方法实际上

56
00:02:30,509 --> 00:02:34,679
针对权重的导数或误差梯度函数

57
00:02:34,680 --> 00:02:38,649
梯度指出了函数增加至最大时的方向

58
00:02:38,649 --> 00:02:41,009
所以梯度为负值 表明

59
00:02:41,009 --> 00:02:44,340
方向朝下时 函数减少到最小

60
00:02:44,340 --> 00:02:49,314
所以我们要向梯度负值方向迈出一步

61
00:02:49,314 --> 00:02:51,254
这表明我们对权值 W i

62
00:02:51,254 --> 00:02:56,469
改为 W i 减去 W i 的误差导数

63
00:02:56,469 --> 00:02:59,064
在实际生活中 我们会用这个导数

64
00:02:59,064 --> 00:03:02,270
乘以学习率 因为我们只想迈出一小步

65
00:03:02,270 --> 00:03:04,094
这表明误差函数正在下降

66
00:03:04,094 --> 00:03:05,995
并且我们更加接近最小值

67
00:03:05,995 --> 00:03:07,740
如果反复进行几次

68
00:03:07,740 --> 00:03:11,680
我们将会得到最小值 或者误差极小的较优值

69
00:03:11,680 --> 00:03:12,885
我们一旦到达这个点

70
00:03:12,884 --> 00:03:16,229
就能得到线性回归问题的较优解

71
00:03:16,229 --> 00:03:18,000
这就是梯度下降法的内容

