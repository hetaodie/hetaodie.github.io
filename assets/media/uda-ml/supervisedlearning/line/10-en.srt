1
00:00:00,000 --> 00:00:04,309
So far, we've learned two algorithms that will fit a line through a set of points.

2
00:00:04,309 --> 00:00:06,179
One is using any of the tricks,

3
00:00:06,179 --> 00:00:08,379
namely the absolute and the square trick,

4
00:00:08,380 --> 00:00:11,560
and the other one is minimizing any of the error functions,

5
00:00:11,560 --> 00:00:14,425
namely the mean absolute error and the mean squared error.

6
00:00:14,425 --> 00:00:17,839
The interesting thing is that these two are actually the exact same thing.

7
00:00:17,839 --> 00:00:20,504
What I'm saying is that when we minimize the mean absolute error,

8
00:00:20,504 --> 00:00:22,369
we're using a gradient descent step,

9
00:00:22,370 --> 00:00:26,789
and that gradient descent step is the exact same thing as the absolute trick.

10
00:00:26,789 --> 00:00:28,710
Likewise, when we minimize the square error,

11
00:00:28,710 --> 00:00:32,579
the gradient descent step is the exact same thing as a square trick.

12
00:00:32,579 --> 00:00:35,954
So, let's see why. Let's start with the mean squared error.

13
00:00:35,954 --> 00:00:37,879
Here's a point with coordinates x, y,

14
00:00:37,880 --> 00:00:43,295
and here's our line with equation y-hat equals w_1_x times x, plus w2.

15
00:00:43,295 --> 00:00:47,460
So, y-hat is our prediction and the line predicts the Y coordinate at this point.

16
00:00:47,460 --> 00:00:49,410
That prediction gives us a point of the line that

17
00:00:49,409 --> 00:00:52,259
matches the X coordinate of a point (x, y).

18
00:00:52,259 --> 00:00:54,210
So, it's the point (x, y-hat).

19
00:00:54,210 --> 00:00:57,899
Now the error for this point is one-half times (y-y-hat)Â².

20
00:00:57,899 --> 00:01:04,664
And the mean squared error for this set of points is the average of all these errors.

21
00:01:04,665 --> 00:01:06,295
But since average is a linear function,

22
00:01:06,295 --> 00:01:09,415
then whatever we do here applies to the entire error.

23
00:01:09,415 --> 00:01:11,910
Now, we know that the gradient descent step uses

24
00:01:11,909 --> 00:01:15,989
these two derivatives namely the derivative with respect to the slope w1,

25
00:01:15,989 --> 00:01:19,609
and the derivative with respect to the Y intercept w2.

26
00:01:19,609 --> 00:01:21,159
If we calculate these derivatives,

27
00:01:21,159 --> 00:01:24,149
and you can see the calculation in detail in the instructor notes,

28
00:01:24,150 --> 00:01:29,430
we get negative times (y-y-hat) times x for the one with respect to slope

29
00:01:29,430 --> 00:01:35,905
and negative times (y-y-hat) for the one with respect to the Y intercept w2.

30
00:01:35,905 --> 00:01:40,890
I noticed that the length of this red segment is precisely (y-y-hat),

31
00:01:40,890 --> 00:01:45,109
and the length of this green segment is precisely x.

32
00:01:45,109 --> 00:01:46,418
And if you remember correctly,

33
00:01:46,418 --> 00:01:52,290
the square trick told us that we have to upgrade the slope by adding (y-y-hat) times x,

34
00:01:52,290 --> 00:01:53,935
times the learning rate Alpha,

35
00:01:53,935 --> 00:01:57,359
and upgrade the Y intercept by adding (y-y-hat),

36
00:01:57,359 --> 00:01:59,429
times the learning rate Alpha.

37
00:01:59,430 --> 00:02:03,370
But that is precisely what this gradient descent step is doing.

38
00:02:03,370 --> 00:02:06,990
If you like, feel free to pause this video or actually write it down in

39
00:02:06,989 --> 00:02:12,180
a little piece of paper to verify that is the exact same calculation.

40
00:02:12,180 --> 00:02:14,580
So, this is why the gradient descent step utilize when we

41
00:02:14,580 --> 00:02:18,315
minimize the mean squared error is the same as the square trick.

42
00:02:18,314 --> 00:02:21,729
We can do the same thing with the absolute trick.

43
00:02:21,729 --> 00:02:23,129
The procedure is very similar,

44
00:02:23,129 --> 00:02:26,009
except we have to be careful about the sign.

45
00:02:26,009 --> 00:02:30,484
This is our error, the absolute value of (y-y-hat).

46
00:02:30,485 --> 00:02:33,990
And the derivatives of the error with respect to w1 one and w2,

47
00:02:33,990 --> 00:02:36,750
are plus or minus x and plus or minus 1,

48
00:02:36,750 --> 00:02:39,435
based on the point is on top or underneath the line.

49
00:02:39,435 --> 00:02:41,412
Since this distance is x,

50
00:02:41,412 --> 00:02:43,439
then you can also check that this is precisely what

51
00:02:43,439 --> 00:02:47,609
the gradient descent step does when we minimize the mean absolute error.

52
00:02:47,610 --> 00:02:50,630
So, that's it. That's why I'm minimizing these errors with gradient descent,

53
00:02:50,629 --> 00:02:54,329
is the exact same thing by using the absolute and the square trick.

