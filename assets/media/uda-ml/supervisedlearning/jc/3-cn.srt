1
00:00:00,000 --> 00:00:01,649
跟我们之前看的一样

2
00:00:01,649 --> 00:00:02,924
boosting 更加复杂

3
00:00:02,924 --> 00:00:08,039
有多种方法可以实现 boosting 但最普遍的一种算法是 AdaBoost（自适应增强算法）

4
00:00:08,039 --> 00:00:11,189
是 1996 年由 Freund 和 Schapire 提出的

5
00:00:11,189 --> 00:00:12,824
稍后我们会详细地探讨它的数学原理

6
00:00:12,824 --> 00:00:15,989
但这个算法的要旨就是

7
00:00:15,990 --> 00:00:18,839
首先要说明 如果你查阅文献资料

8
00:00:18,839 --> 00:00:20,445
文献中的表述可能会不一样

9
00:00:20,445 --> 00:00:23,100
但我保证 除了一些细枝末节的东西

10
00:00:23,100 --> 00:00:26,640
例如要用所有的权重系数乘以常量 这不会改变最终结果

11
00:00:26,640 --> 00:00:29,789
我将完整地介绍 AdaBoost 算法

12
00:00:29,789 --> 00:00:32,850
我们拟合第一个学习器 以最大程度上保证准确率

13
00:00:32,850 --> 00:00:36,750
或者最小程度上减少错误数量

14
00:00:36,750 --> 00:00:40,679
它的性能不是那么好 但它犯的错误数量在三个以内

15
00:00:40,679 --> 00:00:42,914
所以 我们将它拟合 得到需要的模型

16
00:00:42,914 --> 00:00:44,850
记住这个模型 稍后会用到

17
00:00:44,850 --> 00:00:49,020
第二个学习器需要修正第一个学习器所犯的错误

18
00:00:49,020 --> 00:00:53,880
所以我们要做的就是 筛选出那些错误分类的点 并把它们放大

19
00:00:53,880 --> 00:00:58,365
换句话说 如果该模型没有筛选出这些点 我们就要惩罚它

20
00:00:58,365 --> 00:01:01,335
让下一个弱学习器要更关注这些

21
00:01:01,335 --> 00:01:05,670
这是第二个弱学习器 它能够正确的把这些点分类

22
00:01:05,670 --> 00:01:07,685
记住这个形式 稍后会用到

23
00:01:07,685 --> 00:01:10,480
同样 我们要选出那些错误分类的点

24
00:01:10,480 --> 00:01:13,525
把这些点放大

25
00:01:13,525 --> 00:01:17,080
这是第三个弱学习器 它要努力修正这些错误

26
00:01:17,079 --> 00:01:20,920
将这些放大的点正确分类 同样要记住这个模型

27
00:01:20,920 --> 00:01:24,114
这个流程可能要继续下去 但我们假设三步已经足够了

28
00:01:24,114 --> 00:01:26,649
现在要做的就是把这些模型集成在一起

29
00:01:26,650 --> 00:01:29,140
我之后会更加具体的讲解这个集成的过程

30
00:01:29,140 --> 00:01:32,590
但现在 我们先假设像之前一样处理

31
00:01:32,590 --> 00:01:34,780
这就是我们的结果模型

32
00:01:34,780 --> 00:01:36,280
当我们用这个模型处理数据的时候

33
00:01:36,280 --> 00:01:38,515
发现效果非常好

34
00:01:38,515 --> 00:01:41,379
我之前也说过 细节方面不会讲解过深

35
00:01:41,379 --> 00:01:44,329
我在之后的视频中会进一步解释

