1
00:00:00,000 --> 00:00:01,526
我们先来看一下 bagging

2
00:00:01,526 --> 00:00:04,500
现在有一些数据 就是图中这些红点和蓝点

3
00:00:04,500 --> 00:00:06,450
简单起见 我们假设

4
00:00:06,450 --> 00:00:09,285
我们要用的弱学习器就是最简单的学习器

5
00:00:09,285 --> 00:00:11,625
单结点的决策树

6
00:00:11,625 --> 00:00:15,460
也就是说 要么在水平方向上 要么在垂直方向上

7
00:00:15,460 --> 00:00:16,920
一侧都是正例样本

8
00:00:16,920 --> 00:00:19,040
而另一侧都是反例样本

9
00:00:19,039 --> 00:00:21,359
所以 对这些数据 我们做如下处理

10
00:00:21,359 --> 00:00:23,910
由于数据可能过于庞大

11
00:00:23,910 --> 00:00:27,707
一般来说 我们不想针对同样的数据采用多种模型

12
00:00:27,707 --> 00:00:29,459
这样做代价太大

13
00:00:29,460 --> 00:00:32,414
相反 我们选取部分子集

14
00:00:32,414 --> 00:00:35,850
每个子集使用一种弱学习器

15
00:00:35,850 --> 00:00:38,969
这样我们就知道怎么把这些学习器集合到一起

16
00:00:38,969 --> 00:00:41,420
这是第一个子集

17
00:00:41,420 --> 00:00:44,325
还有我们的第一个模型 也就是第一个学习器

18
00:00:44,325 --> 00:00:46,095
记住这个模式

19
00:00:46,094 --> 00:00:48,713
这是第二个子集

20
00:00:48,713 --> 00:00:49,980
和第二个学习器

21
00:00:49,979 --> 00:00:53,879
如果你觉得我这么做看起来很简单 那么确实

22
00:00:53,880 --> 00:00:55,868
总体来说 这些学习器性能都不好

23
00:00:55,868 --> 00:00:57,307
但是 如果我们的数据足够多

24
00:00:57,307 --> 00:01:00,990
随机选择一个子集通常就能让我们了解一定情况

25
00:01:00,990 --> 00:01:03,085
而且能让整个流程进展更快

26
00:01:03,085 --> 00:01:05,165
这是我们的第三个子集

27
00:01:05,165 --> 00:01:06,800
以及第三个学习器

28
00:01:06,799 --> 00:01:09,280
注意 这些数据 我从未做过分区处理

29
00:01:09,280 --> 00:01:12,942
我们完全可以在子集中复制某些点

30
00:01:12,942 --> 00:01:15,759
甚至是忽略某些点

31
00:01:15,760 --> 00:01:19,100
每一步 我们都随机选择一个子集

32
00:01:19,099 --> 00:01:21,409
现在 共有三个弱学习器

33
00:01:21,409 --> 00:01:24,679
我们怎么把它们集合到一起？ 不如通过计票

34
00:01:24,680 --> 00:01:27,350
我们把这些学习器和点重叠在一起

35
00:01:27,349 --> 00:01:29,344
如果数量更多的是蓝点 那结果就是蓝点

36
00:01:29,344 --> 00:01:33,064
如果数量更多的是红点 那结果就是红点

37
00:01:33,064 --> 00:01:34,759
如果数量持平

38
00:01:34,760 --> 00:01:37,010
那么任意选择其中一个即可

39
00:01:37,010 --> 00:01:39,310
尽管在有很多点和模型的时候

40
00:01:39,310 --> 00:01:42,450
很难出现数量持平的情况 但这种情况也会发生

41
00:01:42,450 --> 00:01:46,170
看 采用计票方式我们就得到了这样的结果

42
00:01:46,170 --> 00:01:49,000
这就是 bagging 算法

