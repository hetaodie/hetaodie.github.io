---



layout: post
title: 机器学习-强化学习-时间差分方法
description: 在这章，主要讲解了监督学习相关的技术。
Keywords: 机器学习、模型、评估指标
tagline: 
categories: [ML]
tags: [ML]

---



* 目录
 {:toc  }
# 


## 1、简介

<video controls="" preload="none" style="width:100%; height:100%; object-fit: fill"   src="../assets/media/uda-ml/qinghua/shijianchafenfangfa/1-t.mp4"></video>
## 2、迷你项目：时间差分方法 (OpenAI Gym-CliffWalkingEnv）



# OpenAI Gym：CliffWalkingEnv



在这节课，你将为我们讨论的算法编写自己的 Python 实现。虽然你的算法将需要适合任何 OpenAI Gym 环境，但是你将使用 CliffWalking 环境测试你的代码。



![img](../assets/media/uda-ml/supervisedlearning/2-i1.png)‘来源：维基百科'



在 CliffWalking 环境中，智能体会浏览一个 4x12 网格世界。请在该[教科书](http://go.udacity.com/rl-textbook)的示例 6.6 中详细了解悬崖行走任务。阅读完毕后，你可以打开相应的 [GitHub 文件](https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py)并阅读 CliffWalkingEnv 类中的注释部分，详细了解该环境。为了进行说明，我们还在下方贴出了该环境的说明（注意下面指向 Sutton 和 Barto 教科书的链接也许不能打开，建议你使用[此链接](http://go.udacity.com/rl-textbook)来访问该教科书）：

```text
    """
    这是 Gridworld Cliff 强化学习任务的简单实现。
    改编自《强化学习：简介》（作者：Sutton 和 Barto）的示例 6.6（第 145 页）
    http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf

    灵感来自：
    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py
    该板是一个 4x12 的矩阵（使用 Numpy 矩阵索引）：
        左下角的 [3, 0] 是起点
        右下角的 [3, 11] 是目标
        底部中心的 [3, 1..10] 是悬崖
    每个时间步都会获得 -1 奖励，跌入悬崖会获得 -100 奖励并重置为起点。当智能体抵达目标时阶段结束。
    """
```



## 3、TD 预测：TD(0)

<video controls="" preload="none" style="width:100%; height:100%; object-fit: fill"   src="../assets/media/uda-ml/qinghua/shijianchafenfangfa/2-t.mp4"></video>
## 4、实现



你可以在下方找到 TD(0)（或一步 TD）的伪代码。



![img](../assets/media/uda-ml/supervisedlearning/4-i1.png)



TD(0) **保证会收敛于**真状态值函数，只要步长参数 \alpha*α* 足够小。常量 \alpha*α* MC 预测也是这种情况，如果你还记得的话。但是，TD(0) 具有一些优势：

- 虽然 MC 预测必须等到阶段结束时才能更新值函数估值，但是， TD 预测方法在每个时间步之后都会更新值函数。同样，TD 预测方法适合连续性和阶段性任务，而 MC 预测只能应用于阶段性任务。
- 在实践中，TD 预测的收敛速度比 MC 预测的快。（*但是，没有人能够证明这一点，依然是一个需要验证的问题。*）你可以花时间在自己的实现中检查这一点！要获取了解如何运行此类分析的示例，请参阅该[教科书](http://go.udacity.com/rl-textbook)的第 6.2 个示例。

请在下个部分完成 `Temporal_Difference.ipynb` 的**第 0 部分：探索 CliffWalkingEnv** 和**第 1 部分：TD 预测：状态值**。请记得保存内容！

你可以查看 `Temporal_Difference_Solution.ipynb` 的相应部分，检查你的解决方案是否正确。



## 5、迷你项目：时间差分方法（第 0 部分和第 1 部分）



## 6、TD 预测：动作值


<video controls="" preload="none" style="width:100%; height:100%; object-fit: fill"   src="../assets/media/uda-ml/qinghua/shijianchafenfangfa/3-t.mp4"></video>
## 7、TD 控制：Sarsa(0)

<video controls="" preload="none" style="width:100%; height:100%; object-fit: fill"   src="../assets/media/uda-ml/qinghua/shijianchafenfangfa/4-t.mp4"></video>
## 8、实现



你可以在下方找到 Sarsa（或 Sarsa(0)）的伪代码。



![img](../assets/media/uda-ml/supervisedlearning/8-i1.png)



Sarsa(0) **保证会收敛于**最优动作值函数，只要步长参数 \alpha*α* 足够小，并且满足**有限状态下的无限探索贪婪算法 (GLIE)** 条件。我们在上一节课介绍了 GLIE 条件，并且讲解了 MC 控制。虽然有满足 GLIE 条件的很多方式，但是有一种方式在构建 \epsilon*ϵ* 贪婪策略时会逐渐降低 \epsilon*ϵ* 的值。

尤其是，使 \epsilon_i*ϵ**i* 对应于第 i*i* 个时间步。然后，如果我们按照以下条件设置 \epsilon_i*ϵ**i*：

- 对于所有时间步 i*i*，\epsilon_i > 0*ϵ**i*>0，以及
- 当时间步 i*i* 接近无穷大时，\epsilon_i*ϵ**i* 降低到 0（即 \lim_{i\to\infty} \epsilon_i = 0lim*i*→∞*ϵ**i*=0），

然后该算法会保证产生一个很好的 q_**q*∗ 估值，只要我们运行该算法足够长的时间。然后，可以通过对所有 s\in\mathcal{S}*s*∈S 设置 \pi_*(s) = \arg\max_{a\in\mathcal{A}(s)} q_*(s, a)*π*∗(*s*)=argmax*a*∈A(*s*)*q*∗(*s*,*a*)，获得相应的最优策略 \pi_**π*∗。

请在下个部分完成 `Temporal_Difference.ipynb` 的**第 2 部分：TD 控制：Sarsa**。请记得保存内容！

你可以查看 `Temporal_Difference_Solution.ipynb` 的相应部分，检查你的解决方案是否正确。



## 9、你可以在下方找到 Sarsa（或 Sarsa(0)）的伪代码。



[![img](https://video.udacity-data.com/topher/2017/October/59dfd2f8_sarsa/sarsa.png)](https://classroom.udacity.com/nanodegrees/nd009-cn-advanced/parts/a2386085-8101-47b6-84e0-7b61a76c2b82/modules/dffda80a-0d5b-460d-afbc-3e0ce20e867f/lessons/d2de57a0-cd89-40bd-b87f-ec0298b425cf/concepts/f27a2426-3620-43a2-8284-f3bc476634e0#)



Sarsa(0) **保证会收敛于**最优动作值函数，只要步长参数 \alpha*α* 足够小，并且满足**有限状态下的无限探索贪婪算法 (GLIE)** 条件。我们在上一节课介绍了 GLIE 条件，并且讲解了 MC 控制。虽然有满足 GLIE 条件的很多方式，但是有一种方式在构建 \epsilon*ϵ* 贪婪策略时会逐渐降低 \epsilon*ϵ* 的值。

尤其是，使 \epsilon_i*ϵ**i* 对应于第 i*i* 个时间步。然后，如果我们按照以下条件设置 \epsilon_i*ϵ**i*：

- 对于所有时间步 i*i*，\epsilon_i > 0*ϵ**i*>0，以及
- 当时间步 i*i* 接近无穷大时，\epsilon_i*ϵ**i* 降低到 0（即 \lim_{i\to\infty} \epsilon_i = 0lim*i*→∞*ϵ**i*=0），

然后该算法会保证产生一个很好的 q_**q*∗ 估值，只要我们运行该算法足够长的时间。然后，可以通过对所有 s\in\mathcal{S}*s*∈S 设置 \pi_*(s) = \arg\max_{a\in\mathcal{A}(s)} q_*(s, a)*π*∗(*s*)=argmax*a*∈A(*s*)*q*∗(*s*,*a*)，获得相应的最优策略 \pi_**π*∗。

请在下个部分完成 `Temporal_Difference.ipynb` 的**第 2 部分：TD 控制：Sarsa**。请记得保存内容！

你可以查看 `Temporal_Difference_Solution.ipynb` 的相应部分，检查你的解决方案是否正确。



## 9、迷你项目：时间差分方法（第 2 部分）



## 10、TD 控制：Sarsamax


<video controls="" preload="none" style="width:100%; height:100%; object-fit: fill"   src="../assets/media/uda-ml/qinghua/shijianchafenfangfa/5-t.mp4"></video>
## 11、实现



# 实现：Sarsamax



你可以在下方找到 Sarsamax（或 *Q* 学习）的伪代码。



![img](../assets/media/uda-ml/supervisedlearning/11-i1.png)



Sarsamax 会在保证 Sarsa 会收敛的同一条件下**保证收敛**。

请在下个部分完成 `Temporal_Difference.ipynb` 的**第 3 部分：TD 控制：Q 学习**。请记得保存内容！

你可以查看 `Temporal_Difference_Solution.ipynb` 的相应部分，检查你的解决方案是否正确。



## 13、TD 控制：预期 Sarsa



<video controls="" preload="none" style="width:100%; height:100%; object-fit: fill"   src="../assets/media/uda-ml/qinghua/shijianchafenfangfa/6-t.mp4"></video>





## 14、实现



你可以在下方找到预期 Sarsa 的伪代码。



![img](../assets/media/uda-ml/supervisedlearning/14-i1.png)



预期 Sarsa 会在保证 Sarsa 和 Sarsamax 会收敛的同一条件下**保证收敛**。

注意，***从理论上讲\***，只要步长参数 \alpha*α* 足够小，并且满足**有限状态下的无限探索贪婪算法 (GLIE)** 条件，智能体保证最终会发现最优动作值函数（以及相关的最优策略）。但是，***在实践中\***，对于我们所讨论的所有算法，通常都会完全忽略这些条件并依然能够发现最优策略。你可以在解决方案notebook中找到此类示例。

请在下个部分完成 `Temporal_Difference.ipynb` 的**第 4 部分：TD 控制：预期 Sarsa**。请记得保存内容！

你可以查看 `Temporal_Difference_Solution.ipynb` 的相应部分，检查你的解决方案是否正确。





## 16、分析性能



在以下情况下，我们讨论过的所有 TD 控制算法（Sarsa、Sarsamax、预期 Sarsa）都会收敛于最优动作值函数 q_**q*∗（并生成最优策略 \pi_**π*∗）：(1)\epsilon*ϵ* 的值根据 GLIE 条件逐渐降低，以及 (2) 步长参数 \alpha*α* 足够小。

这些算法之间的区别总结如下：

- Sarsa 和预期 Sarsa 都是**异同策略** TD 控制算法。在这种情况下，我们会根据要评估和改进的相同（\epsilon*ϵ* 贪婪策略）策略选择动作。
- Sarsamax 是**离线策略**方法，我们会评估和改进（\epsilon*ϵ* 贪婪）策略，并根据另一个策略选择动作。
- 既定策略 TD 控制方法（例如预期 Sarsa 和 Sarsa）的在线效果比新策略 TD 控制方法（例如 Sarsamax）的要好。
- 预期 Sarsa 通常效果比 Sarsa 的要好。

如果你要了解详情，建议阅读该[教科书](http://go.udacity.com/rl-textbook)（尤其是第 6.4-6.6 部分）的第 6 章节。

为了加深理解，你可以选择练习重现图 6.4（注意，这道练习是可选练习！）



![img](../assets/media/uda-ml/supervisedlearning/16-i1.png)



该图显示了 Sarsa 和 Q 学习在悬崖行走环境中的效果，常量 \epsilon = 0.1*ϵ*=0.1。正如在教科书中所描述的，在这种情况下，

- Q 学习的在线效果更差（智能体在每个阶段平均收集的奖励更少），但是能够学习最优策略，以及
- Sarsa 可以获得更好的在线效果，但是学到的是次最优“安全”策略。

你应该通过对现有代码稍加修改，就能够重现该图。





## 17、总结



![img](../assets/media/uda-ml/supervisedlearning/17-i1.png)‘悬崖行走任务（Sutton 和 Barto，2017 年）'](https://classroom.udacity.com/nanodegrees/nd009-cn-advanced/parts/a2386085-8101-47b6-84e0-7b61a76c2b82/modules/dffda80a-0d5b-460d-afbc-3e0ce20e867f/lessons/d2de57a0-cd89-40bd-b87f-ec0298b425cf/concepts/7d2dafe6-e522-4a8d-beb0-e9dd6eadddfc#)



## TD 预测：TD(0)

- 虽然蒙特卡洛 (MC) 预测方法必须等到阶段结束时才能更新值函数估值，但是时间差分 (TD) 方法在每个时间步之后都会更新值函数。
- 对于任何固定策略，**一步 TD**（或 **TD(0)**）保证会收敛于真状态值函数，只要步长参数 \alpha*α* 足够小。
- 在实践中，TD 预测的收敛速度比 MC 预测得要快。



![img](../assets/media/uda-ml/supervisedlearning/17-i2.png)](https://classroom.udacity.com/nanodegrees/nd009-cn-advanced/parts/a2386085-8101-47b6-84e0-7b61a76c2b82/modules/dffda80a-0d5b-460d-afbc-3e0ce20e867f/lessons/d2de57a0-cd89-40bd-b87f-ec0298b425cf/concepts/7d2dafe6-e522-4a8d-beb0-e9dd6eadddfc#)



## TD 预测：动作值

-（在此部分，我们讨论了估算动作值的 TD 预测算法。和 TD(0) 算法相似，该算法保证会收敛于真动作值函数，只要步长参数 \alpha*α* 足够小。）



## TD 控制：Sarsa(0)

- **Sarsa(0)**（或 **Sarsa**）是既定策略 TD 控制方法。它保证会收敛于最优动作值函数 q_**q*∗，只要步长参数 \alpha*α* 足够小，并且所选的 \epsilon*ϵ* 满足**有限状态下的无限探索贪婪算法 (GLIE)** 条件。



![img](../assets/media/uda-ml/supervisedlearning/17-i3.png)](https://classroom.udacity.com/nanodegrees/nd009-cn-advanced/parts/a2386085-8101-47b6-84e0-7b61a76c2b82/modules/dffda80a-0d5b-460d-afbc-3e0ce20e867f/lessons/d2de57a0-cd89-40bd-b87f-ec0298b425cf/concepts/7d2dafe6-e522-4a8d-beb0-e9dd6eadddfc#)



## TD 控制：Sarsamax

- **Sarsamax**（或 **Q 学习**）是一种新策略 TD 控制方法。它会在保证 Sarsa 算法会收敛的相同条件下保证收敛于最优动作值函数 q_**q*∗。



![img](../assets/media/uda-ml/supervisedlearning/17-i6.png)](https://classroom.udacity.com/nanodegrees/nd009-cn-advanced/parts/a2386085-8101-47b6-84e0-7b61a76c2b82/modules/dffda80a-0d5b-460d-afbc-3e0ce20e867f/lessons/d2de57a0-cd89-40bd-b87f-ec0298b425cf/concepts/7d2dafe6-e522-4a8d-beb0-e9dd6eadddfc#)



## TD 控制：预期 Sarsa

- **预期 Sarsa** 是一种新策略 TD 控制方法。它会在保证 Sarsa 和 Sarsamax 算法会收敛的相同条件下保证收敛于最优动作值函数 q_**q*∗。



![img](../assets/media/uda-ml/supervisedlearning/17-i9.png)](https://classroom.udacity.com/nanodegrees/nd009-cn-advanced/parts/a2386085-8101-47b6-84e0-7b61a76c2b82/modules/dffda80a-0d5b-460d-afbc-3e0ce20e867f/lessons/d2de57a0-cd89-40bd-b87f-ec0298b425cf/concepts/7d2dafe6-e522-4a8d-beb0-e9dd6eadddfc#)



## 分析性能

- 既定策略 TD 控制方法（例如 Sarsa 和 Sarsa）比新策略 TD 控制方法（例如 Q 学习）的在线性能好。
- 预期 Sarsa 通常性能比 Sarsa 好。