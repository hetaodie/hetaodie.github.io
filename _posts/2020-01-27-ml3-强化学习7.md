---



layout: post
title: 机器学习-强化学习-迷你项目
description: 在这章，主要讲解了监督学习相关的技术。
Keywords: 机器学习、模型、评估指标
tagline: 
categories: [ML]
tags: [ML]

---



* 目录
 {:toc  }
# 

# 项目说明



对于此迷你项目，你将使用 OpenAI Gym 的 `Taxi-v2` 环境设计一个算法，并指导出租车智能体浏览一个小的网格世界。

现在花时间在新窗口中打开下个部分的工作区。



![img](/Users/weixu/github/hetaodie.github.io/assets/media/uda-ml/supervisedlearning/1-11.png)在新窗口中打开工作区。](https://classroom.udacity.com/nanodegrees/nd009-cn-advanced/parts/a2386085-8101-47b6-84e0-7b61a76c2b82/modules/dffda80a-0d5b-460d-afbc-3e0ce20e867f/lessons/5b2d11fe-6d94-4d16-8c69-453bf9beb1e8/concepts/8e2b7375-7af9-4aee-9589-be17029935bd#)



接着，通过点击**新建终端**打开终端。在配置环境时，你将看到一些输出结果。



1![img](/Users/weixu/github/hetaodie.github.io/assets/media/uda-ml/supervisedlearning/1-112.png)打开新的终端。](https://classroom.udacity.com/nanodegrees/nd009-cn-advanced/parts/a2386085-8101-47b6-84e0-7b61a76c2b82/modules/dffda80a-0d5b-460d-afbc-3e0ce20e867f/lessons/5b2d11fe-6d94-4d16-8c69-453bf9beb1e8/concepts/8e2b7375-7af9-4aee-9589-be17029935bd#)



工作区包含三个文件：

- `Agent.py`：在此处开发强化学习智能体。这是你唯一需要修改的文件。
- `Monitor.py`：`interact` 函数测试智能体通过与环境互动达到的学习效果。
- `main.py`：在终端里运行此文件，以检查智能体的性能。

在工作区里打开所有三个文件。



![img](/Users/weixu/github/hetaodie.github.io/assets/media/uda-ml/supervisedlearning/1-1123.png)‘打开 `agent.py`、`Monitor.py` 和 `main.py`。’](https://classroom.udacity.com/nanodegrees/nd009-cn-advanced/parts/a2386085-8101-47b6-84e0-7b61a76c2b82/modules/dffda80a-0d5b-460d-afbc-3e0ce20e867f/lessons/5b2d11fe-6d94-4d16-8c69-453bf9beb1e8/concepts/8e2b7375-7af9-4aee-9589-be17029935bd#)



接着，通过在终端中执行 `python main.py` 运行 `main.py`。



![img](/Users/weixu/github/hetaodie.github.io/assets/media/uda-ml/supervisedlearning/1-21.png)在终端中运行 `python main.py`。](https://classroom.udacity.com/nanodegrees/nd009-cn-advanced/parts/a2386085-8101-47b6-84e0-7b61a76c2b82/modules/dffda80a-0d5b-460d-afbc-3e0ce20e867f/lessons/5b2d11fe-6d94-4d16-8c69-453bf9beb1e8/concepts/8e2b7375-7af9-4aee-9589-be17029935bd#)



当你运行 `main.py` 时，你在 `Agent.py` 中指定的智能体会与环境互动 20,000 个阶段。互动详情在 `Monitor.py` 中指定，它会返回两个变量：`avg_rewards` 和 `best_avg_reward`。

- `avg_rewards` 是一个双队列，其中 `avg_rewards[i]` 是智能体从阶段 `i+1` 到阶段 `i+100`（含）收集的平均（未折扣）回报。例如，`avg_rewards[0]` 是智能体在前 100 个阶段中收集的平均回报。
- `best_avg_reward` 是 `avg_rewards` 中的最大项。这是你在确定智能体在该任务中的效果时应该使用的最终得分。

你的任务是修改 `Agents.py` 文件以改进智能体的性能。

- 使用 `__init__()` 方法定义任何所需的实例变量。目前，我们定义了智能体可以采取的动作数量 (`nA`) ，并将动作值 (`Q`) 初始化为空的数组字典。你可以随意添加更多实例变量；例如，如果智能体使用 Epsilon 贪婪策略选择动作，你可能会发现有必要定义 ε 的值。
- `select_action()` 方法将环境状态作为输入，并返回智能体选择的动作。我们提供的默认代码会随机地选择动作。
- `step()` 方法将（`state`、`action`、`reward`、`next_state`）元组以及 `done` 变量作为输入，如果 ε 已结束，该变量将为 `True`。默认代码（你肯定需要更改！）会使上个状态动作对的值加一。你应该更改此方法，以使用抽取的经验元组更新智能体对问题的了解。

修改该函数后，你只需运行 `python main.py` 以测试新智能体。

虽然你可以实现你所选的任何算法，但是注意，你可以通过使用我们在课程中介绍的一些方法达到满意的性能。



### 评估性能



OpenAI Gym 将[“解决”](https://gym.openai.com/envs/Taxi-v1/)该任务定义为在连续尝试 100 次以上之后平均回报为 9.7。

虽然我们不会给此迷你项目打分，但是，建议你在连续尝试 100 次以上之后至少达到 9.1（`best_avg_reward` > 9.1）。



### 分享成果



如果你对你的实现很满意，请在 MLND slack 的 **#ml-reinforcement** 通道中分享你的结果！你也可以在该 slack 通道中提问、获取实现提示、分享观点或寻找合作者！

最后一步，为了与更广泛的 RL 社区分享你的观点，可能需要写一个总结并提交到 [OpenAI Gym Leaderboard](https://github.com/openai/gym/wiki/Leaderboard)！